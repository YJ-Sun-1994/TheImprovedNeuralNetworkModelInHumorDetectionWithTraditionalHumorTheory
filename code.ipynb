{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "“Copy of code.ipynb（副本）”的副本",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YJ-Sun-1994/TheImprovedNeuralNetworkModelInHumorDetectionWithTraditionalHumorTheory/blob/main/%E2%80%9CCopy_of_code_ipynb%EF%BC%88%E5%89%AF%E6%9C%AC%EF%BC%89%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ2pWYdhRMPR"
      },
      "source": [
        "#Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H8XaE25WlHY"
      },
      "source": [
        "##Connect Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BbQVpZkQVKo"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3neLflZWpiX"
      },
      "source": [
        "##Install Transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhZp3J18WvK9"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fm3rotrtWplO"
      },
      "source": [
        "##Import Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sedIANtJW3Gr"
      },
      "source": [
        "from pathlib import Path\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from tokenizers.implementations import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "from tokenizers import AddedToken\n",
        "from transformers import RobertaTokenizer\n",
        "import math\n",
        "import csv\n",
        "import re\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import lr_scheduler\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "import transformers\n",
        "import tokenizers\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers import RobertaConfig, RobertaModel\n",
        "#from transformers.modeling_roberta import RobertaClassificationHead\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.autonotebook import tqdm\n",
        "import utils\n",
        "from transformers import RobertaConfig, RobertaModel, RobertaForSequenceClassification, RobertaTokenizer\n",
        "from transformers import BertConfig, BertModel, BertForSequenceClassification, BertTokenizer\n",
        "from transformers import AdamW\n",
        "import transformers\n",
        "import torch\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import zipfile\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSWx0N0PWpxH"
      },
      "source": [
        "##SetSeed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87uKP33uW5_A"
      },
      "source": [
        "def fix_seed(seed=1):\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  random.seed(seed)\n",
        "fix_seed()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbJ-giXExVBN"
      },
      "source": [
        "##Configuration&Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNLUwuLBxYAF"
      },
      "source": [
        "configuration = RobertaConfig()\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmA7gmIQQ_Tp"
      },
      "source": [
        "#Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOF9PUNtXBus"
      },
      "source": [
        "##Task1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TADbGJsAXNux"
      },
      "source": [
        "###No Dev.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiLWOJcoXNA1"
      },
      "source": [
        "dev_type = 'No_dev'\n",
        "import csv\n",
        "import re\n",
        "def readTask1Data(data_address):\n",
        "  data_id = list()\n",
        "  data_mask = list()\n",
        "  data_orginal = list()\n",
        "  data_replaced = list()\n",
        "  data_grades = list()\n",
        "  data_meanGrade = list()\n",
        "  data_editword = list()\n",
        "  data_orginalword = list()\n",
        "  flag = 0\n",
        "  p = r\"(?<=<).+?(?=/>)\" \n",
        "  with open(data_address, 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    for row in reader:\n",
        "      if flag == 0:\n",
        "        flag = 1\n",
        "        continue\n",
        "      data_id.append(row[0])\n",
        "      line = row[1]\n",
        "      while '  ' in line:\n",
        "        line.replace('  ',' ')\n",
        "      line = line.replace('/ >','/>')\n",
        "      line = line.replace('< ','<')\n",
        "      line = line.replace(' />','/>')\n",
        "      data_orginalword.append(re.findall(p, line))\n",
        "      data_orginal_temp = line.replace('<','').replace('/>','').replace('  ',' ')\n",
        "      data_orginal.append(data_orginal_temp)\n",
        "      data_mask_temp = re.sub(p, 'edit', line)\n",
        "      if '<edit/>' not in data_mask_temp:\n",
        "        print(data_mask_temp)\n",
        "      data_mask.append(data_mask_temp)\n",
        "      data_replaced_temp = data_mask_temp.replace('<edit/>',row[2])\n",
        "      data_replaced.append(data_replaced_temp)\n",
        "      data_grades.append(row[3])\n",
        "      data_meanGrade.append([float(row[4])])\n",
        "      data_editword.append(row[2])\n",
        "  return  data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword\n",
        "data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword = readTask1Data('/content/drive/MyDrive/Colab Notebooks/semeval-2020-task-7-dataset/subtask-1/train.csv')\n",
        "test_id, test_mask, test_orginal, test_replaced, test_grades, test_meanGrade, test_editword, test_orginalword = readTask1Data('/content/drive/MyDrive/Colab Notebooks/semeval-2020-task-7-dataset/subtask-1/test.csv')\n",
        "def splitTrainValid(data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword, valid_num=0.3):\n",
        "  train_id = list()\n",
        "  train_mask = list()\n",
        "  train_orginal = list()\n",
        "  train_replaced = list()\n",
        "  train_grades = list()\n",
        "  train_meanGrade = list()\n",
        "  train_editword = list()\n",
        "  train_orginalword = list()\n",
        "  val_id = list()\n",
        "  val_mask = list()\n",
        "  val_orginal = list()\n",
        "  val_replaced = list()\n",
        "  val_grades = list()\n",
        "  val_meanGrade = list()\n",
        "  val_editword = list()\n",
        "  val_orginalword = list()\n",
        "  temp_list = list()\n",
        "  for num in range(len(data_id)):\n",
        "    temp_list.append([data_id[num], data_mask[num], data_orginal[num], data_replaced[num], data_grades[num], data_meanGrade[num], data_editword[num], data_orginalword[num]])\n",
        "  train_set,valid_set = train_test_split(temp_list,test_size = valid_num)\n",
        "  for (set_id_temp, set_mask_temp, set_orginal_temp, set_replaced_temp, set_grades_temp, set_meanGrade_temp, set_editword_temp, set_orginalword_temp) in train_set:\n",
        "    train_id.append(set_id_temp)\n",
        "    train_mask.append(set_mask_temp)\n",
        "    train_orginal.append(set_orginal_temp)\n",
        "    train_replaced.append(set_replaced_temp)\n",
        "    train_grades.append(set_grades_temp)\n",
        "    train_meanGrade.append(set_meanGrade_temp)\n",
        "    train_editword.append(set_editword_temp)\n",
        "    train_orginalword.append(set_orginalword_temp)\n",
        "  for (set_id_temp, set_mask_temp, set_orginal_temp, set_replaced_temp, set_grades_temp, set_meanGrade_temp, set_editword_temp, set_orginalword_temp) in valid_set:\n",
        "    val_id.append(set_id_temp)\n",
        "    val_mask.append(set_mask_temp)\n",
        "    val_orginal.append(set_orginal_temp)\n",
        "    val_replaced.append(set_replaced_temp)\n",
        "    val_grades.append(set_grades_temp)\n",
        "    val_meanGrade.append(set_meanGrade_temp)\n",
        "    val_editword.append(set_editword_temp)\n",
        "    val_orginalword.append(set_orginalword_temp)\n",
        "  return train_id, train_mask, train_orginal, train_replaced, train_grades, train_meanGrade, train_editword, train_orginalword, val_id,val_mask,val_orginal,val_replaced,val_grades,val_meanGrade,val_editword,val_orginalword\n",
        "data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword, val_id, val_mask, val_orginal, val_replaced, val_grades, val_meanGrade, val_editword, val_orginalword = splitTrainValid(data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dPcec-_XO7v"
      },
      "source": [
        "###With Dev.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEoSuWT5XG72"
      },
      "source": [
        "dev_type = 'with_dev'\n",
        "import csv\n",
        "import re\n",
        "def readTask1Data(data_address):\n",
        "  data_id = list()\n",
        "  data_mask = list()\n",
        "  data_orginal = list()\n",
        "  data_replaced = list()\n",
        "  data_grades = list()\n",
        "  data_meanGrade = list()\n",
        "  data_editword = list()\n",
        "  data_orginalword = list()\n",
        "  flag = 0\n",
        "  p = r\"(?<=<).+?(?=/>)\" \n",
        "  with open(data_address, 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    for row in reader:\n",
        "      if flag == 0:\n",
        "        flag = 1\n",
        "        continue\n",
        "      data_id.append(row[0])\n",
        "      line = row[1]\n",
        "      while '  ' in line:\n",
        "        line.replace('  ',' ')\n",
        "      line = line.replace('/ >','/>')\n",
        "      line = line.replace('< ','<')\n",
        "      line = line.replace(' />','/>')\n",
        "      data_orginalword.append(re.findall(p, line))\n",
        "      data_orginal_temp = line.replace('<','').replace('/>','').replace('  ',' ')\n",
        "      data_orginal.append(data_orginal_temp)\n",
        "      data_mask_temp = re.sub(p, 'edit', line)\n",
        "      if '<edit/>' not in data_mask_temp:\n",
        "        print(data_mask_temp)\n",
        "      data_mask.append(data_mask_temp)\n",
        "      data_replaced_temp = data_mask_temp.replace('<edit/>',row[2])\n",
        "      data_replaced.append(data_replaced_temp)\n",
        "      data_grades.append(row[3])\n",
        "      data_meanGrade.append([float(row[4])])\n",
        "      data_editword.append(row[2])\n",
        "  return  data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword\n",
        "data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword = readTask1Data('/content/drive/MyDrive/Colab Notebooks/semeval-2020-task-7-dataset/subtask-1/train.csv')\n",
        "test_id, test_mask, test_orginal, test_replaced, test_grades, test_meanGrade, test_editword, test_orginalword = readTask1Data('/content/drive/MyDrive/Colab Notebooks/semeval-2020-task-7-dataset/subtask-1/test.csv')\n",
        "val_id, val_mask, val_orginal, val_replaced, val_grades, val_meanGrade, val_editword, val_orginalword = readTask1Data('/content/drive/MyDrive/Colab Notebooks/semeval-2020-task-7-dataset/subtask-1/dev.csv')\n",
        "data_id = data_id + val_id\n",
        "data_mask = data_mask + val_mask\n",
        "data_orginal = data_orginal + val_orginal\n",
        "data_replaced = data_replaced + val_replaced\n",
        "data_grades = data_grades + val_grades\n",
        "data_meanGrade = data_meanGrade + val_meanGrade\n",
        "data_editword = data_editword + val_editword\n",
        "data_orginalword = data_orginalword + val_orginalword\n",
        "def splitTrainValid(data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword, valid_num=0.3):\n",
        "  train_id = list()\n",
        "  train_mask = list()\n",
        "  train_orginal = list()\n",
        "  train_replaced = list()\n",
        "  train_grades = list()\n",
        "  train_meanGrade = list()\n",
        "  train_editword = list()\n",
        "  train_orginalword = list()\n",
        "  val_id = list()\n",
        "  val_mask = list()\n",
        "  val_orginal = list()\n",
        "  val_replaced = list()\n",
        "  val_grades = list()\n",
        "  val_meanGrade = list()\n",
        "  val_editword = list()\n",
        "  val_orginalword = list()\n",
        "  temp_list = list()\n",
        "  for num in range(len(data_id)):\n",
        "    temp_list.append([data_id[num], data_mask[num], data_orginal[num], data_replaced[num], data_grades[num], data_meanGrade[num], data_editword[num], data_orginalword[num]])\n",
        "  train_set,valid_set = train_test_split(temp_list,test_size = valid_num)\n",
        "  for (set_id_temp, set_mask_temp, set_orginal_temp, set_replaced_temp, set_grades_temp, set_meanGrade_temp, set_editword_temp, set_orginalword_temp) in train_set:\n",
        "    train_id.append(set_id_temp)\n",
        "    train_mask.append(set_mask_temp)\n",
        "    train_orginal.append(set_orginal_temp)\n",
        "    train_replaced.append(set_replaced_temp)\n",
        "    train_grades.append(set_grades_temp)\n",
        "    train_meanGrade.append(set_meanGrade_temp)\n",
        "    train_editword.append(set_editword_temp)\n",
        "    train_orginalword.append(set_orginalword_temp)\n",
        "  for (set_id_temp, set_mask_temp, set_orginal_temp, set_replaced_temp, set_grades_temp, set_meanGrade_temp, set_editword_temp, set_orginalword_temp) in valid_set:\n",
        "    val_id.append(set_id_temp)\n",
        "    val_mask.append(set_mask_temp)\n",
        "    val_orginal.append(set_orginal_temp)\n",
        "    val_replaced.append(set_replaced_temp)\n",
        "    val_grades.append(set_grades_temp)\n",
        "    val_meanGrade.append(set_meanGrade_temp)\n",
        "    val_editword.append(set_editword_temp)\n",
        "    val_orginalword.append(set_orginalword_temp)\n",
        "  return train_id, train_mask, train_orginal, train_replaced, train_grades, train_meanGrade, train_editword, train_orginalword, val_id,val_mask,val_orginal,val_replaced,val_grades,val_meanGrade,val_editword,val_orginalword\n",
        "data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword, val_id, val_mask, val_orginal, val_replaced, val_grades, val_meanGrade, val_editword, val_orginalword = splitTrainValid(data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZzRzi67XByG"
      },
      "source": [
        "##Task2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEIQt891Xh7e"
      },
      "source": [
        "###No Dev.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyG9IUuHXm5m"
      },
      "source": [
        "import csv\n",
        "import re\n",
        "def readTask1Data(data_address):\n",
        "  data_id = list()\n",
        "  data_mask1 = list()\n",
        "  data_orginal1 = list()\n",
        "  data_replaced1 = list()\n",
        "  data_grades1 = list()\n",
        "  data_meanGrade1 = list()\n",
        "  data_editword1 = list()\n",
        "  data_orginalword1 = list()\n",
        "  data_mask2 = list()\n",
        "  data_orginal2 = list()\n",
        "  data_replaced2 = list()\n",
        "  data_grades2 = list()\n",
        "  data_meanGrade2 = list()\n",
        "  data_editword2 = list()\n",
        "  data_orginalword2 = list()\n",
        "  data_label = list()\n",
        "  flag = 0\n",
        "  p = r\"(?<=<).+?(?=/>)\" \n",
        "  with open(data_address, 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    for row in reader:\n",
        "      if flag == 0:\n",
        "        flag = 1\n",
        "        continue\n",
        "      data_id.append(row[0])\n",
        "      line1 = row[1]\n",
        "      while '  ' in line1:\n",
        "        line1.replace('  ',' ')\n",
        "      line1 = line1.replace('/ >','/>')\n",
        "      line1 = line1.replace('< ','<')\n",
        "      line1 = line1.replace(' />','/>')\n",
        "      data_orginalword1.append(re.findall(p, line1))\n",
        "      data_orginal_temp1 = line1.replace('<','').replace('/>','').replace('  ',' ')\n",
        "      data_orginal1.append(data_orginal_temp1)\n",
        "      data_mask_temp1 = re.sub(p, 'edit', line1)\n",
        "      if '<edit/>' not in data_mask_temp1:\n",
        "        print(data_mask_temp)\n",
        "      data_mask1.append(data_mask_temp1)\n",
        "      data_replaced_temp1 = data_mask_temp1.replace('<edit/>',row[2])\n",
        "      data_replaced1.append(data_replaced_temp1)\n",
        "      data_grades1.append(row[3])\n",
        "      data_meanGrade1.append([float(row[4])])\n",
        "      data_editword1.append(row[2])\n",
        "\n",
        "\n",
        "      line2 = row[5]\n",
        "      while '  ' in line2:\n",
        "        line2.replace('  ',' ')\n",
        "      line2 = line2.replace('/ >','/>')\n",
        "      line2 = line2.replace('< ','<')\n",
        "      line2 = line2.replace(' />','/>')\n",
        "      data_orginalword2.append(re.findall(p, line2))\n",
        "      data_orginal_temp2 = line2.replace('<','').replace('/>','').replace('  ',' ')\n",
        "      data_orginal2.append(data_orginal_temp2)\n",
        "      data_mask_temp2 = re.sub(p, 'edit', line2)\n",
        "      if '<edit/>' not in data_mask_temp2:\n",
        "        print(data_mask_temp)\n",
        "      data_mask2.append(data_mask_temp2)\n",
        "      data_replaced_temp2 = data_mask_temp2.replace('<edit/>',row[6])\n",
        "      data_replaced2.append(data_replaced_temp2)\n",
        "      data_grades2.append(row[7])\n",
        "      data_meanGrade2.append([float(row[8])])\n",
        "      data_editword2.append(row[6])\n",
        "\n",
        "      data_label.append(int(row[9]))\n",
        "  return  data_id, data_mask1, data_orginal1, data_replaced1, data_grades1, data_meanGrade1, data_editword1, data_orginalword1, data_mask2, data_orginal2, data_replaced2, data_grades2, data_meanGrade2, data_editword2, data_orginalword2, data_label\n",
        "data_id, data_mask1, data_orginal1, data_replaced1, data_grades1, data_meanGrade1, data_editword1, data_orginalword1, data_mask2, data_orginal2, data_replaced2, data_grades2, data_meanGrade2, data_editword2, data_orginalword2, data_label = readTask1Data('/content/drive/My Drive/FinalProject/semeval-2020-task-7-dataset/semeval-2020-task-7-dataset/subtask-2/train.csv')\n",
        "test_id, test_mask1, test_orginal1, test_replaced1, test_grades1, test_meanGrade1, test_editword1, test_orginalword1, test_mask2, test_orginal2, test_replaced2, test_grades2, test_meanGrade2, test_editword2, test_orginalword2, test_label = readTask1Data('/content/drive/My Drive/FinalProject/semeval-2020-task-7-dataset/semeval-2020-task-7-dataset/subtask-2/test.csv')\n",
        "def splitTrainValid(data_id, data_mask1, data_orginal1, data_replaced1, data_grades1, data_meanGrade1, data_editword1, data_orginalword1, data_mask2, data_orginal2, data_replaced2, data_grades2, data_meanGrade2, data_editword2, data_orginalword2, data_label, valid_num=0.3):\n",
        "  train_id = list()\n",
        "  train_mask1 = list()\n",
        "  train_orginal1 = list()\n",
        "  train_replaced1 = list()\n",
        "  train_grades1 = list()\n",
        "  train_meanGrade1 = list()\n",
        "  train_editword1 = list()\n",
        "  train_orginalword1 = list()\n",
        "  train_mask2 = list()\n",
        "  train_orginal2 = list()\n",
        "  train_replaced2 = list()\n",
        "  train_grades2 = list()\n",
        "  train_meanGrade2 = list()\n",
        "  train_editword2 = list()\n",
        "  train_orginalword2 = list()\n",
        "  train_label = list()\n",
        "  val_id = list()\n",
        "  val_mask1 = list()\n",
        "  val_orginal1 = list()\n",
        "  val_replaced1 = list()\n",
        "  val_grades1 = list()\n",
        "  val_meanGrade1 = list()\n",
        "  val_editword1 = list()\n",
        "  val_orginalword1 = list()\n",
        "  val_mask2 = list()\n",
        "  val_orginal2 = list()\n",
        "  val_replaced2 = list()\n",
        "  val_grades2 = list()\n",
        "  val_meanGrade2 = list()\n",
        "  val_editword2 = list()\n",
        "  val_orginalword2 = list()\n",
        "  val_label = list()\n",
        "  temp_list = list()\n",
        "  for num in range(len(data_id)):\n",
        "    temp_list.append([data_id[num], data_mask1[num], data_orginal1[num], data_replaced1[num], data_grades1[num], data_meanGrade1[num], data_editword1[num], data_orginalword1[num], data_mask2[num], data_orginal2[num], data_replaced2[num], data_grades2[num], data_meanGrade2[num], data_editword2[num], data_orginalword2[num], data_label[num]])\n",
        "  train_set,valid_set = train_test_split(temp_list,test_size = valid_num)\n",
        "  for (set_id_temp, set_mask1_temp, set_orginal1_temp, set_replaced1_temp, set_grades1_temp, set_meanGrade1_temp, set_editword1_temp, set_orginalword1_temp, set_mask2_temp, set_orginal2_temp, set_replaced2_temp, set_grades2_temp, set_meanGrade2_temp, set_editword2_temp, set_orginalword2_temp, set_label_temp) in train_set:\n",
        "    train_id.append(set_id_temp)\n",
        "    train_mask1.append(set_mask1_temp)\n",
        "    train_orginal1.append(set_orginal1_temp)\n",
        "    train_replaced1.append(set_replaced1_temp)\n",
        "    train_grades1.append(set_grades1_temp)\n",
        "    train_meanGrade1.append(set_meanGrade1_temp)\n",
        "    train_editword1.append(set_editword1_temp)\n",
        "    train_orginalword1.append(set_orginalword1_temp)\n",
        "    train_mask2.append(set_mask2_temp)\n",
        "    train_orginal2.append(set_orginal2_temp)\n",
        "    train_replaced2.append(set_replaced2_temp)\n",
        "    train_grades2.append(set_grades2_temp)\n",
        "    train_meanGrade2.append(set_meanGrade2_temp)\n",
        "    train_editword2.append(set_editword2_temp)\n",
        "    train_orginalword2.append(set_orginalword2_temp)\n",
        "    train_label.append(set_label_temp)\n",
        "  for (set_id_temp, set_mask1_temp, set_orginal1_temp, set_replaced1_temp, set_grades1_temp, set_meanGrade1_temp, set_editword1_temp, set_orginalword1_temp, set_mask2_temp, set_orginal2_temp, set_replaced2_temp, set_grades2_temp, set_meanGrade2_temp, set_editword2_temp, set_orginalword2_temp, set_label_temp) in valid_set:\n",
        "    val_id.append(set_id_temp)\n",
        "    val_mask1.append(set_mask1_temp)\n",
        "    val_orginal1.append(set_orginal1_temp)\n",
        "    val_replaced1.append(set_replaced1_temp)\n",
        "    val_grades1.append(set_grades1_temp)\n",
        "    val_meanGrade1.append(set_meanGrade1_temp)\n",
        "    val_editword1.append(set_editword1_temp)\n",
        "    val_orginalword1.append(set_orginalword1_temp)\n",
        "    val_mask2.append(set_mask2_temp)\n",
        "    val_orginal2.append(set_orginal2_temp)\n",
        "    val_replaced2.append(set_replaced2_temp)\n",
        "    val_grades2.append(set_grades2_temp)\n",
        "    val_meanGrade2.append(set_meanGrade2_temp)\n",
        "    val_editword2.append(set_editword2_temp)\n",
        "    val_orginalword2.append(set_orginalword2_temp)\n",
        "    val_label.append(set_label_temp)\n",
        "  return train_id, train_mask1, train_orginal1, train_replaced1, train_grades1, train_meanGrade1, train_editword1, train_orginalword1, train_mask2, train_orginal2, train_replaced2, train_grades2, train_meanGrade2, train_editword2, train_orginalword2, train_label, val_id,val_mask1,val_orginal1,val_replaced1,val_grades1,val_meanGrade1,val_editword1,val_orginalword1,val_mask2,val_orginal2,val_replaced2,val_grades2,val_meanGrade2,val_editword2,val_orginalword2,val_label\n",
        "data_id, data_mask1, data_orginal1, data_replaced1, data_grades1, data_meanGrade1, data_editword1, data_orginalword1, data_mask2, data_orginal2, data_replaced2, data_grades2, data_meanGrade2, data_editword2, data_orginalword2, data_label,val_id,val_mask1,val_orginal1,val_replaced1,val_grades1,val_meanGrade1,val_editword1,val_orginalword1,val_mask2,val_orginal2,val_replaced2,val_grades2,val_meanGrade2,val_editword2,val_orginalword2,val_label = splitTrainValid(data_id, data_mask1, data_orginal1, data_replaced1, data_grades1, data_meanGrade1, data_editword1, data_orginalword1, data_mask2, data_orginal2, data_replaced2, data_grades2, data_meanGrade2, data_editword2, data_orginalword2, data_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGkiw0fiXh-Y"
      },
      "source": [
        "###With Dev.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt2aylSAXr4o"
      },
      "source": [
        "import csv\n",
        "import re\n",
        "def readTask1Data(data_address):\n",
        "  data_id = list()\n",
        "  data_mask1 = list()\n",
        "  data_orginal1 = list()\n",
        "  data_replaced1 = list()\n",
        "  data_grades1 = list()\n",
        "  data_meanGrade1 = list()\n",
        "  data_editword1 = list()\n",
        "  data_orginalword1 = list()\n",
        "  data_mask2 = list()\n",
        "  data_orginal2 = list()\n",
        "  data_replaced2 = list()\n",
        "  data_grades2 = list()\n",
        "  data_meanGrade2 = list()\n",
        "  data_editword2 = list()\n",
        "  data_orginalword2 = list()\n",
        "  data_label = list()\n",
        "  flag = 0\n",
        "  p = r\"(?<=<).+?(?=/>)\" \n",
        "  with open(data_address, 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    for row in reader:\n",
        "      if flag == 0:\n",
        "        flag = 1\n",
        "        continue\n",
        "      data_id.append(row[0])\n",
        "      line1 = row[1]\n",
        "      while '  ' in line1:\n",
        "        line1.replace('  ',' ')\n",
        "      line1 = line1.replace('/ >','/>')\n",
        "      line1 = line1.replace('< ','<')\n",
        "      line1 = line1.replace(' />','/>')\n",
        "      data_orginalword1.append(re.findall(p, line1))\n",
        "      data_orginal_temp1 = line1.replace('<','').replace('/>','').replace('  ',' ')\n",
        "      data_orginal1.append(data_orginal_temp1)\n",
        "      data_mask_temp1 = re.sub(p, 'edit', line1)\n",
        "      if '<edit/>' not in data_mask_temp1:\n",
        "        print(data_mask_temp)\n",
        "      data_mask1.append(data_mask_temp1)\n",
        "      data_replaced_temp1 = data_mask_temp1.replace('<edit/>',row[2])\n",
        "      data_replaced1.append(data_replaced_temp1)\n",
        "      data_grades1.append(row[3])\n",
        "      data_meanGrade1.append([float(row[4])])\n",
        "      data_editword1.append(row[2])\n",
        "\n",
        "\n",
        "      line2 = row[5]\n",
        "      while '  ' in line2:\n",
        "        line2.replace('  ',' ')\n",
        "      line2 = line2.replace('/ >','/>')\n",
        "      line2 = line2.replace('< ','<')\n",
        "      line2 = line2.replace(' />','/>')\n",
        "      data_orginalword2.append(re.findall(p, line2))\n",
        "      data_orginal_temp2 = line2.replace('<','').replace('/>','').replace('  ',' ')\n",
        "      data_orginal2.append(data_orginal_temp2)\n",
        "      data_mask_temp2 = re.sub(p, 'edit', line2)\n",
        "      if '<edit/>' not in data_mask_temp2:\n",
        "        print(data_mask_temp)\n",
        "      data_mask2.append(data_mask_temp2)\n",
        "      data_replaced_temp2 = data_mask_temp2.replace('<edit/>',row[6])\n",
        "      data_replaced2.append(data_replaced_temp2)\n",
        "      data_grades2.append(row[7])\n",
        "      data_meanGrade2.append([float(row[8])])\n",
        "      data_editword2.append(row[6])\n",
        "\n",
        "      data_label.append(int(row[9]))\n",
        "  return  data_id, data_mask1, data_orginal1, data_replaced1, data_grades1, data_meanGrade1, data_editword1, data_orginalword1, data_mask2, data_orginal2, data_replaced2, data_grades2, data_meanGrade2, data_editword2, data_orginalword2, data_label\n",
        "data_id, data_mask1, data_orginal1, data_replaced1, data_grades1, data_meanGrade1, data_editword1, data_orginalword1, data_mask2, data_orginal2, data_replaced2, data_grades2, data_meanGrade2, data_editword2, data_orginalword2, data_label = readTask1Data('/content/drive/My Drive/FinalProject/semeval-2020-task-7-dataset/semeval-2020-task-7-dataset/subtask-2/train.csv')\n",
        "test_id, test_mask1, test_orginal1, test_replaced1, test_grades1, test_meanGrade1, test_editword1, test_orginalword1, test_mask2, test_orginal2, test_replaced2, test_grades2, test_meanGrade2, test_editword2, test_orginalword2, test_label = readTask1Data('/content/drive/My Drive/FinalProject/semeval-2020-task-7-dataset/semeval-2020-task-7-dataset/subtask-2/test.csv')\n",
        "dev_id, dev_mask1, dev_orginal1, dev_replaced1, dev_grades1, dev_meanGrade1, dev_editword1, dev_orginalword1, dev_mask2, dev_orginal2, dev_replaced2, dev_grades2, dev_meanGrade2, dev_editword2, dev_orginalword2, dev_label = readTask1Data('/content/drive/My Drive/FinalProject/semeval-2020-task-7-dataset/semeval-2020-task-7-dataset/subtask-2/dev.csv')\n",
        "data_id = dev_id + data_id\n",
        "data_mask1 = dev_mask1 + data_mask1\n",
        "data_orginal1 = dev_orginal1 + data_orginal1\n",
        "data_replaced1 = dev_replaced1 + data_replaced1\n",
        "data_grades1 = dev_grades1 + data_grades1\n",
        "data_meanGrade1 = dev_meanGrade1 + data_meanGrade1\n",
        "data_editword1 = dev_editword1 + data_editword1\n",
        "data_orginalword1 = dev_orginalword1 + data_orginalword1\n",
        "data_mask2 = dev_mask2 + data_mask2\n",
        "data_orginal2 = dev_orginal2 + data_orginal2\n",
        "data_replaced2 = dev_replaced2 + data_replaced2\n",
        "data_grades2 = dev_grades2 + data_grades2\n",
        "data_meanGrade2 = dev_meanGrade2 + data_meanGrade2\n",
        "data_editword2 = dev_editword2 + data_editword2\n",
        "data_orginalword2 = dev_orginalword2 + data_orginalword2\n",
        "data_label = dev_label + data_label\n",
        "def splitTrainValid(data_id, data_mask1, data_orginal1, data_replaced1, data_grades1, data_meanGrade1, data_editword1, data_orginalword1, data_mask2, data_orginal2, data_replaced2, data_grades2, data_meanGrade2, data_editword2, data_orginalword2, data_label, valid_num=0.3):\n",
        "  train_id = list()\n",
        "  train_mask1 = list()\n",
        "  train_orginal1 = list()\n",
        "  train_replaced1 = list()\n",
        "  train_grades1 = list()\n",
        "  train_meanGrade1 = list()\n",
        "  train_editword1 = list()\n",
        "  train_orginalword1 = list()\n",
        "  train_mask2 = list()\n",
        "  train_orginal2 = list()\n",
        "  train_replaced2 = list()\n",
        "  train_grades2 = list()\n",
        "  train_meanGrade2 = list()\n",
        "  train_editword2 = list()\n",
        "  train_orginalword2 = list()\n",
        "  train_label = list()\n",
        "  val_id = list()\n",
        "  val_mask1 = list()\n",
        "  val_orginal1 = list()\n",
        "  val_replaced1 = list()\n",
        "  val_grades1 = list()\n",
        "  val_meanGrade1 = list()\n",
        "  val_editword1 = list()\n",
        "  val_orginalword1 = list()\n",
        "  val_mask2 = list()\n",
        "  val_orginal2 = list()\n",
        "  val_replaced2 = list()\n",
        "  val_grades2 = list()\n",
        "  val_meanGrade2 = list()\n",
        "  val_editword2 = list()\n",
        "  val_orginalword2 = list()\n",
        "  val_label = list()\n",
        "  temp_list = list()\n",
        "  for num in range(len(data_id)):\n",
        "    temp_list.append([data_id[num], data_mask1[num], data_orginal1[num], data_replaced1[num], data_grades1[num], data_meanGrade1[num], data_editword1[num], data_orginalword1[num], data_mask2[num], data_orginal2[num], data_replaced2[num], data_grades2[num], data_meanGrade2[num], data_editword2[num], data_orginalword2[num], data_label[num]])\n",
        "  train_set,valid_set = train_test_split(temp_list,test_size = valid_num)\n",
        "  for (set_id_temp, set_mask1_temp, set_orginal1_temp, set_replaced1_temp, set_grades1_temp, set_meanGrade1_temp, set_editword1_temp, set_orginalword1_temp, set_mask2_temp, set_orginal2_temp, set_replaced2_temp, set_grades2_temp, set_meanGrade2_temp, set_editword2_temp, set_orginalword2_temp, set_label_temp) in train_set:\n",
        "    train_id.append(set_id_temp)\n",
        "    train_mask1.append(set_mask1_temp)\n",
        "    train_orginal1.append(set_orginal1_temp)\n",
        "    train_replaced1.append(set_replaced1_temp)\n",
        "    train_grades1.append(set_grades1_temp)\n",
        "    train_meanGrade1.append(set_meanGrade1_temp)\n",
        "    train_editword1.append(set_editword1_temp)\n",
        "    train_orginalword1.append(set_orginalword1_temp)\n",
        "    train_mask2.append(set_mask2_temp)\n",
        "    train_orginal2.append(set_orginal2_temp)\n",
        "    train_replaced2.append(set_replaced2_temp)\n",
        "    train_grades2.append(set_grades2_temp)\n",
        "    train_meanGrade2.append(set_meanGrade2_temp)\n",
        "    train_editword2.append(set_editword2_temp)\n",
        "    train_orginalword2.append(set_orginalword2_temp)\n",
        "    train_label.append(set_label_temp)\n",
        "  for (set_id_temp, set_mask1_temp, set_orginal1_temp, set_replaced1_temp, set_grades1_temp, set_meanGrade1_temp, set_editword1_temp, set_orginalword1_temp, set_mask2_temp, set_orginal2_temp, set_replaced2_temp, set_grades2_temp, set_meanGrade2_temp, set_editword2_temp, set_orginalword2_temp, set_label_temp) in valid_set:\n",
        "    val_id.append(set_id_temp)\n",
        "    val_mask1.append(set_mask1_temp)\n",
        "    val_orginal1.append(set_orginal1_temp)\n",
        "    val_replaced1.append(set_replaced1_temp)\n",
        "    val_grades1.append(set_grades1_temp)\n",
        "    val_meanGrade1.append(set_meanGrade1_temp)\n",
        "    val_editword1.append(set_editword1_temp)\n",
        "    val_orginalword1.append(set_orginalword1_temp)\n",
        "    val_mask2.append(set_mask2_temp)\n",
        "    val_orginal2.append(set_orginal2_temp)\n",
        "    val_replaced2.append(set_replaced2_temp)\n",
        "    val_grades2.append(set_grades2_temp)\n",
        "    val_meanGrade2.append(set_meanGrade2_temp)\n",
        "    val_editword2.append(set_editword2_temp)\n",
        "    val_orginalword2.append(set_orginalword2_temp)\n",
        "    val_label.append(set_label_temp)\n",
        "  return train_id, train_mask1, train_orginal1, train_replaced1, train_grades1, train_meanGrade1, train_editword1, train_orginalword1, train_mask2, train_orginal2, train_replaced2, train_grades2, train_meanGrade2, train_editword2, train_orginalword2, train_label, val_id,val_mask1,val_orginal1,val_replaced1,val_grades1,val_meanGrade1,val_editword1,val_orginalword1,val_mask2,val_orginal2,val_replaced2,val_grades2,val_meanGrade2,val_editword2,val_orginalword2,val_label\n",
        "data_id, data_mask1, data_orginal1, data_replaced1, data_grades1, data_meanGrade1, data_editword1, data_orginalword1, data_mask2, data_orginal2, data_replaced2, data_grades2, data_meanGrade2, data_editword2, data_orginalword2, data_label,val_id,val_mask1,val_orginal1,val_replaced1,val_grades1,val_meanGrade1,val_editword1,val_orginalword1,val_mask2,val_orginal2,val_replaced2,val_grades2,val_meanGrade2,val_editword2,val_orginalword2,val_label = splitTrainValid(data_id, data_mask1, data_orginal1, data_replaced1, data_grades1, data_meanGrade1, data_editword1, data_orginalword1, data_mask2, data_orginal2, data_replaced2, data_grades2, data_meanGrade2, data_editword2, data_orginalword2, data_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuql0WlOQvVb"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaiSFS-2cy4N"
      },
      "source": [
        "##Task1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTSVWvffc6vr"
      },
      "source": [
        "###OurModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWa1viacnjQc"
      },
      "source": [
        "configuration.num_labels = 1\n",
        "class editHeadlineModel(transformers.BertPreTrainedModel):\n",
        "  def __init__(self, conf):\n",
        "    super(editHeadlineModel, self).__init__(conf)\n",
        "    self.roberta = transformers.RobertaModel.from_pretrained('roberta-base')\n",
        "    self.drop_out = nn.Dropout(0.1)\n",
        "    #self.pooling = nn.AdaptiveAvgPool1d(1024)\n",
        "    self.dense = nn.Linear(768, 768)\n",
        "    #self.linear = nn.Linear(768, 1)\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    out = self.roberta(input_ids,attention_mask=attention_mask)[0]\n",
        "    out = out[:,0,:]\n",
        "    out = self.drop_out(out)\n",
        "    out = torch.tanh(out)\n",
        "    out = self.drop_out(out)\n",
        "    #out = self.linear(out)\n",
        "    #logits = self.linear(out)\n",
        "    return out\n",
        "class wordModel(transformers.BertPreTrainedModel):\n",
        "  def __init__(self, conf):\n",
        "    super(wordModel, self).__init__(conf)\n",
        "    self.roberta = transformers.RobertaModel.from_pretrained('roberta-base')\n",
        "    self.drop_out = nn.Dropout(0.1)\n",
        "    self.dense = nn.Linear(768, 768)\n",
        "    self.linear = nn.Linear(768, 1)\n",
        "  def forward(self, input_ids, attention_mask, edit_matrix, edit_total_num):\n",
        "    batch_size = input_ids.size()[0]\n",
        "    out = self.roberta(input_ids,attention_mask=attention_mask)[0]\n",
        "    seq_size = out.size()[1]\n",
        "    out = torch.bmm(edit_matrix, out)\n",
        "    #print(out.size(),out.squeeze(dim=1).size())\n",
        "    out = out/edit_total_num\n",
        "    out = out.squeeze(dim=1)\n",
        "    out = self.drop_out(out)\n",
        "    out = torch.tanh(out)\n",
        "    out = self.drop_out(out)\n",
        "    #logits = self.linear(out)\n",
        "    return out\n",
        "class twoHeadlineModel_v1(transformers.BertPreTrainedModel):\n",
        "  def __init__(self, conf):\n",
        "    super(twoHeadlineModel_v1, self).__init__(conf)\n",
        "    self.roberta = transformers.RobertaModel.from_pretrained('roberta-base')\n",
        "    #self.roberta = transformers.RobertaModel('roberta-base')\n",
        "    self.dense = nn.Linear(768*4, 768*4)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.out_proj = nn.Linear(768*4, 1)\n",
        "    self.linear = nn.Linear(768*4,1)\n",
        "  def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2):\n",
        "    out1 = self.roberta(input_ids1,attention_mask=attention_mask1)[0]\n",
        "    out2 = self.roberta(input_ids2,attention_mask=attention_mask2)[0]\n",
        "    out1 = out1[:,0,:]\n",
        "    out2 = out2[:,0,:]\n",
        "    merge = [out1, out2, (out1 - out2).abs(), out1 * out2]\n",
        "    cat = torch.cat(merge, dim=-1)\n",
        "    out = self.dense(cat)\n",
        "    #out = self.out_proj(out)\n",
        "    #logits = self.Linear(out)\n",
        "    return out\n",
        "class stakeModel(transformers.BertPreTrainedModel):\n",
        "  def __init__(self, conf):\n",
        "    super(stakeModel, self).__init__(conf)\n",
        "    self.editHeadlineModel = editHeadlineModel(conf)\n",
        "    self.wordModel = wordModel(conf)\n",
        "    self.twoHeadlineModel_v1 = twoHeadlineModel_v1(conf)\n",
        "    print(1)\n",
        "    self.dense = nn.Linear(768*6,768*6)\n",
        "    self.linear =  nn.Linear(768*6,1)\n",
        "  def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2, edit_matrix, edit_total_num):\n",
        "    editHeadlineModeloutput = self.editHeadlineModel(input_ids1, attention_mask1)\n",
        "    wordModeloutput = self.wordModel(input_ids1, attention_mask1, edit_matrix, edit_total_num)\n",
        "    twoHeadlineModel_v1output = self.twoHeadlineModel_v1(input_ids1, attention_mask1, input_ids2, attention_mask2)\n",
        "    merge = [editHeadlineModeloutput, wordModeloutput, twoHeadlineModel_v1output]\n",
        "    cat = torch.cat(merge, dim=-1)\n",
        "    out = self.dense(cat)\n",
        "    out = self.linear(out)\n",
        "    return out\n",
        "model = stakeModel(configuration)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsw4zxoHWmWz"
      },
      "source": [
        "##RoBERTa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "es9_tU6hc_oL"
      },
      "source": [
        "###RoBERTaSingleHeadlineModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu5J7sVbnOh9"
      },
      "source": [
        "class editHeadlineModel(transformers.BertPreTrainedModel):\n",
        "  def __init__(self, conf):\n",
        "    super(editHeadlineModel, self).__init__(conf)\n",
        "    self.roberta = transformers.RobertaModel.from_pretrained('roberta-base')\n",
        "    self.drop_out = nn.Dropout(0.1)\n",
        "    #self.pooling = nn.AdaptiveAvgPool1d(1024)\n",
        "    self.dense = nn.Linear(768, 768)\n",
        "    self.linear = nn.Linear(768, 1)\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    out = self.roberta(input_ids,attention_mask=attention_mask)[0]\n",
        "    out = out[:,0,:]\n",
        "    out = self.drop_out(out)\n",
        "    out = torch.tanh(out)\n",
        "    out = self.drop_out(out)\n",
        "    out = self.linear(out)\n",
        "    return out\n",
        "model = editHeadlineModel(configuration)\n",
        "model_name = 'editHeadlineModel'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fn8qLFSCdGCk"
      },
      "source": [
        "###RoBERTaEditedWordModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcubZe9CnQ02"
      },
      "source": [
        "class wordModel(transformers.BertPreTrainedModel):\n",
        "  def __init__(self, conf):\n",
        "    super(wordModel, self).__init__(conf)\n",
        "    self.roberta = transformers.RobertaModel.from_pretrained('roberta-base')\n",
        "    self.drop_out = nn.Dropout(0.1)\n",
        "    self.dense = nn.Linear(768, 768)\n",
        "    self.linear = nn.Linear(768, 1)\n",
        "  def forward(self, input_ids, attention_mask, edit_matrix, edit_total_num):\n",
        "    batch_size = input_ids.size()[0]\n",
        "    out = self.roberta(input_ids,attention_mask=attention_mask)[0]\n",
        "    seq_size = out.size()[1]\n",
        "    #print(edit_matrix.size(), out.size())\n",
        "    out = torch.bmm(edit_matrix, out)\n",
        "    #print(out.size(),out.squeeze(dim=1).size())\n",
        "    out = out/edit_total_num\n",
        "    out = out.squeeze(dim=1)\n",
        "    out = self.drop_out(out)\n",
        "    out = torch.tanh(out)\n",
        "    out = self.drop_out(out)\n",
        "    logits = self.linear(out)\n",
        "    return logits\n",
        "model = wordModel(configuration)\n",
        "model_name = 'wordModel'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DinLjX7BdI5n"
      },
      "source": [
        "###RoBERTaTwoHeadlineModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovvOFpLKnY8I"
      },
      "source": [
        "class twoHeadlineModel_v1(transformers.BertPreTrainedModel):\n",
        "  def __init__(self, conf):\n",
        "    super(twoHeadlineModel_v1, self).__init__(conf)\n",
        "    self.roberta = transformers.RobertaModel.from_pretrained('roberta-base')\n",
        "    #self.roberta = transformers.RobertaModel('roberta-base')\n",
        "    self.dense = nn.Linear(768*4, 768*4)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.linear = nn.Linear(768*4,1)\n",
        "  def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2):\n",
        "    out1 = self.roberta(input_ids1,attention_mask=attention_mask1)[0]\n",
        "    out2 = self.roberta(input_ids2,attention_mask=attention_mask2)[0]\n",
        "    out1 = out1[:,0,:]\n",
        "    out2 = out2[:,0,:]\n",
        "    merge = [out1, out2, (out1 - out2).abs(), out1 * out2]\n",
        "    cat = torch.cat(merge, dim=-1)\n",
        "    out = self.dense(cat)\n",
        "    logits = self.linear(out)\n",
        "    return logits\n",
        "model = twoHeadlineModel_v1(configuration)\n",
        "model_name = 'twoHeadlineModel_v1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlmC43audTJe"
      },
      "source": [
        "###RoBERTaSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxe4Inqyng-U"
      },
      "source": [
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base', return_dict=True, num_labels=1)\n",
        "model_name = 'OrginalRoBERTaSequenceClassification'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjZ8lxHhWqqY"
      },
      "source": [
        "##BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXBqx9Oy2apa"
      },
      "source": [
        "###Configuration&Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6SsDlrV2apf"
      },
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "configuration = RobertaConfig()\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\", do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6kXwwi0KZAz"
      },
      "source": [
        "###BERTSingleHeadlineModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHU3Y7_nKXe_"
      },
      "source": [
        "class editHeadlineModel(transformers.BertPreTrainedModel):\n",
        "  def __init__(self, conf):\n",
        "    super(editHeadlineModel, self).__init__(conf)\n",
        "    self.bert = transformers.BertModel.from_pretrained('bert-base-cased')\n",
        "    self.drop_out = nn.Dropout(0.1)\n",
        "    #self.pooling = nn.AdaptiveAvgPool1d(1024)\n",
        "    self.dense = nn.Linear(768, 768)\n",
        "    self.linear = nn.Linear(768, 1)\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    out = self.bert(input_ids,attention_mask=attention_mask)[0]\n",
        "    out = out[:,0,:]\n",
        "    out = self.drop_out(out)\n",
        "    out = torch.tanh(out)\n",
        "    out = self.drop_out(out)\n",
        "    out = self.linear(out)\n",
        "    return out\n",
        "model = editHeadlineModel(configuration)\n",
        "model_name = 'editHeadlineModel'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WBHjKtSKacm"
      },
      "source": [
        "###BERTEditedWordModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSVZMZ2MKXn6"
      },
      "source": [
        "class wordModel(transformers.BertPreTrainedModel):\n",
        "  def __init__(self, conf):\n",
        "    super(wordModel, self).__init__(conf)\n",
        "    self.bert = transformers.BertModel.from_pretrained('bert-base-cased')\n",
        "    self.drop_out = nn.Dropout(0.1)\n",
        "    self.dense = nn.Linear(768, 768)\n",
        "    self.linear = nn.Linear(768, 1)\n",
        "  def forward(self, input_ids, attention_mask, edit_matrix, edit_total_num):\n",
        "    batch_size = input_ids.size()[0]\n",
        "    out = self.bert(input_ids,attention_mask=attention_mask)[0]\n",
        "    seq_size = out.size()[1]\n",
        "    #print(edit_matrix.size(), out.size())\n",
        "    out = torch.bmm(edit_matrix, out)\n",
        "    #print(out.size(),out.squeeze(dim=1).size())\n",
        "    out = out/edit_total_num\n",
        "    out = out.squeeze(dim=1)\n",
        "    out = self.drop_out(out)\n",
        "    out = torch.tanh(out)\n",
        "    out = self.drop_out(out)\n",
        "    logits = self.linear(out)\n",
        "    return logits\n",
        "model = wordModel(configuration)\n",
        "model_name = 'wordModel'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Cvn3SwYKbJK"
      },
      "source": [
        "###BERTTwoHeadlineModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35RGA63aKXsz"
      },
      "source": [
        "class twoHeadlineModel_v1(transformers.BertPreTrainedModel):\n",
        "  def __init__(self, conf):\n",
        "    super(twoHeadlineModel_v1, self).__init__(conf)\n",
        "    self.bert = transformers.BertModel.from_pretrained('bert-base-cased')\n",
        "    #self.roberta = transformers.RobertaModel('roberta-base')\n",
        "    self.dense = nn.Linear(768*4, 768*4)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.linear = nn.Linear(768*4,1)\n",
        "  def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2):\n",
        "    out1 = self.bert(input_ids1,attention_mask=attention_mask1)[0]\n",
        "    out2 = self.bert(input_ids2,attention_mask=attention_mask2)[0]\n",
        "    out1 = out1[:,0,:]\n",
        "    out2 = out2[:,0,:]\n",
        "    merge = [out1, out2, (out1 - out2).abs(), out1 * out2]\n",
        "    cat = torch.cat(merge, dim=-1)\n",
        "    out = self.dense(cat)\n",
        "    logits = self.linear(out)\n",
        "    return logits\n",
        "model = twoHeadlineModel_v1(configuration)\n",
        "model_name = 'twoHeadlineModel_v1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32FAEAfCKbtP"
      },
      "source": [
        "###BERTSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVeVlJzoKX0B"
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained('bert-base-cased', return_dict=True, num_labels=1)\n",
        "model_name = 'OrginalRoBERTaSequenceClassification'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FkiJxh6WvKS"
      },
      "source": [
        "##LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rnr_77sEW5Ic"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import gensim\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from math import sqrt\n",
        "import string\n",
        "import re\n",
        "import math\n",
        "import codecs\n",
        "import random\n",
        "import nltk\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from collections import defaultdict\n",
        "from scipy.spatial.distance import euclidean, cosine\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "from tqdm import tqdm \n",
        "from nltk import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words_list = stopwords.words('english')\n",
        "from nltk.tag import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOKqUSqHKwoi"
      },
      "source": [
        "###LSTMSingleHeadlineModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhtabXpUKv65"
      },
      "source": [
        "class LSTMSingleHeadlineModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, out_size, max_length, n_layer = 1, dropout = 0.3, pool_feature_num = 200):\n",
        "        super(LSTMSingleHeadlineModel, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.max_length = max_length\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0).cuda()\n",
        "        self.n_layer = n_layer\n",
        "        print('Initialization LSTM Model')\n",
        "        self.rnn = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, dropout=dropout, num_layers=self.n_layer, bidirectional=False).cuda()\n",
        "        self.pooling = nn.AdaptiveMaxPool1d(pool_feature_num).cuda()\n",
        "        self.out = nn.Linear(pool_feature_num, out_size).cuda()\n",
        "    def forward(self, X):\n",
        "        print(X.size())\n",
        "        input = self.embedding(X)\n",
        "        input = input.permute(1, 0, 2)\n",
        "        batch_size = input.size()[1]\n",
        "        hidden_state = Variable(\n",
        "            torch.randn(self.n_layer, batch_size, self.hidden_dim)).cuda()\n",
        "        cell_state = Variable(\n",
        "            torch.randn(self.n_layer, batch_size, self.hidden_dim)).cuda()\n",
        "        output, (final_hidden_state, final_cell_state) = self.rnn(input, (hidden_state, cell_state))\n",
        "        #output = output.permute(1, 0, 2).squeeze(1)\n",
        "        #output = output.reshape(batch_size, self.hidden_dim * self.max_length)\n",
        "        #print(final_hidden_state.size())\n",
        "        #final_hidden_state = final_hidden_state.permute(1, 0, 2).squeeze(1)\n",
        "        output = output.permute(1, 0, 2)\n",
        "        output = output.reshape(1, batch_size, self.hidden_dim * X.size()[1])\n",
        "        pool = self.pooling(output)\n",
        "        pool = pool.permute(1, 0, 2)\n",
        "        pool = pool.squeeze(dim=1)\n",
        "        output = self.out(pool)\n",
        "\n",
        "        return  output # model : [batch_size, num_classes], attention : [batch_size, n_step]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-H8j6-mKw_o"
      },
      "source": [
        "###LSTMEditedWordModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDhAdqNvKv95"
      },
      "source": [
        "class LSTMEditedWordModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, out_size, max_length, n_layer = 1, dropout = 0.3):\n",
        "        super(LSTMEditedWordModel, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.max_length = max_length\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0).cuda()\n",
        "        self.n_layer = n_layer\n",
        "        print('Initialization LSTM Model')\n",
        "        self.rnn = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, dropout=dropout, num_layers=self.n_layer, bidirectional=False).cuda()\n",
        "        self.out = nn.Linear(hidden_dim, out_size).cuda()\n",
        "    def forward(self, X, edit_matrix, edit_total_num):\n",
        "        print(X.size())\n",
        "        input = self.embedding(X)\n",
        "        input = input.permute(1, 0, 2)\n",
        "        batch_size = input.size()[1]\n",
        "        hidden_state = Variable(\n",
        "            torch.randn(self.n_layer, batch_size, self.hidden_dim)).cuda()\n",
        "        cell_state = Variable(\n",
        "            torch.randn(self.n_layer, batch_size, self.hidden_dim)).cuda()\n",
        "        output, (final_hidden_state, final_cell_state) = self.rnn(input, (hidden_state, cell_state))\n",
        "        #output = output.permute(1, 0, 2).squeeze(1)\n",
        "        #output = output.reshape(batch_size, self.hidden_dim * self.max_length)\n",
        "        #print(final_hidden_state.size())\n",
        "        \n",
        "        edit_matrix = edit_matrix.cuda()\n",
        "        out = output.permute(1, 0, 2).squeeze(1)\n",
        "        print(output.size(), edit_matrix.size())\n",
        "        out = torch.bmm(edit_matrix, out).cuda()\n",
        "        out = out/edit_total_num.cuda()\n",
        "        out = out.squeeze(dim=1)\n",
        "        out = self.out(out)\n",
        "        return out\n",
        "\n",
        "        #return  output # model : [batch_size, num_classes], attention : [batch_size, n_step]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZSk2f7gKxqe"
      },
      "source": [
        "###LSTMTwoHeadlineModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47d0NZCDKwA5"
      },
      "source": [
        "class LSTMTwoHeadlineModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, out_size, max_length, n_layer = 1, dropout = 0.3):\n",
        "        super(LSTMTwoHeadlineModel, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.max_length = max_length\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0).cuda()\n",
        "        self.n_layer = n_layer\n",
        "        print('Initialization LSTM Model')\n",
        "        self.rnn = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, dropout=dropout, num_layers=self.n_layer, bidirectional=False).cuda()\n",
        "        self.dense = nn.Linear(hidden_dim*4, hidden_dim*4).cuda()\n",
        "        self.dropout = nn.Dropout(0.1).cuda()\n",
        "        self.linear = nn.Linear(hidden_dim*4,1).cuda()\n",
        "    def forward(self, X1, X2):\n",
        "        input1 = self.embedding(X1)\n",
        "        input2 = self.embedding(X2)\n",
        "        input1 = input1.permute(1, 0, 2)\n",
        "        input2 = input2.permute(1, 0, 2)\n",
        "        batch_size1 = input1.size()[1]\n",
        "        batch_size2 = input2.size()[1]\n",
        "        hidden_state1 = Variable(\n",
        "            torch.randn(self.n_layer, batch_size1, self.hidden_dim)).cuda()\n",
        "        hidden_state2 = Variable(\n",
        "            torch.randn(self.n_layer, batch_size2, self.hidden_dim)).cuda()\n",
        "        cell_state1 = Variable(\n",
        "            torch.randn(self.n_layer, batch_size1, self.hidden_dim)).cuda()\n",
        "        cell_state2 = Variable(\n",
        "            torch.randn(self.n_layer, batch_size2, self.hidden_dim)).cuda()\n",
        "        output1, (final_hidden_state1, final_cell_state1) = self.rnn(input1, (hidden_state1, cell_state1))\n",
        "        output2, (final_hidden_state2, final_cell_state2) = self.rnn(input2, (hidden_state2, cell_state2))\n",
        "        #output = output.permute(1, 0, 2).squeeze(1)\n",
        "        #output = output.reshape(batch_size, self.hidden_dim * self.max_length)\n",
        "        #print(final_hidden_state.size())\n",
        "        #final_hidden_state = final_hidden_state.permute(1, 0, 2).squeeze(1)\n",
        "        #output = self.out(final_hidden_state)\n",
        "        output1 = final_hidden_state1.permute(1, 0, 2).squeeze(1)\n",
        "        output2 = final_hidden_state2.permute(1, 0, 2).squeeze(1)\n",
        "        print(output1.size(),final_hidden_state1.size(),final_cell_state1.size(),output2.size(),final_hidden_state2.size(),final_cell_state2.size())\n",
        "        merge = [output1, output2, (output1 - output2).abs(), output1 * output2]\n",
        "        cat = torch.cat(merge, dim=-1)\n",
        "        out = self.dense(cat)\n",
        "        logits = self.linear(out)\n",
        "\n",
        "        return  logits # model : [batch_size, num_classes], attention : [batch_size, n_step]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drr20ukaKyL3"
      },
      "source": [
        "###LSTMSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGv0En-tKwD9"
      },
      "source": [
        "class LSTMAtt(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, out_size, max_length, n_layer = 1, dropout = 0.3):\n",
        "        super(LSTMAtt, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.max_length = max_length\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0).cuda()\n",
        "        self.n_layer = n_layer\n",
        "        print('Initialization LSTM Model')\n",
        "        self.rnn = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, dropout=dropout, num_layers=self.n_layer, bidirectional=False).cuda()\n",
        "        self.out = nn.Linear(hidden_dim, out_size).cuda()\n",
        "    def forward(self, X):\n",
        "        print(X.size())\n",
        "        input = self.embedding(X)\n",
        "        input = input.permute(1, 0, 2)\n",
        "        batch_size = input.size()[1]\n",
        "        hidden_state = Variable(\n",
        "            torch.randn(self.n_layer, batch_size, self.hidden_dim)).cuda()\n",
        "        cell_state = Variable(\n",
        "            torch.randn(self.n_layer, batch_size, self.hidden_dim)).cuda()\n",
        "        output, (final_hidden_state, final_cell_state) = self.rnn(input, (hidden_state, cell_state))\n",
        "        #output = output.permute(1, 0, 2).squeeze(1)\n",
        "        #output = output.reshape(batch_size, self.hidden_dim * self.max_length)\n",
        "        #print(final_hidden_state.size())\n",
        "        final_hidden_state = final_hidden_state.permute(1, 0, 2).squeeze(1)\n",
        "        output = self.out(final_hidden_state)\n",
        "\n",
        "        return  output # model : [batch_size, num_classes], attention : [batch_size, n_step]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8RY3HZ6RDVv"
      },
      "source": [
        "#train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGlI-sGkd23x"
      },
      "source": [
        "##RoBERTa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkEC3avHsBmD"
      },
      "source": [
        "###Hyperparameter Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h9Q_vursCkz"
      },
      "source": [
        "lr = 3e-5\n",
        "max_grad_norm = 1.0\n",
        "totalEpoch = 1\n",
        "eps = 1e-8\n",
        "batchSize = 16\n",
        "num_train_steps = totalEpoch\n",
        "WU = 0.2\n",
        "num_training_steps = ((len(data_id)/batchSize)+1) * totalEpoch\n",
        "num_warmup_steps = int(num_training_steps * WU)\n",
        "#scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_train_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao8ecDGbsOOq"
      },
      "source": [
        "###Optimizer&Scheduler&Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrg1wsJHsNgY"
      },
      "source": [
        "optimizer = AdamW(model.parameters(), lr = lr, eps = eps)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_train_steps, num_training_steps=num_training_steps)\n",
        "loss_fn = nn.MSELoss()\n",
        "#loss_fn = nn.CrossEntropyLoss()##Task2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW6DfzEaeXVj"
      },
      "source": [
        "###SingleHeadlineModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qt2KArW5oryE"
      },
      "source": [
        "device = torch.device(\"cuda\")\n",
        "model.to(device)\n",
        "for epoch in range(totalEpoch):\n",
        "  batchStart = 0\n",
        "  train_loss = 0\n",
        "  optimizer.zero_grad()\n",
        "  while batchStart <= len(data_meanGrade):\n",
        "    batchEnd = batchStart + batchSize\n",
        "    if batchEnd > len(data_meanGrade):\n",
        "      batchEnd = len(data_meanGrade)\n",
        "    encoding = tokenizer(data_replaced[batchStart:batchEnd], return_tensors='pt', padding=True, truncation=True)\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    labels = torch.tensor(data_meanGrade[batchStart:batchEnd]).to(device)\n",
        "    model.train()\n",
        "    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    #outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    #loss = torch.sqrt(loss_fn(outputs,labels))\n",
        "    loss = torch.sqrt(((outputs - labels)**2).mean())\n",
        "    train_loss += loss.item()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    #torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "    batchStart = batchStart + batchSize\n",
        "  with torch.no_grad():\n",
        "    encoding = tokenizer(val_replaced, return_tensors='pt', padding=True, truncation=True)\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    #outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    labels = torch.tensor(val_meanGrade).to(device)\n",
        "    loss = torch.sqrt(loss_fn(outputs,labels))\n",
        "    valid_loss = loss.item()\n",
        "    print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ThFQHdbeXVk"
      },
      "source": [
        "###EditedWordModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpGAC863f6wo"
      },
      "source": [
        "def getEditMatrix(tokenizer, encoding_sentence, encoding_index, replace_matrix, replace_words):\n",
        "  edit_matrix = torch.zeros(encoding_sentence.size()[0], 1, encoding_sentence.size()[1])\n",
        "  edit_total_num = torch.zeros(encoding_sentence.size()[0], 1, 1)\n",
        "  for batch_ in range(len(replace_words)):\n",
        "    token_list = tokenizer.convert_ids_to_tokens(encoding_index[batch_][1:-1])\n",
        "    token_str= \" \".join(token_list)\n",
        "    token_str = token_str.split('Ġ')\n",
        "    token_dict = dict()\n",
        "    for temp in token_str:\n",
        "      token_dict[temp.replace(' ','')] = ('Ġ' + temp).split(' ')\n",
        "    for key in token_dict:\n",
        "      if replace_words[batch_].lower() in key.lower() and float(len(replace_words[batch_])/len(key)) > 0.5:\n",
        "        for tok_num in range(len(token_dict[key])):\n",
        "          if len(token_dict[key]) == 2 and token_dict[key][0].replace('Ġ','') in token_list:\n",
        "            edit_matrix[batch_][0][encoding_index[batch_].index(tokenizer.convert_tokens_to_ids(token_dict[key][0].replace('Ġ','')))] = 1\n",
        "            continue\n",
        "          if token_dict[key][tok_num] == '':\n",
        "            continue\n",
        "          edit_token_ids = tokenizer.convert_tokens_to_ids(token_dict[key][tok_num])\n",
        "          edit_matrix[batch_][0][encoding_index[batch_].index(tokenizer.convert_tokens_to_ids(token_dict[key][tok_num]))] = 1\n",
        "  for batch_ in range(edit_matrix.size()[0]):\n",
        "    edit_total_num[batch_][0][0] = edit_matrix[batch_].sum()\n",
        "  return edit_matrix, edit_total_num\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)\n",
        "for epoch in range(totalEpoch):\n",
        "  batchStart = 0\n",
        "  train_loss = 0\n",
        "  optimizer.zero_grad()\n",
        "  while batchStart <= len(data_meanGrade):\n",
        "    batchEnd = batchStart + batchSize\n",
        "    if batchEnd > len(data_meanGrade):\n",
        "      batchEnd = len(data_meanGrade)\n",
        "    encoding = tokenizer(data_replaced[batchStart:batchEnd], return_tensors='pt', padding=True, truncation=True)\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    labels = torch.tensor(data_meanGrade[batchStart:batchEnd]).to(device)\n",
        "    encoding_index = tokenizer(data_replaced[batchStart:batchEnd], add_special_tokens=data_editword)['input_ids']\n",
        "    replace_words = data_editword[batchStart:batchEnd]\n",
        "    replace_matrix = tokenizer(replace_words, return_tensors='pt', padding=True, truncation=True, add_special_tokens=data_editword)['input_ids']\n",
        "    edit_matrix, edit_total_num = getEditMatrix(tokenizer,encoding['input_ids'],encoding_index,replace_matrix,replace_words)\n",
        "    model.train()\n",
        "    #outputs = model(input_ids, attention_mask=attention_mask)[0]\n",
        "    outputs = model(input_ids, attention_mask=attention_mask,edit_matrix=edit_matrix.to(device), edit_total_num=edit_total_num.to(device))\n",
        "    #loss = torch.sqrt(loss_fn(outputs,labels))\n",
        "    loss = torch.sqrt(((outputs - labels)**2).mean())\n",
        "    train_loss += loss.item()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    #torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "    batchStart = batchStart + batchSize\n",
        "    \n",
        "  with torch.no_grad():\n",
        "    encoding = tokenizer(val_replaced, return_tensors='pt', padding=True, truncation=True)\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    #outputs = model(input_ids, attention_mask=attention_mask)[0]\n",
        "    encoding_index = tokenizer(val_replaced, add_special_tokens=val_editword)['input_ids']\n",
        "    replace_words = val_editword\n",
        "    replace_matrix = tokenizer(replace_words, return_tensors='pt', padding=True, truncation=True, add_special_tokens=replace_words)['input_ids']\n",
        "    edit_matrix, edit_total_num = getEditMatrix(tokenizer,encoding['input_ids'],encoding_index,replace_matrix,replace_words)\n",
        "    outputs = model(input_ids, attention_mask=attention_mask,edit_matrix=edit_matrix.to(device), edit_total_num=edit_total_num.to(device))\n",
        "    labels = torch.tensor(val_meanGrade).to(device)\n",
        "    #loss = torch.sqrt(loss_fn(outputs,labels))\n",
        "    loss = torch.sqrt(((outputs - labels)**2).mean())\n",
        "    valid_loss = loss.item()\n",
        "    print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlzTbeMpeXVk"
      },
      "source": [
        "###TwoHeadlineModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmK5Rnlio-UP"
      },
      "source": [
        "device = torch.device(\"cuda\")\n",
        "model.to(device)\n",
        "for epoch in range(totalEpoch):\n",
        "  batchStart = 0\n",
        "  train_loss = 0\n",
        "  optimizer.zero_grad()\n",
        "  while batchStart <= len(data_meanGrade):\n",
        "    batchEnd = batchStart + batchSize\n",
        "    if batchEnd > len(data_meanGrade):\n",
        "      batchEnd = len(data_meanGrade)\n",
        "    old_title = data_orginal[batchStart:batchEnd]\n",
        "    new_title = data_replaced[batchStart:batchEnd]\n",
        "    encoding_old = tokenizer(old_title, return_tensors='pt', padding=True, truncation=True)\n",
        "    encoding_new = tokenizer(new_title, return_tensors='pt', padding=True, truncation=True)\n",
        "    input_ids_old = encoding_old['input_ids'].to(device)\n",
        "    input_ids_new = encoding_new['input_ids'].to(device)\n",
        "    attention_mask_old = encoding_old['attention_mask'].to(device)\n",
        "    attention_mask_new = encoding_new['attention_mask'].to(device)\n",
        "    labels = torch.tensor(data_meanGrade[batchStart:batchEnd]).to(device)\n",
        "    model.train()\n",
        "    #outputs = model(input_ids_new, input_ids_old, attention_mask_new, attention_mask_old)[0]\n",
        "    outputs = model(input_ids_new, attention_mask_new, input_ids_old, attention_mask_old)\n",
        "    #loss = torch.sqrt(loss_fn(outputs,labels))\n",
        "    loss = torch.sqrt(((outputs - labels)**2).mean())\n",
        "    train_loss += loss.item()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    #torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "    batchStart = batchStart + batchSize\n",
        "    \n",
        "  with torch.no_grad():\n",
        "    new_title = val_replaced\n",
        "    old_title = val_orginal\n",
        "    encoding_old = tokenizer(old_title, return_tensors='pt', padding=True, truncation=True)\n",
        "    encoding_new = tokenizer(new_title, return_tensors='pt', padding=True, truncation=True)\n",
        "    input_ids_old = encoding_old['input_ids'].to(device)\n",
        "    input_ids_new = encoding_new['input_ids'].to(device)\n",
        "    attention_mask_old = encoding_old['attention_mask'].to(device)\n",
        "    attention_mask_new = encoding_new['attention_mask'].to(device)\n",
        "    #outputs = model(input_ids_new, input_ids_old, attention_mask_new, attention_mask_old)[0]\n",
        "    outputs = model(input_ids_new, attention_mask_new, input_ids_old, attention_mask_old)\n",
        "    labels = torch.tensor(val_meanGrade).to(device)\n",
        "    loss = torch.sqrt(loss_fn(outputs,labels))\n",
        "    valid_loss = loss.item()\n",
        "    print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TQAmWkR8tdC"
      },
      "source": [
        "outputs.size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Il8XCxyXeXVk"
      },
      "source": [
        "###SequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhXjKtfjpQ8T"
      },
      "source": [
        "device = torch.device(\"cuda\")\n",
        "model.to(device)\n",
        "for epoch in range(totalEpoch):\n",
        "  batchStart = 0\n",
        "  train_loss = 0\n",
        "  optimizer.zero_grad()\n",
        "  while batchStart <= len(data_meanGrade):\n",
        "    batchEnd = batchStart + batchSize\n",
        "    if batchEnd > len(data_meanGrade):\n",
        "      batchEnd = len(data_meanGrade)\n",
        "    input_title = data_replaced[batchStart:batchEnd]\n",
        "    input_word = data_editword[batchStart:batchEnd]\n",
        "    input_data = [[input_title[num_],input_word[num_]] for num_ in range(len(input_title))]\n",
        "    encoding = tokenizer(input_data, return_tensors='pt', padding=True, truncation=True)\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    labels = torch.tensor(data_meanGrade[batchStart:batchEnd]).to(device)\n",
        "    model.train()\n",
        "    outputs = model(input_ids, attention_mask=attention_mask)[0]\n",
        "    #outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    loss = torch.sqrt(loss_fn(outputs,labels))\n",
        "    #loss = 2*torch.sqrt(((outputs - labels)**2).mean())\n",
        "    train_loss += loss.item()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    #torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "    batchStart = batchStart + batchSize\n",
        "    \n",
        "  with torch.no_grad():\n",
        "    input_title = val_replaced\n",
        "    input_word = val_editword\n",
        "    input_data = [[input_title[num_],input_word[num_]] for num_ in range(len(input_title))]\n",
        "    encoding = tokenizer(input_data, return_tensors='pt', padding=True, truncation=True)\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    outputs = model(input_ids, attention_mask=attention_mask)[0]\n",
        "    #outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    labels = torch.tensor(val_meanGrade).to(device)\n",
        "    loss = torch.sqrt(loss_fn(outputs,labels))\n",
        "    valid_loss = loss.item()\n",
        "    print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VnMxLMkp7hs"
      },
      "source": [
        "##BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9xzVy3ip7hu"
      },
      "source": [
        "###Hyperparameter Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CBYFh0jp7hv"
      },
      "source": [
        "lr = 3e-5\n",
        "max_grad_norm = 1.0\n",
        "totalEpoch = 1\n",
        "eps = 1e-8\n",
        "batchSize = 16\n",
        "num_train_steps = totalEpoch\n",
        "WU = 0.2\n",
        "num_training_steps = ((len(data_id)/batchSize)+1) * totalEpoch\n",
        "num_warmup_steps = int(num_training_steps * WU)\n",
        "#scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_train_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-FJ4cPsp7hv"
      },
      "source": [
        "###Optimizer&Scheduler&Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hnNJi8tp7hv"
      },
      "source": [
        "optimizer = AdamW(model.parameters(), lr = lr, eps = eps)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_train_steps, num_training_steps=num_training_steps)\n",
        "loss_fn = nn.MSELoss()\n",
        "#loss_fn = nn.CrossEntropyLoss()##Task2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpMqsZfYp7hv"
      },
      "source": [
        "###SingleHeadlineModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts2rfhSGp7hv"
      },
      "source": [
        "device = torch.device(\"cuda\")\n",
        "model.to(device)\n",
        "for epoch in range(totalEpoch):\n",
        "  batchStart = 0\n",
        "  train_loss = 0\n",
        "  optimizer.zero_grad()\n",
        "  while batchStart <= len(data_meanGrade):\n",
        "    batchEnd = batchStart + batchSize\n",
        "    if batchEnd > len(data_meanGrade):\n",
        "      batchEnd = len(data_meanGrade)\n",
        "    encoding = tokenizer(data_replaced[batchStart:batchEnd], return_tensors='pt', padding=True, truncation=True)\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    labels = torch.tensor(data_meanGrade[batchStart:batchEnd]).to(device)\n",
        "    model.train()\n",
        "    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    #outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    #loss = torch.sqrt(loss_fn(outputs,labels))\n",
        "    loss = torch.sqrt(((outputs - labels)**2).mean())\n",
        "    train_loss += loss.item()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    #torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "    batchStart = batchStart + batchSize\n",
        "  with torch.no_grad():\n",
        "    encoding = tokenizer(val_replaced, return_tensors='pt', padding=True, truncation=True)\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    #outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    labels = torch.tensor(val_meanGrade).to(device)\n",
        "    loss = torch.sqrt(loss_fn(outputs,labels))\n",
        "    valid_loss = loss.item()\n",
        "    print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yROzihy1iVS"
      },
      "source": [
        "input_ids.size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfWrVfKz1l2Z"
      },
      "source": [
        "attention_mask.size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpTWM1Oyp7hw"
      },
      "source": [
        "###EditedWordModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qx4IGvgop7hw"
      },
      "source": [
        "def getEditMatrix(tokenizer, encoding_sentence, encoding_index, replace_matrix, replace_words):\n",
        "  edit_matrix = torch.zeros(encoding_sentence.size()[0], 1, encoding_sentence.size()[1])\n",
        "  edit_total_num = torch.zeros(encoding_sentence.size()[0], 1, 1)\n",
        "  for batch_ in range(len(replace_words)):\n",
        "    token_list = tokenizer.convert_ids_to_tokens(encoding_index[batch_][1:-1])\n",
        "    token_str= \" \".join(token_list)\n",
        "    token_str = token_str.split('Ġ')\n",
        "    token_dict = dict()\n",
        "    for temp in token_str:\n",
        "      token_dict[temp.replace(' ','')] = ('Ġ' + temp).split(' ')\n",
        "    for key in token_dict:\n",
        "      if replace_words[batch_].lower() in key.lower() and float(len(replace_words[batch_])/len(key)) > 0.5:\n",
        "        for tok_num in range(len(token_dict[key])):\n",
        "          if len(token_dict[key]) == 2 and token_dict[key][0].replace('Ġ','') in token_list:\n",
        "            edit_matrix[batch_][0][encoding_index[batch_].index(tokenizer.convert_tokens_to_ids(token_dict[key][0].replace('Ġ','')))] = 1\n",
        "            continue\n",
        "          if token_dict[key][tok_num] == '':\n",
        "            continue\n",
        "          edit_token_ids = tokenizer.convert_tokens_to_ids(token_dict[key][tok_num])\n",
        "          edit_matrix[batch_][0][encoding_index[batch_].index(tokenizer.convert_tokens_to_ids(token_dict[key][tok_num]))] = 1\n",
        "  for batch_ in range(edit_matrix.size()[0]):\n",
        "    edit_total_num[batch_][0][0] = edit_matrix[batch_].sum()\n",
        "  return edit_matrix, edit_total_num\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)\n",
        "for epoch in range(totalEpoch):\n",
        "  batchStart = 0\n",
        "  train_loss = 0\n",
        "  optimizer.zero_grad()\n",
        "  while batchStart <= len(data_meanGrade):\n",
        "    batchEnd = batchStart + batchSize\n",
        "    if batchEnd > len(data_meanGrade):\n",
        "      batchEnd = len(data_meanGrade)\n",
        "    encoding = tokenizer(data_replaced[batchStart:batchEnd], return_tensors='pt', padding=True, truncation=True)\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    labels = torch.tensor(data_meanGrade[batchStart:batchEnd]).to(device)\n",
        "    encoding_index = tokenizer(data_replaced[batchStart:batchEnd], add_special_tokens=data_editword)['input_ids']\n",
        "    replace_words = data_editword[batchStart:batchEnd]\n",
        "    replace_matrix = tokenizer(replace_words, return_tensors='pt', padding=True, truncation=True, add_special_tokens=data_editword)['input_ids']\n",
        "    edit_matrix, edit_total_num = getEditMatrix(tokenizer,encoding['input_ids'],encoding_index,replace_matrix,replace_words)\n",
        "    model.train()\n",
        "    #outputs = model(input_ids, attention_mask=attention_mask)[0]\n",
        "    outputs = model(input_ids, attention_mask=attention_mask,edit_matrix=edit_matrix.to(device), edit_total_num=edit_total_num.to(device))\n",
        "    #loss = torch.sqrt(loss_fn(outputs,labels))\n",
        "    loss = torch.sqrt(((outputs - labels)**2).mean())\n",
        "    train_loss += loss.item()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    #torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "    batchStart = batchStart + batchSize\n",
        "    \n",
        "  with torch.no_grad():\n",
        "    encoding = tokenizer(val_replaced, return_tensors='pt', padding=True, truncation=True)\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    #outputs = model(input_ids, attention_mask=attention_mask)[0]\n",
        "    encoding_index = tokenizer(val_replaced, add_special_tokens=val_editword)['input_ids']\n",
        "    replace_words = val_editword\n",
        "    replace_matrix = tokenizer(replace_words, return_tensors='pt', padding=True, truncation=True, add_special_tokens=replace_words)['input_ids']\n",
        "    edit_matrix, edit_total_num = getEditMatrix(tokenizer,encoding['input_ids'],encoding_index,replace_matrix,replace_words)\n",
        "    outputs = model(input_ids, attention_mask=attention_mask,edit_matrix=edit_matrix.to(device), edit_total_num=edit_total_num.to(device))\n",
        "    labels = torch.tensor(val_meanGrade).to(device)\n",
        "    #loss = torch.sqrt(loss_fn(outputs,labels))\n",
        "    loss = torch.sqrt(((outputs - labels)**2).mean())\n",
        "    valid_loss = loss.item()\n",
        "    print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D03snoXp7hw"
      },
      "source": [
        "###TwoHeadlineModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpjOXOTJp7hw"
      },
      "source": [
        "device = torch.device(\"cuda\")\n",
        "model.to(device)\n",
        "for epoch in range(totalEpoch):\n",
        "  batchStart = 0\n",
        "  train_loss = 0\n",
        "  optimizer.zero_grad()\n",
        "  while batchStart <= len(data_meanGrade):\n",
        "    batchEnd = batchStart + batchSize\n",
        "    if batchEnd > len(data_meanGrade):\n",
        "      batchEnd = len(data_meanGrade)\n",
        "    old_title = data_orginal[batchStart:batchEnd]\n",
        "    new_title = data_replaced[batchStart:batchEnd]\n",
        "    encoding_old = tokenizer(old_title, return_tensors='pt', padding=True, truncation=True)\n",
        "    encoding_new = tokenizer(new_title, return_tensors='pt', padding=True, truncation=True)\n",
        "    input_ids_old = encoding_old['input_ids'].to(device)\n",
        "    input_ids_new = encoding_new['input_ids'].to(device)\n",
        "    attention_mask_old = encoding_old['attention_mask'].to(device)\n",
        "    attention_mask_new = encoding_new['attention_mask'].to(device)\n",
        "    labels = torch.tensor(data_meanGrade[batchStart:batchEnd]).to(device)\n",
        "    model.train()\n",
        "    #outputs = model(input_ids_new, input_ids_old, attention_mask_new, attention_mask_old)[0]\n",
        "    outputs = model(input_ids_new, attention_mask_new, input_ids_old, attention_mask_old)\n",
        "    #loss = torch.sqrt(loss_fn(outputs,labels))\n",
        "    loss = torch.sqrt(((outputs - labels)**2).mean())\n",
        "    train_loss += loss.item()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    #torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "    batchStart = batchStart + batchSize\n",
        "    \n",
        "  with torch.no_grad():\n",
        "    new_title = val_replaced\n",
        "    old_title = val_orginal\n",
        "    encoding_old = tokenizer(old_title, return_tensors='pt', padding=True, truncation=True)\n",
        "    encoding_new = tokenizer(new_title, return_tensors='pt', padding=True, truncation=True)\n",
        "    input_ids_old = encoding_old['input_ids'].to(device)\n",
        "    input_ids_new = encoding_new['input_ids'].to(device)\n",
        "    attention_mask_old = encoding_old['attention_mask'].to(device)\n",
        "    attention_mask_new = encoding_new['attention_mask'].to(device)\n",
        "    #outputs = model(input_ids_new, input_ids_old, attention_mask_new, attention_mask_old)[0]\n",
        "    outputs = model(input_ids_new, attention_mask_new, input_ids_old, attention_mask_old)\n",
        "    labels = torch.tensor(val_meanGrade).to(device)\n",
        "    loss = torch.sqrt(loss_fn(outputs,labels))\n",
        "    valid_loss = loss.item()\n",
        "    print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erlOsq6lp7hw"
      },
      "source": [
        "outputs.size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnzO4_opp7hx"
      },
      "source": [
        "###SequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMqD_XJip7hx"
      },
      "source": [
        "device = torch.device(\"cuda\")\n",
        "model.to(device)\n",
        "for epoch in range(totalEpoch):\n",
        "  batchStart = 0\n",
        "  train_loss = 0\n",
        "  optimizer.zero_grad()\n",
        "  while batchStart <= len(data_meanGrade):\n",
        "    batchEnd = batchStart + batchSize\n",
        "    if batchEnd > len(data_meanGrade):\n",
        "      batchEnd = len(data_meanGrade)\n",
        "    input_title = data_replaced[batchStart:batchEnd]\n",
        "    input_word = data_editword[batchStart:batchEnd]\n",
        "    input_data = [[input_title[num_],input_word[num_]] for num_ in range(len(input_title))]\n",
        "    encoding = tokenizer(input_data, return_tensors='pt', padding=True, truncation=True)\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    labels = torch.tensor(data_meanGrade[batchStart:batchEnd]).to(device)\n",
        "    model.train()\n",
        "    outputs = model(input_ids, attention_mask=attention_mask)[0]\n",
        "    #outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    loss = torch.sqrt(loss_fn(outputs,labels))\n",
        "    #loss = 2*torch.sqrt(((outputs - labels)**2).mean())\n",
        "    train_loss += loss.item()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    #torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "    batchStart = batchStart + batchSize\n",
        "    \n",
        "  with torch.no_grad():\n",
        "    input_title = val_replaced\n",
        "    input_word = val_editword\n",
        "    input_data = [[input_title[num_],input_word[num_]] for num_ in range(len(input_title))]\n",
        "    encoding = tokenizer(input_data, return_tensors='pt', padding=True, truncation=True)\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    outputs = model(input_ids, attention_mask=attention_mask)[0]\n",
        "    #outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    labels = torch.tensor(val_meanGrade).to(device)\n",
        "    loss = torch.sqrt(loss_fn(outputs,labels))\n",
        "    valid_loss = loss.item()\n",
        "    print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Wlt1OCilBxz"
      },
      "source": [
        "##LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVD0AKlLlKQ-"
      },
      "source": [
        "###Original Setting\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4D6Mx_Q6lO13"
      },
      "source": [
        "EMBEDDING_DIM = 300\n",
        "\n",
        "SCALE_EMBS = 1\n",
        "\n",
        "hidden_dim = 20\n",
        "\n",
        "out_size = 1\n",
        "\n",
        "def fix_seed(seed=234):\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  random.seed(seed)\n",
        "fix_seed()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7F2ER7hSNsV"
      },
      "source": [
        "###PreProcessMethod"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ktu8Is3ZSRHS"
      },
      "source": [
        "def pre_process(sentence):\n",
        "  sentence_temp = sentence.lower()\n",
        "  sentence_tokenize = word_tokenize(sentence)\n",
        "  sentence_tokenize = nltk.pos_tag(sentence_tokenize)\n",
        "  pos_sentence = []\n",
        "  for token in sentence_tokenize: \n",
        "    if token[1] == \"NNP\" or token[1] == \"NN\":\n",
        "      pos_sentence.append(token[0].capitalize())\n",
        "    else:\n",
        "      pos_sentence.append(token[0])\n",
        "  stop_word_removed_sentence = []\n",
        "  for token in pos_sentence:\n",
        "    if token not in stop_words_list:\n",
        "      stop_word_removed_sentence.append(token)\n",
        "  return_sentence = \" \".join(stop_word_removed_sentence)\n",
        "  return return_sentence\n",
        "\n",
        "def getCorpus(data_address):\n",
        "  data_replaced = list()\n",
        "  data_id = list()\n",
        "  labels = list()\n",
        "  file = open(data_address,'r',encoding='utf-8')\n",
        "  file_read = file.readlines()\n",
        "  flag = 0\n",
        "  for line in file_read:\n",
        "    if flag == 0:\n",
        "      flag = 1\n",
        "      continue\n",
        "    line = line.replace('\\n','').replace('\\r','')\n",
        "    line = line.split(',')\n",
        "    if len(line) > 5:\n",
        "      line_temp = list()\n",
        "      line_temp.append(line[0])\n",
        "      line_content = ','.join(line[1:-3])\n",
        "      line_content_temp = line_content.split(' ')\n",
        "      for num in range(len(line_content_temp)):\n",
        "        if '<' in line_content_temp[num] and '/>' in line_content_temp[num]:\n",
        "          line_content_temp[num] = line[-3]\n",
        "      line_content = ' '.join(line_content_temp)\n",
        "      line_content = pre_process(line_content)\n",
        "      line_temp.append(line_content)\n",
        "      line_temp.append(line[-3])\n",
        "      line_temp.append(line[-2])\n",
        "      line_temp.append(line[-1])\n",
        "      line = line_temp\n",
        "    else:\n",
        "      line_content_temp = line[1].split(' ')\n",
        "      for num in range(len(line_content_temp)):\n",
        "        if '<' in line_content_temp[num] and '/>' in line_content_temp[num]:\n",
        "          line_content_temp[num] = line[-3]\n",
        "      line_content = ' '.join(line_content_temp)\n",
        "      line[1] = line_content\n",
        "    line_temp = line[1].replace(' ','')\n",
        "    if '<' in line_temp and '/>' in line_temp:\n",
        "      p = r\"(?<=<).+?(?=/>)\" \n",
        "      line[1] = line[1].replace('/ >','/>')\n",
        "      line[1] = re.sub(p, line[2], line[1])\n",
        "      line[1] = line[1].replace('<','').replace('/>','')\n",
        "      line[1] = pre_process(line[1])\n",
        "    #orginal_data.append(line)\n",
        "    data_id.append(line[0])\n",
        "    data_replaced.append(line[1])\n",
        "    labels.append(float(line[4]))\n",
        "  file.close()\n",
        "  return data_id, data_replaced, labels\n",
        "def average_error(output, target):\n",
        "  output = torch.round(torch.sigmoid(output))\n",
        "  err_num = 0\n",
        "  err_mean = 0.0\n",
        "  err_total = 0.0\n",
        "  for num in range(len(output)):\n",
        "    err_total = err_total + abs(output[num].item() - target[num])\n",
        "  err_mean = float(err_total/len(output))\n",
        "  return err_mean\n",
        "def get_tokenized_corpus(corpus):\n",
        "  tokenized_corpus = []\n",
        "  for sent in corpus:\n",
        "    tokenised_sentence = []\n",
        "    for token in sent.split(' '):\n",
        "      tokenised_sentence.append(token)\n",
        "    tokenized_corpus.append(tokenised_sentence)\n",
        "  return tokenized_corpus\n",
        "def get_word2idx(tokenized_corpus):\n",
        "  vocabulary = []\n",
        "  for sentence in tokenized_corpus:\n",
        "    for token in sentence:\n",
        "        if token not in vocabulary:\n",
        "            vocabulary.append(token)\n",
        "  \n",
        "  word2idx = {w: idx+1 for (idx, w) in enumerate(vocabulary)}\n",
        "  # we reserve the 0 index for the padding token\n",
        "  word2idx['<pad>'] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7fvnLqaSciT"
      },
      "source": [
        "def getTokenizedCorpus(corpus):\n",
        "  tokenized_corpus = []\n",
        "  for sentence in corpus:\n",
        "    tokenized_sentence = []\n",
        "    for token in sentence.split(' '): \n",
        "      tokenized_sentence.append(token)\n",
        "    tokenized_corpus.append(tokenized_sentence)\n",
        "  return tokenized_corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PkmwsotLgt-"
      },
      "source": [
        "###GetData"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwXDTkUcYD4v"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "# Unzip the file: 4 different embedding sizes are provided\n",
        "!unzip glove.6B.zip\n",
        "# Check the file format\n",
        "!head -n10 glove.6B.50d.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKGU06nrX0-l"
      },
      "source": [
        "train_id, train, train_labels = getCorpus('/content/drive/My Drive/Colab Notebooks/semeval-2020-task-7-dataset/subtask-1/train.csv')\n",
        "test_id, test, test_labels = getCorpus('/content/drive/My Drive/Colab Notebooks/semeval-2020-task-7-dataset/subtask-1/test.csv')\n",
        "valid_id, valid, valid_labels = getCorpus('/content/drive/My Drive/Colab Notebooks/semeval-2020-task-7-dataset/subtask-1/train.csv')\n",
        "\n",
        "train = getTokenizedCorpus(train)\n",
        "test = getTokenizedCorpus(test)\n",
        "valid = getTokenizedCorpus(valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iESv-Z10lTJn"
      },
      "source": [
        "###WordVector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YYPK843lVMd"
      },
      "source": [
        "def getWord2idx(tokenized_corpus):\n",
        "  vocabulary = []\n",
        "  for sentence in tokenized_corpus:\n",
        "    for token in sentence:\n",
        "        if token not in vocabulary:\n",
        "            vocabulary.append(token)\n",
        "  word2idx = {w: idx+1 for (idx, w) in enumerate(vocabulary)}\n",
        "  word2idx['<pad>'] = 0\n",
        "  return word2idx\n",
        "def encodeData(tokenized_corpus, word2idx):\n",
        "  enocded_corpus = list()\n",
        "  for sentence in tokenized_corpus:\n",
        "    encoded_sentence = list()\n",
        "    for token in sentence:\n",
        "      encoded_sentence.append(word2idx[token])\n",
        "    enocded_corpus.append(encoded_sentence.copy())\n",
        "  return enocded_corpus\n",
        "\n",
        "word2idx = getWord2idx(train + test + valid)\n",
        "wvecs = np.zeros((len(word2idx), EMBEDDING_DIM), dtype='float32')\n",
        "\n",
        "with open(f'glove.6B.{EMBEDDING_DIM}d.txt', 'r') as f:\n",
        "  for line in tqdm(f):\n",
        "    if len(line.strip().split()) > 3:\n",
        "      word = line.strip().split()[0]\n",
        "      if word in word2idx:\n",
        "        (word, vec) = (word, list(map(float, line.strip().split()[1:])))\n",
        "        idx = word2idx[word]\n",
        "        wvecs[idx] = vec\n",
        "\n",
        "wvecs = wvecs * SCALE_EMBS\n",
        "wvecs = np.zeros((len(word2idx), EMBEDDING_DIM), dtype='float32')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEGVpwfRhyDw"
      },
      "source": [
        "###LSTMSingleHeadlineModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIKElZdmr5gQ"
      },
      "source": [
        "####ProcessData"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtqrG2I9r5gQ"
      },
      "source": [
        "def generateBatchData(data_id, input_data, data_labels, batch_size):\n",
        "  batch_data = list()\n",
        "  single_batch_data = list()\n",
        "  single_batch_label = list()\n",
        "  single_batch_id = list()\n",
        "  max_length = 0\n",
        "  for num in range(len(input_data)):\n",
        "    if len(single_batch_data) < batch_size:\n",
        "      single_batch_data.append(input_data[num].copy())\n",
        "      single_batch_label.append(data_labels[num])\n",
        "      single_batch_id.append(data_id[num])\n",
        "      if len(input_data[num]) > max_length:\n",
        "        max_length = len(input_data[num])\n",
        "    else:\n",
        "      for num in range(len(single_batch_data)):\n",
        "        while len(single_batch_data[num]) < max_length:\n",
        "          single_batch_data[num].append(0)\n",
        "      batch_data.append((single_batch_id.copy(), single_batch_data.copy(), single_batch_label.copy()))\n",
        "      single_batch_data = list()\n",
        "      single_batch_label = list()\n",
        "      single_batch_id = list()\n",
        "      max_length = 0\n",
        "      single_batch_data.append(input_data[num].copy())\n",
        "      single_batch_label.append(data_labels[num])\n",
        "      single_batch_id.append(data_id[num])\n",
        "      if len(input_data[num]) > max_length:\n",
        "        max_length = len(input_data[num])\n",
        "  if len(single_batch_data) >0:\n",
        "    for num in range(len(single_batch_data)):\n",
        "        while len(single_batch_data[num]) < max_length:\n",
        "          single_batch_data[num].append(0)\n",
        "    batch_data.append((single_batch_id.copy(), single_batch_data.copy(), single_batch_label.copy()))\n",
        "  return batch_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJObCgN1r5gR"
      },
      "source": [
        "def get_model_inputs(tokenized_corpus, word2idx, labels, max_len = 0):\n",
        "  # we index our sentences\n",
        "  vectorized_sents = [[word2idx[tok] for tok in sent if tok in word2idx] for sent in tokenized_corpus]\n",
        "  # Sentence lengths\n",
        "  sent_lengths = [len(sent) for sent in vectorized_sents]\n",
        "  # Get maximum length\n",
        "  max_len = max(sent_lengths)\n",
        "  # if max_len == 0:\n",
        "  #   max_len = int(max(sent_lengths))\n",
        "  # we create a tensor of a fixed size filled with zeroes for padding\n",
        "  sent_tensor = torch.zeros((len(vectorized_sents), max_len)).long().cuda()\n",
        "  # we fill it with our vectorized sentences \n",
        "  for idx, (sent, sentlen) in enumerate(zip(vectorized_sents, sent_lengths)):\n",
        "    sent_tensor[idx, :sentlen] = torch.LongTensor(sent).cuda()\n",
        "  # Label tensor\n",
        "  label_tensor = torch.FloatTensor(labels).cuda()\n",
        "  \n",
        "  return sent_tensor, label_tensor, max_len\n",
        "train_sent_tensor, train_label_tensor, max_len = get_model_inputs(train, word2idx, train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xiu8pRZir5gR"
      },
      "source": [
        "encoded_train = encodeData(train, word2idx)\n",
        "encoded_valid = encodeData(valid, word2idx)\n",
        "encoded_test = encodeData(test, word2idx)\n",
        "\n",
        "batch_train = generateBatchData(train_id, encoded_train, train_labels, 24)\n",
        "batch_test = generateBatchData(test_id, encoded_test, test_labels, 24)\n",
        "batch_valid = generateBatchData(valid_id, encoded_valid, valid_labels, 24)\n",
        "\n",
        "train_sent_tensor, train_label_tensor, max_len = get_model_inputs(train, word2idx, train_labels)\n",
        "test_sent_tensor, test_label_tensor, max_len = get_model_inputs(test, word2idx, test_labels, max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vklTpLCEr5gR"
      },
      "source": [
        "####Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uoluiNir5gR"
      },
      "source": [
        "model = LSTMSingleHeadlineModel(len(wvecs), EMBEDDING_DIM, hidden_dim, out_size, max_length = train_sent_tensor.size()[1] , n_layer = 1, dropout = 0.3, pool_feature_num=200)\n",
        "model.embedding.weight.data.copy_(torch.from_numpy(wvecs))#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jRL3XLIr5gR"
      },
      "source": [
        "LRATE = 0.001\n",
        "EPOCHS = 60\n",
        "optimizer = optim.Adam(model.parameters(), lr=LRATE)\n",
        "loss_fn = nn.MSELoss()\n",
        "feature_train = train_sent_tensor\n",
        "target_train = train_label_tensor\n",
        "\n",
        "feature_valid = test_sent_tensor\n",
        "target_valid = test_label_tensor\n",
        "\n",
        "\n",
        "feature_test = test_sent_tensor\n",
        "target_test = test_label_tensor\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  model.train()\n",
        "  \n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  predictions = model(feature_train).squeeze(1)\n",
        "  loss = torch.sqrt(loss_fn(predictions, target_train))\n",
        "  train_loss = loss.item()\n",
        "  train_ave_error = average_error(predictions, target_train)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    predictions_valid = model(feature_valid).squeeze(1)\n",
        "    valid_loss = torch.sqrt(loss_fn(predictions_valid, target_valid)).item()\n",
        "    valid_ave_error = average_error(predictions_valid, target_valid)\n",
        "  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Train ave_error: {train_ave_error} | Val. Loss: {valid_loss:.3f} | Val. ave_error: {valid_ave_error} |')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRnK1cDjr8LF"
      },
      "source": [
        "####Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzHQZ5jMr8LF"
      },
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  file = open('./LSTMSingleHeadlineModel.csv','w',encoding='utf-8')\n",
        "  file.writelines('id,pred\\n')\n",
        "  for num in range(len(encoded_test)):\n",
        "    while len(encoded_test[num]) < 27:\n",
        "      encoded_test[num].append(0)\n",
        "    feature_test = torch.LongTensor([encoded_test[num]]).cuda()\n",
        "    target_test = torch.FloatTensor([test_labels[num]]).cuda()\n",
        "    predictions = model(feature_test)\n",
        "    loss = torch.sqrt(loss_fn(predictions, target_test))\n",
        "    ave_error = average_error(predictions, target_test)\n",
        "    str_content = test_id[num]  + ',' + str(abs(predictions.item())) + '\\n'\n",
        "    file.writelines(str_content)\n",
        "  file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pkC-g34SJbK"
      },
      "source": [
        "###LSTMEditedWordModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9ATlM-oSd4S"
      },
      "source": [
        "####GetData"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPBx9Xd6SpRZ"
      },
      "source": [
        "def getCorpus(data_address):\n",
        "  data_replaced = list()\n",
        "  data_id = list()\n",
        "  labels = list()\n",
        "  data_replace_num = list()\n",
        "  file = open(data_address,'r',encoding='utf-8')\n",
        "  file_read = file.readlines()\n",
        "  flag = 0\n",
        "  for line in file_read:\n",
        "    if flag == 0:\n",
        "      flag = 1\n",
        "      continue\n",
        "    line = line.replace('\\n','').replace('\\r','')\n",
        "    line = line.split(',')\n",
        "    if len(line) > 5:\n",
        "      line_temp = list()\n",
        "      line_temp.append(line[0])\n",
        "      line_content = ','.join(line[1:-3])\n",
        "      line_content_temp = line_content.split(' ')\n",
        "      for num in range(len(line_content_temp)):\n",
        "        if '<' in line_content_temp[num] and '/>' in line_content_temp[num]:\n",
        "          line_content_temp[num] = line[-3]\n",
        "      line_content = ' '.join(line_content_temp)\n",
        "      #line_content = pre_process(line_content)\n",
        "      line_temp.append(line_content)\n",
        "      line_temp.append(line[-3])\n",
        "      line_temp.append(line[-2])\n",
        "      line_temp.append(line[-1])\n",
        "      line = line_temp\n",
        "    else:\n",
        "      line_content_temp = line[1].split(' ')\n",
        "      for num in range(len(line_content_temp)):\n",
        "        if '<' in line_content_temp[num] and '/>' in line_content_temp[num]:\n",
        "          line_content_temp[num] = line[-3]\n",
        "      line_content = ' '.join(line_content_temp)\n",
        "      line[1] = line_content\n",
        "    line_temp = line[1].replace(' ','')\n",
        "    if '<' in line_temp and '/>' in line_temp:\n",
        "      p = r\"(?<=<).+?(?=/>)\" \n",
        "      line[1] = line[1].replace('/ >','/>')\n",
        "      line[1] = re.sub(p, line[2], line[1])\n",
        "      line[1] = line[1].replace('<','').replace('/>','')\n",
        "      #line[1] = pre_process(line[1])\n",
        "    #orginal_data.append(line)\n",
        "    data_id.append(line[0])\n",
        "    data_replaced.append(line[1])\n",
        "    labels.append(float(line[4]))\n",
        "    data_replace_num_temp = line[2].lower().split(' ')\n",
        "    data_replace_num_temp2 = list()\n",
        "    data_replaced_temp = line[1].lower().split(' ')\n",
        "    \n",
        "    if len(data_replace_num_temp)>1: \n",
        "      for word in data_replace_num_temp:\n",
        "        if word == '' or word == ' ':\n",
        "          continue\n",
        "        data_replace_num_temp2.append(data_replaced_temp.index(word))\n",
        "    elif len(data_replace_num_temp) == 1:\n",
        "      data_replace_num_temp2.append(data_replaced_temp.index(data_replace_num_temp[0])) \n",
        "    elif len(data_replace_num_temp) == 0:\n",
        "      print('&&&&&&&&&&&&&&&&&&')\n",
        "    data_replace_num.append(data_replace_num_temp2.copy())\n",
        "    if ' ' in line[2]:\n",
        "      print(line[1],'****',line[2],'$$$$$',data_replace_num_temp2)\n",
        "  file.close()\n",
        "  return data_id, data_replaced, data_replace_num, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaPv_2_PUZ4P"
      },
      "source": [
        "train_id, train, train_num, train_labels = getCorpus('/content/drive/My Drive/Colab Notebooks/semeval-2020-task-7-dataset/subtask-1/train.csv')\n",
        "test_id, test, test_num, test_labels = getCorpus('/content/drive/My Drive/Colab Notebooks/semeval-2020-task-7-dataset/subtask-1/test.csv')\n",
        "valid_id, valid, valid_num, valid_labels = getCorpus('/content/drive/My Drive/Colab Notebooks/semeval-2020-task-7-dataset/subtask-1/train.csv')\n",
        "\n",
        "train = getTokenizedCorpus(train)\n",
        "test = getTokenizedCorpus(test)\n",
        "valid = getTokenizedCorpus(valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oafuv2-xDVl0"
      },
      "source": [
        "###WordVector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1kpCD_nDVl2"
      },
      "source": [
        "def getWord2idx(tokenized_corpus):\n",
        "  vocabulary = []\n",
        "  for sentence in tokenized_corpus:\n",
        "    for token in sentence:\n",
        "        if token not in vocabulary:\n",
        "            vocabulary.append(token)\n",
        "  word2idx = {w: idx+1 for (idx, w) in enumerate(vocabulary)}\n",
        "  word2idx['<pad>'] = 0\n",
        "  return word2idx\n",
        "def encodeData(tokenized_corpus, word2idx):\n",
        "  enocded_corpus = list()\n",
        "  for sentence in tokenized_corpus:\n",
        "    encoded_sentence = list()\n",
        "    for token in sentence:\n",
        "      encoded_sentence.append(word2idx[token])\n",
        "    enocded_corpus.append(encoded_sentence.copy())\n",
        "  return enocded_corpus\n",
        "\n",
        "word2idx = getWord2idx(train + test + valid)\n",
        "wvecs = np.zeros((len(word2idx), EMBEDDING_DIM), dtype='float32')\n",
        "\n",
        "with open(f'glove.6B.{EMBEDDING_DIM}d.txt', 'r') as f:\n",
        "  for line in tqdm(f):\n",
        "    if len(line.strip().split()) > 3:\n",
        "      word = line.strip().split()[0]\n",
        "      if word in word2idx:\n",
        "        (word, vec) = (word, list(map(float, line.strip().split()[1:])))\n",
        "        idx = word2idx[word]\n",
        "        wvecs[idx] = vec\n",
        "\n",
        "wvecs = wvecs * SCALE_EMBS\n",
        "wvecs = np.zeros((len(word2idx), EMBEDDING_DIM), dtype='float32')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuMyirYPSP32"
      },
      "source": [
        "####ProcessData"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhxKypQcSP32"
      },
      "source": [
        "def generateBatchData(data_id, input_data, input_data_num, data_labels, batch_size):\n",
        "  batch_data = list()\n",
        "  single_batch_data = list()\n",
        "  single_batch_num = list()\n",
        "  single_batch_label = list()\n",
        "  single_batch_id = list()\n",
        "  max_length = 0\n",
        "  for num in range(len(input_data)):\n",
        "    if len(single_batch_data) < batch_size:\n",
        "      single_batch_data.append(input_data[num].copy())\n",
        "      single_batch_num.append(input_data_num[num].copy())\n",
        "      single_batch_label.append(data_labels[num])\n",
        "      single_batch_id.append(data_id[num])\n",
        "      if len(input_data[num]) > max_length:\n",
        "        max_length = len(input_data[num])\n",
        "    else:\n",
        "      for num in range(len(single_batch_data)):\n",
        "        while len(single_batch_data[num]) < max_length:\n",
        "          single_batch_data[num].append(0)\n",
        "      batch_data.append((single_batch_id.copy(), single_batch_data.copy(), single_batch_num.copy(), single_batch_label.copy()))\n",
        "      single_batch_data = list()\n",
        "      single_batch_label = list()\n",
        "      single_batch_id = list()\n",
        "      max_length = 0\n",
        "      single_batch_data.append(input_data[num].copy())\n",
        "      single_batch_num.append(input_data_num[num].copy())\n",
        "      single_batch_label.append(data_labels[num])\n",
        "      single_batch_id.append(data_id[num])\n",
        "      if len(input_data[num]) > max_length:\n",
        "        max_length = len(input_data[num])\n",
        "  if len(single_batch_data) >0:\n",
        "    for num in range(len(single_batch_data)):\n",
        "        while len(single_batch_data[num]) < max_length:\n",
        "          single_batch_data[num].append(0)\n",
        "    batch_data.append((single_batch_id.copy(), single_batch_data.copy(), single_batch_num.copy(), single_batch_label.copy()))\n",
        "  return batch_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTbvdqHUSP33"
      },
      "source": [
        "def get_model_inputs(tokenized_corpus, word2idx, labels, max_len = 0):\n",
        "  # we index our sentences\n",
        "  vectorized_sents = [[word2idx[tok] for tok in sent if tok in word2idx] for sent in tokenized_corpus]\n",
        "  # Sentence lengths\n",
        "  sent_lengths = [len(sent) for sent in vectorized_sents]\n",
        "  # Get maximum length\n",
        "  max_len = max(sent_lengths)\n",
        "  # if max_len == 0:\n",
        "  #   max_len = int(max(sent_lengths))\n",
        "  # we create a tensor of a fixed size filled with zeroes for padding\n",
        "  sent_tensor = torch.zeros((len(vectorized_sents), max_len)).long().cuda()\n",
        "  # we fill it with our vectorized sentences \n",
        "  for idx, (sent, sentlen) in enumerate(zip(vectorized_sents, sent_lengths)):\n",
        "    sent_tensor[idx, :sentlen] = torch.LongTensor(sent).cuda()\n",
        "  # Label tensor\n",
        "  label_tensor = torch.FloatTensor(labels).cuda()\n",
        "  \n",
        "  return sent_tensor, label_tensor, max_len\n",
        "train_sent_tensor, train_label_tensor, max_len = get_model_inputs(train, word2idx, train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txN3ruh4SP33"
      },
      "source": [
        "encoded_train = encodeData(train, word2idx)\n",
        "encoded_valid = encodeData(valid, word2idx)\n",
        "encoded_test = encodeData(test, word2idx)\n",
        "\n",
        "\n",
        "train_sent_tensor, train_label_tensor, max_len = get_model_inputs(train, word2idx, train_labels)\n",
        "test_sent_tensor, test_label_tensor, max_len = get_model_inputs(test, word2idx, test_labels, max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wz6CdHbwbILL"
      },
      "source": [
        "def getEditMatrix(data_sent_tensor,data_num):\n",
        "  edit_matrix = torch.zeros(data_sent_tensor.size()[0], 1, data_sent_tensor.size()[1])\n",
        "  edit_total_num = torch.zeros(data_sent_tensor.size()[0], 1, 1)\n",
        "  \n",
        "  for batch_ in range(len(data_num)):\n",
        "\n",
        "    for num in data_num[batch_]:\n",
        "      edit_matrix[batch_][0][num] = 1\n",
        "  for batch_ in range(edit_matrix.size()[0]):\n",
        "    edit_total_num[batch_][0][0] = edit_matrix[batch_].sum()\n",
        "  return edit_matrix, edit_total_num"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL9672LqSP33"
      },
      "source": [
        "####Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg9puqqUSP33"
      },
      "source": [
        "model = LSTMEditedWordModel(len(wvecs), EMBEDDING_DIM, hidden_dim, out_size, max_length = train_sent_tensor.size()[1] , n_layer = 1, dropout = 0.3)\n",
        "model.embedding.weight.data.copy_(torch.from_numpy(wvecs))#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6Kj3wS3SP34"
      },
      "source": [
        "LRATE = 0.001\n",
        "EPOCHS = 60\n",
        "optimizer = optim.Adam(model.parameters(), lr=LRATE)\n",
        "loss_fn = nn.MSELoss()\n",
        "feature_train = train_sent_tensor\n",
        "target_train = train_label_tensor\n",
        "train_matrix, train_total_num = getEditMatrix(feature_train,train_num)\n",
        "\n",
        "feature_valid = test_sent_tensor\n",
        "target_valid = test_label_tensor\n",
        "print(len(feature_valid),len(test_num))\n",
        "valid_matrix, valid_total_num = getEditMatrix(feature_valid,test_num)\n",
        "\n",
        "feature_test = test_sent_tensor\n",
        "target_test = test_label_tensor\n",
        "test_matrix, test_total_num = getEditMatrix(feature_test,test_num)\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  model.train()\n",
        "  \n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  predictions = model(feature_train, train_matrix, train_total_num).squeeze(1)\n",
        "  loss = torch.sqrt(loss_fn(predictions, target_train))\n",
        "  train_loss = loss.item()\n",
        "  train_ave_error = average_error(predictions, target_train)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    predictions_valid = model(feature_valid, valid_matrix, valid_total_num).squeeze(1)\n",
        "    valid_loss = torch.sqrt(loss_fn(predictions_valid, target_valid)).item()\n",
        "    valid_ave_error = average_error(predictions_valid, target_valid)\n",
        "  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Train ave_error: {train_ave_error} | Val. Loss: {valid_loss:.3f} | Val. ave_error: {valid_ave_error} |')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM2b8XGiSP34"
      },
      "source": [
        "####Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fhig8s8kSP34"
      },
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  file = open('./LSTMEditedWordModel.csv','w',encoding='utf-8')\n",
        "  file.writelines('id,pred\\n')\n",
        "  for num in range(len(encoded_test)):\n",
        "    while len(encoded_test[num]) < 27:\n",
        "      encoded_test[num].append(0)\n",
        "    feature_test = torch.LongTensor([encoded_test[num]]).cuda()\n",
        "    target_test = torch.FloatTensor([test_labels[num]]).cuda()\n",
        "    test_matrix, test_total_num = getEditMatrix(feature_test,[test_num[num]])\n",
        "    predictions = model(feature_test, test_matrix, test_total_num)\n",
        "    loss = torch.sqrt(loss_fn(predictions, target_test))\n",
        "    ave_error = average_error(predictions, target_test)\n",
        "    str_content = test_id[num]  + ',' + str(abs(predictions.item())) + '\\n'\n",
        "    file.writelines(str_content)\n",
        "  file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t6Db3R7ut1e"
      },
      "source": [
        "###LSTMTwoHeadlineModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40j00yWjKjT1"
      },
      "source": [
        "####GetData"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kx3HJ52gKzkp"
      },
      "source": [
        "def pre_process(sentence):\n",
        "  sentence_temp = sentence.lower()\n",
        "  sentence_tokenize = word_tokenize(sentence)\n",
        "  sentence_tokenize = nltk.pos_tag(sentence_tokenize)\n",
        "  pos_sentence = []\n",
        "  for token in sentence_tokenize: \n",
        "    if token[1] == \"NNP\" or token[1] == \"NN\":\n",
        "      pos_sentence.append(token[0].capitalize())\n",
        "    else:\n",
        "      pos_sentence.append(token[0])\n",
        "  stop_word_removed_sentence = []\n",
        "  for token in pos_sentence:\n",
        "    if token not in stop_words_list:\n",
        "      stop_word_removed_sentence.append(token)\n",
        "  return_sentence = \" \".join(stop_word_removed_sentence)\n",
        "  return return_sentence\n",
        "\n",
        "def getCorpus(data_address):\n",
        "  data_replaced = list()\n",
        "  data_orginal = list()\n",
        "  data_orginal_temp = ''\n",
        "  word_orginal = ''\n",
        "  data_id = list()\n",
        "  labels = list()\n",
        "  file = open(data_address,'r',encoding='utf-8')\n",
        "  file_read = file.readlines()\n",
        "  flag = 0\n",
        "  for line in file_read:\n",
        "    word_orginal = ''\n",
        "    if flag == 0:\n",
        "      flag = 1\n",
        "      continue\n",
        "    line = line.replace('\\n','').replace('\\r','')\n",
        "    line = line.split(',')\n",
        "    if len(line) > 5:\n",
        "      line_temp = list()\n",
        "      line_temp.append(line[0])\n",
        "      line_content = ','.join(line[1:-3])\n",
        "      line_content_temp = line_content.split(' ')\n",
        "      for num in range(len(line_content_temp)):\n",
        "        if '<' in line_content_temp[num] and '/>' in line_content_temp[num]:\n",
        "          word_orginal = line_content_temp[num].replace('<','').replace('/>','')\n",
        "          line_content_temp[num] = line[-3]\n",
        "      line_content = ' '.join(line_content_temp)\n",
        "      line_content = pre_process(line_content)\n",
        "      line_temp.append(line_content)\n",
        "      line_temp.append(line[-3])\n",
        "      line_temp.append(line[-2])\n",
        "      line_temp.append(line[-1])\n",
        "      line = line_temp\n",
        "      \n",
        "      line[1] = pre_process(line[1])\n",
        "      data_orginal_temp = line[1].replace(line[2],word_orginal)\n",
        "    else:\n",
        "      line_content_temp = line[1].split(' ')\n",
        "      for num in range(len(line_content_temp)):\n",
        "        if '<' in line_content_temp[num] and '/>' in line_content_temp[num]:\n",
        "          word_orginal = line_content_temp[num].replace('<','').replace('/>','')\n",
        "          line_content_temp[num] = line[-3]\n",
        "      line_content = ' '.join(line_content_temp)\n",
        "      line[1] = line_content\n",
        "      \n",
        "      line[1] = pre_process(line[1])\n",
        "      data_orginal_temp = line[1].replace(line[-3],word_orginal)\n",
        "    line_temp = line[1].replace(' ','')\n",
        "    if '<' in line_temp and '/>' in line_temp:\n",
        "      p = r\"(?<=<).+?(?=/>)\" \n",
        "      line[1] = line[1].replace('/ >','/>')\n",
        "      if word_orginal == '':\n",
        "        word_orginal = line[1].split('<')[1]\n",
        "        word_orginal = word_orginal.split('/>')[0]\n",
        "        #print(line[1],'*************',line[2])\n",
        "        #print(word_orginal,line[2])\n",
        "      line[1] = re.sub(p, line[2], line[1])\n",
        "      line[1] = line[1].replace('<','').replace('/>','')\n",
        "      \n",
        "      line[1] = pre_process(line[1])\n",
        "      data_orginal_temp = line[1].replace(line[2],word_orginal)\n",
        "      \n",
        "    print(line[1],'**************',data_orginal_temp)\n",
        "\n",
        "    #orginal_data.append(line)\n",
        "    data_id.append(line[0])\n",
        "    data_orginal.append(data_orginal_temp)\n",
        "    data_replaced.append(line[1])\n",
        "    labels.append(float(line[4]))\n",
        "    \n",
        "  file.close()\n",
        "  return data_id, data_orginal, data_replaced, labels\n",
        "def average_error(output, target):\n",
        "  output = torch.round(torch.sigmoid(output))\n",
        "  err_num = 0\n",
        "  err_mean = 0.0\n",
        "  err_total = 0.0\n",
        "  for num in range(len(output)):\n",
        "    err_total = err_total + abs(output[num].item() - target[num])\n",
        "  err_mean = float(err_total/len(output))\n",
        "  return err_mean\n",
        "def get_tokenized_corpus(corpus):\n",
        "  tokenized_corpus = []\n",
        "  for sent in corpus:\n",
        "    tokenised_sentence = []\n",
        "    for token in sent.split(' '):\n",
        "      tokenised_sentence.append(token)\n",
        "    tokenized_corpus.append(tokenised_sentence)\n",
        "  return tokenized_corpus\n",
        "def get_word2idx(tokenized_corpus):\n",
        "  vocabulary = []\n",
        "  for sentence in tokenized_corpus:\n",
        "    for token in sentence:\n",
        "        if token not in vocabulary:\n",
        "            vocabulary.append(token)\n",
        "  \n",
        "  word2idx = {w: idx+1 for (idx, w) in enumerate(vocabulary)}\n",
        "  # we reserve the 0 index for the padding token\n",
        "  word2idx['<pad>'] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhisza-oKi0p"
      },
      "source": [
        "\n",
        "\n",
        "train_id, train_orginal, train, train_labels = getCorpus('/content/drive/My Drive/Colab Notebooks/semeval-2020-task-7-dataset/subtask-1/train.csv')\n",
        "test_id, test_orginal, test, test_labels = getCorpus('/content/drive/My Drive/Colab Notebooks/semeval-2020-task-7-dataset/subtask-1/test.csv')\n",
        "valid_id, valid_orginal, valid, valid_labels = getCorpus('/content/drive/My Drive/Colab Notebooks/semeval-2020-task-7-dataset/subtask-1/train.csv')\n",
        "\n",
        "train = getTokenizedCorpus(train)\n",
        "train_orginal = getTokenizedCorpus(train_orginal)\n",
        "test = getTokenizedCorpus(test)\n",
        "test_orginal = getTokenizedCorpus(test_orginal)\n",
        "valid = getTokenizedCorpus(valid)\n",
        "valid_orginal = getTokenizedCorpus(valid_orginal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzfQzq-AKHSx"
      },
      "source": [
        "####WordVector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWnsqgm_KHSz"
      },
      "source": [
        "def getWord2idx(tokenized_corpus):\n",
        "  vocabulary = []\n",
        "  for sentence in tokenized_corpus:\n",
        "    for token in sentence:\n",
        "        if token not in vocabulary:\n",
        "            vocabulary.append(token)\n",
        "  word2idx = {w: idx+1 for (idx, w) in enumerate(vocabulary)}\n",
        "  word2idx['<pad>'] = 0\n",
        "  return word2idx\n",
        "def encodeData(tokenized_corpus, word2idx):\n",
        "  enocded_corpus = list()\n",
        "  for sentence in tokenized_corpus:\n",
        "    encoded_sentence = list()\n",
        "    for token in sentence:\n",
        "      encoded_sentence.append(word2idx[token])\n",
        "    enocded_corpus.append(encoded_sentence.copy())\n",
        "  return enocded_corpus\n",
        "\n",
        "word2idx = getWord2idx(train + test + valid + train_orginal + test_orginal + valid_orginal)\n",
        "wvecs = np.zeros((len(word2idx), EMBEDDING_DIM), dtype='float32')\n",
        "\n",
        "with open(f'glove.6B.{EMBEDDING_DIM}d.txt', 'r') as f:\n",
        "  for line in tqdm(f):\n",
        "    if len(line.strip().split()) > 3:\n",
        "      word = line.strip().split()[0]\n",
        "      if word in word2idx:\n",
        "        (word, vec) = (word, list(map(float, line.strip().split()[1:])))\n",
        "        idx = word2idx[word]\n",
        "        wvecs[idx] = vec\n",
        "\n",
        "wvecs = wvecs * SCALE_EMBS\n",
        "wvecs = np.zeros((len(word2idx), EMBEDDING_DIM), dtype='float32')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSRvenQtTV85"
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnLVQNDg3jwz"
      },
      "source": [
        "####ProcessData"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1nCGo2N3jwz"
      },
      "source": [
        "def generateBatchData(data_id, input_data, data_labels, batch_size):\n",
        "  batch_data = list()\n",
        "  single_batch_data = list()\n",
        "  single_batch_label = list()\n",
        "  single_batch_id = list()\n",
        "  max_length = 0\n",
        "  for num in range(len(input_data)):\n",
        "    if len(single_batch_data) < batch_size:\n",
        "      single_batch_data.append(input_data[num].copy())\n",
        "      single_batch_label.append(data_labels[num])\n",
        "      single_batch_id.append(data_id[num])\n",
        "      if len(input_data[num]) > max_length:\n",
        "        max_length = len(input_data[num])\n",
        "    else:\n",
        "      for num in range(len(single_batch_data)):\n",
        "        while len(single_batch_data[num]) < max_length:\n",
        "          single_batch_data[num].append(0)\n",
        "      batch_data.append((single_batch_id.copy(), single_batch_data.copy(), single_batch_label.copy()))\n",
        "      single_batch_data = list()\n",
        "      single_batch_label = list()\n",
        "      single_batch_id = list()\n",
        "      max_length = 0\n",
        "      single_batch_data.append(input_data[num].copy())\n",
        "      single_batch_label.append(data_labels[num])\n",
        "      single_batch_id.append(data_id[num])\n",
        "      if len(input_data[num]) > max_length:\n",
        "        max_length = len(input_data[num])\n",
        "  if len(single_batch_data) >0:\n",
        "    for num in range(len(single_batch_data)):\n",
        "        while len(single_batch_data[num]) < max_length:\n",
        "          single_batch_data[num].append(0)\n",
        "    batch_data.append((single_batch_id.copy(), single_batch_data.copy(), single_batch_label.copy()))\n",
        "  return batch_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJMkkyZW3jwz"
      },
      "source": [
        "def get_model_inputs(tokenized_orginal_corpus, tokenized_corpus, word2idx, labels, max_len = 0):\n",
        "  # we index our sentences\n",
        "  vectorized_sents = [[word2idx[tok] for tok in sent if tok in word2idx] for sent in tokenized_corpus]\n",
        "  vectorized_orginal_sents = [[word2idx[tok] for tok in sent if tok in word2idx] for sent in tokenized_orginal_corpus]\n",
        "  # Sentence lengths\n",
        "  sent_lengths = [len(sent) for sent in vectorized_sents]\n",
        "  sent_orginal_lengths = [len(sent) for sent in vectorized_orginal_sents]\n",
        "  # Get maximum length\n",
        "  max_len = max(sent_lengths)\n",
        "  max_orginal_len = max(sent_orginal_lengths)\n",
        "  print(max_len)\n",
        "  # if max_len == 0:\n",
        "  #   max_len = int(max(sent_lengths))\n",
        "  # we create a tensor of a fixed size filled with zeroes for padding\n",
        "  sent_tensor = torch.zeros((len(vectorized_sents), max_len)).long().cuda()\n",
        "  sent_orginal_tensor = torch.zeros((len(vectorized_orginal_sents), max_orginal_len)).long().cuda()\n",
        "  # we fill it with our vectorized sentences \n",
        "  for idx, (sent, sentlen) in enumerate(zip(vectorized_sents, sent_lengths)):\n",
        "    \n",
        "    sent_tensor[idx, :sentlen] = torch.LongTensor(sent).cuda()\n",
        "  for idx, (sent, sentlen) in enumerate(zip(vectorized_orginal_sents, sent_orginal_lengths)):\n",
        "    sent_orginal_tensor[idx, :sentlen] = torch.LongTensor(sent).cuda()\n",
        "  # Label tensor\n",
        "  label_tensor = torch.FloatTensor(labels).cuda()\n",
        "  \n",
        "  return sent_orginal_tensor, sent_tensor, label_tensor, max_len\n",
        "#train_sent_tensor, train_label_tensor, max_len = get_model_inputs(train_orginal, train, word2idx, train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76ZstlP13jwz"
      },
      "source": [
        "encoded_train = encodeData(train, word2idx)\n",
        "encoded_valid = encodeData(valid, word2idx)\n",
        "encoded_test = encodeData(test, word2idx)\n",
        "encoded_test_orginal = encodeData(test_orginal, word2idx)\n",
        "# batch_train = generateBatchData(train_id, encoded_train, train_labels, 24)\n",
        "# batch_test = generateBatchData(test_id, encoded_test, test_labels, 24)\n",
        "# batch_valid = generateBatchData(valid_id, encoded_valid, valid_labels, 24)\n",
        "\n",
        "train_sent_orginal_tensor, train_sent_tensor, train_label_tensor, max_len = get_model_inputs(train_orginal, train, word2idx, train_labels)\n",
        "test_sent_orginal_tensor, test_sent_tensor, test_label_tensor, max_len = get_model_inputs(test_orginal, test, word2idx, test_labels, max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-GhLgITTHuc"
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTYd9Vbb3jwz"
      },
      "source": [
        "####Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFebx11t3jwz"
      },
      "source": [
        "model = LSTMTwoHeadlineModel(len(wvecs), EMBEDDING_DIM, hidden_dim, out_size, max_length = train_sent_tensor.size()[1] , n_layer = 1, dropout = 0.3)\n",
        "model.embedding.weight.data.copy_(torch.from_numpy(wvecs))#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_332Yye3jw0"
      },
      "source": [
        "LRATE = 0.001\n",
        "EPOCHS = 60\n",
        "optimizer = optim.Adam(model.parameters(), lr=LRATE)\n",
        "loss_fn = nn.MSELoss()\n",
        "feature_orginal_train = train_sent_orginal_tensor\n",
        "feature_train = train_sent_tensor\n",
        "target_train = train_label_tensor\n",
        "\n",
        "feature_orginal_valid = test_sent_orginal_tensor\n",
        "feature_valid = test_sent_tensor\n",
        "target_valid = test_label_tensor\n",
        "\n",
        "\n",
        "feature_orginal_test = test_sent_orginal_tensor\n",
        "feature_test = test_sent_tensor\n",
        "target_test = test_label_tensor\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  model.train()\n",
        "  \n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  predictions = model(feature_train, feature_orginal_train).squeeze(1)\n",
        "  loss = torch.sqrt(loss_fn(predictions, target_train))\n",
        "  train_loss = loss.item()\n",
        "  train_ave_error = average_error(predictions, target_train)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    predictions_valid = model(feature_valid, feature_orginal_valid).squeeze(1)\n",
        "    valid_loss = torch.sqrt(loss_fn(predictions_valid, target_valid)).item()\n",
        "    valid_ave_error = average_error(predictions_valid, target_valid)\n",
        "  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Train ave_error: {train_ave_error} | Val. Loss: {valid_loss:.3f} | Val. ave_error: {valid_ave_error} |')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgX_pv6i3jw0"
      },
      "source": [
        "####Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-3RLnGW3jw0"
      },
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  file = open('./LSTMTwoHeadlineModel.csv','w',encoding='utf-8')\n",
        "  file.writelines('id,pred\\n')\n",
        "  for num in range(len(encoded_test)):\n",
        "    while len(encoded_test[num]) < 27:\n",
        "      encoded_test[num].append(0)\n",
        "    feature_test = torch.LongTensor([encoded_test[num]]).cuda()\n",
        "    feature_orginal_test = torch.LongTensor([encoded_test_orginal[num]]).cuda()\n",
        "    target_test = torch.FloatTensor([test_labels[num]]).cuda()\n",
        "    predictions = model(feature_test, feature_orginal_test)\n",
        "    loss = torch.sqrt(loss_fn(predictions, target_test))\n",
        "    ave_error = average_error(predictions, target_test)\n",
        "    str_content = test_id[num]  + ',' + str(abs(predictions.item())) + '\\n'\n",
        "    file.writelines(str_content)\n",
        "  file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa7oZLwBadV9"
      },
      "source": [
        "###LSTMSequenceClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nroOmHsHTAG3"
      },
      "source": [
        "####ProcessData"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCCQo26-S5Py"
      },
      "source": [
        "def generateBatchData(data_id, input_data, data_labels, batch_size):\n",
        "  batch_data = list()\n",
        "  single_batch_data = list()\n",
        "  single_batch_label = list()\n",
        "  single_batch_id = list()\n",
        "  max_length = 0\n",
        "  for num in range(len(input_data)):\n",
        "    if len(single_batch_data) < batch_size:\n",
        "      single_batch_data.append(input_data[num].copy())\n",
        "      single_batch_label.append(data_labels[num])\n",
        "      single_batch_id.append(data_id[num])\n",
        "      if len(input_data[num]) > max_length:\n",
        "        max_length = len(input_data[num])\n",
        "    else:\n",
        "      for num in range(len(single_batch_data)):\n",
        "        while len(single_batch_data[num]) < max_length:\n",
        "          single_batch_data[num].append(0)\n",
        "      batch_data.append((single_batch_id.copy(), single_batch_data.copy(), single_batch_label.copy()))\n",
        "      single_batch_data = list()\n",
        "      single_batch_label = list()\n",
        "      single_batch_id = list()\n",
        "      max_length = 0\n",
        "      single_batch_data.append(input_data[num].copy())\n",
        "      single_batch_label.append(data_labels[num])\n",
        "      single_batch_id.append(data_id[num])\n",
        "      if len(input_data[num]) > max_length:\n",
        "        max_length = len(input_data[num])\n",
        "  if len(single_batch_data) >0:\n",
        "    for num in range(len(single_batch_data)):\n",
        "        while len(single_batch_data[num]) < max_length:\n",
        "          single_batch_data[num].append(0)\n",
        "    batch_data.append((single_batch_id.copy(), single_batch_data.copy(), single_batch_label.copy()))\n",
        "  return batch_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmtZaF4caTQs"
      },
      "source": [
        "def get_model_inputs(tokenized_corpus, word2idx, labels, max_len = 0):\n",
        "  # we index our sentences\n",
        "  vectorized_sents = [[word2idx[tok] for tok in sent if tok in word2idx] for sent in tokenized_corpus]\n",
        "  # Sentence lengths\n",
        "  sent_lengths = [len(sent) for sent in vectorized_sents]\n",
        "  # Get maximum length\n",
        "  max_len = max(sent_lengths)\n",
        "  # if max_len == 0:\n",
        "  #   max_len = int(max(sent_lengths))\n",
        "  # we create a tensor of a fixed size filled with zeroes for padding\n",
        "  sent_tensor = torch.zeros((len(vectorized_sents), max_len)).long().cuda()\n",
        "  # we fill it with our vectorized sentences \n",
        "  for idx, (sent, sentlen) in enumerate(zip(vectorized_sents, sent_lengths)):\n",
        "    sent_tensor[idx, :sentlen] = torch.LongTensor(sent).cuda()\n",
        "  # Label tensor\n",
        "  label_tensor = torch.FloatTensor(labels).cuda()\n",
        "  \n",
        "  return sent_tensor, label_tensor, max_len\n",
        "train_sent_tensor, train_label_tensor, max_len = get_model_inputs(train, word2idx, train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsULiGgDLnnv"
      },
      "source": [
        "encoded_train = encodeData(train, word2idx)\n",
        "encoded_valid = encodeData(valid, word2idx)\n",
        "encoded_test = encodeData(test, word2idx)\n",
        "\n",
        "\n",
        "batch_train = generateBatchData(train_id, encoded_train, train_labels, 24)\n",
        "batch_test = generateBatchData(test_id, encoded_test, test_labels, 24)\n",
        "batch_valid = generateBatchData(valid_id, encoded_valid, valid_labels, 24)\n",
        "\n",
        "train_sent_tensor, train_label_tensor, max_len = get_model_inputs(train, word2idx, train_labels)\n",
        "test_sent_tensor, test_label_tensor, max_len = get_model_inputs(test, word2idx, test_labels, max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-amyrB0l-aG"
      },
      "source": [
        "####Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBUKgBDUaeQg"
      },
      "source": [
        "model = LSTMAtt(len(wvecs), EMBEDDING_DIM, hidden_dim, out_size, max_length = train_sent_tensor.size()[1] , n_layer = 1, dropout = 0.3)\n",
        "model.embedding.weight.data.copy_(torch.from_numpy(wvecs))#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJZY-iOyOYLr"
      },
      "source": [
        "LRATE = 0.001\n",
        "EPOCHS = 60\n",
        "optimizer = optim.Adam(model.parameters(), lr=LRATE)\n",
        "loss_fn = nn.MSELoss()\n",
        "feature_train = train_sent_tensor\n",
        "target_train = train_label_tensor\n",
        "\n",
        "feature_valid = test_sent_tensor\n",
        "target_valid = test_label_tensor\n",
        "\n",
        "\n",
        "feature_test = test_sent_tensor\n",
        "target_test = test_label_tensor\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  model.train()\n",
        "  \n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  predictions = model(feature_train).squeeze(1)\n",
        "  loss = torch.sqrt(loss_fn(predictions, target_train))\n",
        "  train_loss = loss.item()\n",
        "  train_ave_error = average_error(predictions, target_train)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    predictions_valid = model(feature_valid).squeeze(1)\n",
        "    valid_loss = torch.sqrt(loss_fn(predictions_valid, target_valid)).item()\n",
        "    valid_ave_error = average_error(predictions_valid, target_valid)\n",
        "  print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Train ave_error: {train_ave_error} | Val. Loss: {valid_loss:.3f} | Val. ave_error: {valid_ave_error} |')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6o07R7gbvmC"
      },
      "source": [
        "feature_train.size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAMyTGJ6PCPj"
      },
      "source": [
        "batch_test = generateBatchData(test_id, encoded_test, test_labels, 24)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYZjPYT7hqJh"
      },
      "source": [
        "####Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6wFDz2XPH1q"
      },
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  file = open('./LSTMSequenceClassification.csv','w',encoding='utf-8')\n",
        "  file.writelines('id,pred\\n')\n",
        "  for num in range(len(encoded_test)):\n",
        "    while len(encoded_test[num]) < 27:\n",
        "      encoded_test[num].append(0)\n",
        "    feature_test = torch.LongTensor([encoded_test[num]]).cuda()\n",
        "    target_test = torch.FloatTensor([test_labels[num]]).cuda()\n",
        "    predictions = model(feature_test)\n",
        "    loss = torch.sqrt(loss_fn(predictions, target_test))\n",
        "    ave_error = average_error(predictions, target_test)\n",
        "    str_content = test_id[num]  + ',' + str(abs(predictions.item())) + '\\n'\n",
        "    file.writelines(str_content)\n",
        "  file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQDCMFwpRE_J"
      },
      "source": [
        "#Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UCTJj6CeJvZ"
      },
      "source": [
        "##Task1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNBSE0FjfSOm"
      },
      "source": [
        "###OurModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Th0zc9mwn4cs"
      },
      "source": [
        "with torch.no_grad():\n",
        "  new_title = test_replaced\n",
        "  old_title = test_orginal\n",
        "  encoding_old = tokenizer(old_title, return_tensors='pt', padding=True, truncation=True)\n",
        "  encoding_new = tokenizer(new_title, return_tensors='pt', padding=True, truncation=True)\n",
        "  input_ids_old = encoding_old['input_ids'].to(device)\n",
        "  input_ids_new = encoding_new['input_ids'].to(device)\n",
        "  attention_mask_old = encoding_old['attention_mask'].to(device)\n",
        "  attention_mask_new = encoding_new['attention_mask'].to(device)\n",
        "  # outputs = model(input_ids_new, input_ids_old, attention_mask_new, attention_mask_old)[0]\n",
        "  encoding_index = tokenizer(test_replaced, add_special_tokens=test_editword)['input_ids']\n",
        "  replace_words = test_editword\n",
        "  replace_matrix = tokenizer(new_title, return_tensors='pt', padding=True, truncation=True, add_special_tokens=replace_words)['input_ids']\n",
        "  edit_matrix, edit_total_num = getEditMatrix(tokenizer,encoding_new['input_ids'],encoding_index,replace_matrix,replace_words)\n",
        "  outputs = model(input_ids_new, attention_mask_new, input_ids_old, attention_mask_old, edit_matrix.to(device), edit_total_num.to(device))\n",
        "  labels = torch.tensor(test_meanGrade).to(device)\n",
        "  loss = torch.sqrt(((outputs - labels)**2).mean())\n",
        "  valid_loss = loss.item()\n",
        "  print(f'| Val. Loss: {valid_loss:.3f}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixtkqexifSOn"
      },
      "source": [
        "###SingleHeadlineModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AA_kEuSRfp3b"
      },
      "source": [
        "with torch.no_grad():\n",
        "  encoding = tokenizer(test_replaced, return_tensors='pt', padding=True, truncation=True)\n",
        "  input_ids = encoding['input_ids'].to(device)\n",
        "  attention_mask = encoding['attention_mask'].to(device)\n",
        "  outputs = model(input_ids, attention_mask=attention_mask)\n",
        "  #outputs = model(input_ids, attention_mask=attention_mask)\n",
        "  labels = torch.tensor(test_meanGrade).to(device)\n",
        "  loss = torch.sqrt(loss_fn(outputs,labels))\n",
        "  valid_loss = loss.item()\n",
        "  print(f'| Val. Loss: {valid_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srTU-bVz9lDE"
      },
      "source": [
        "input_ids.size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixuTHhXCfSOo"
      },
      "source": [
        "###EditedWordModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4EJ3IwEo4g8"
      },
      "source": [
        "with torch.no_grad():\n",
        "  encoding = tokenizer(test_replaced, return_tensors='pt', padding=True, truncation=True)\n",
        "  input_ids = encoding['input_ids'].to(device)\n",
        "  attention_mask = encoding['attention_mask'].to(device)\n",
        "  #outputs = model(input_ids, attention_mask=attention_mask)[0]\n",
        "  encoding_index = tokenizer(test_replaced, add_special_tokens=test_editword)['input_ids']\n",
        "  replace_words = test_editword\n",
        "  replace_matrix = tokenizer(replace_words, return_tensors='pt', padding=True, truncation=True, add_special_tokens=replace_words)['input_ids']\n",
        "  edit_matrix, edit_total_num = getEditMatrix(tokenizer,encoding['input_ids'],encoding_index,replace_matrix,replace_words)\n",
        "  outputs = model(input_ids, attention_mask=attention_mask,edit_matrix=edit_matrix.to(device), edit_total_num=edit_total_num.to(device))\n",
        "  labels = torch.tensor(test_meanGrade).to(device)\n",
        "  #loss = torch.sqrt(loss_fn(outputs,labels))\n",
        "  loss = torch.sqrt(((outputs - labels)**2).mean())\n",
        "  valid_loss = loss.item()\n",
        "  print(f'| Val. Loss: {valid_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIDVoaitfSOo"
      },
      "source": [
        "###TwoHeadlineModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EE4eBxpNDfVh"
      },
      "source": [
        "with torch.no_grad():\n",
        "  new_title = test_replaced\n",
        "  old_title = test_orginal\n",
        "  encoding_old = tokenizer(old_title, return_tensors='pt', padding=True, truncation=True)\n",
        "  encoding_new = tokenizer(new_title, return_tensors='pt', padding=True, truncation=True)\n",
        "  input_ids_old = encoding_old['input_ids'].to(device)\n",
        "  input_ids_new = encoding_new['input_ids'].to(device)\n",
        "  attention_mask_old = encoding_old['attention_mask'].to(device)\n",
        "  attention_mask_new = encoding_new['attention_mask'].to(device)\n",
        "  #outputs = model(input_ids_new, input_ids_old, attention_mask_new, attention_mask_old)[0]\n",
        "  outputs = model(input_ids_new, attention_mask_new, input_ids_old, attention_mask_old)\n",
        "  labels = torch.tensor(test_meanGrade).to(device)\n",
        "  loss = torch.sqrt(loss_fn(outputs,labels))\n",
        "  valid_loss = loss.item()\n",
        "  print(f'| Val. Loss: {valid_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOjkr86ofSOo"
      },
      "source": [
        "###RoBERTaSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vK9_AamlCuo"
      },
      "source": [
        "with torch.no_grad():\n",
        "  input_title = test_replaced\n",
        "  input_word = test_editword\n",
        "  input_data = [[input_title[num_],input_word[num_]] for num_ in range(len(input_title))]\n",
        "  encoding = tokenizer(input_data, return_tensors='pt', padding=True, truncation=True)\n",
        "  input_ids = encoding['input_ids'].to(device)\n",
        "  attention_mask = encoding['attention_mask'].to(device)\n",
        "  outputs = model(input_ids, attention_mask=attention_mask)[0]\n",
        "  #outputs = model(input_ids, attention_mask=attention_mask)\n",
        "  labels = torch.tensor(test_meanGrade).to(device)\n",
        "  loss = torch.sqrt(loss_fn(outputs,labels))\n",
        "  valid_loss = loss.item()\n",
        "  print(f'| Val. Loss: {valid_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOKfZUuReJ84"
      },
      "source": [
        "##Task2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_fcSNj3fSk8"
      },
      "source": [
        "###OurModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2ow3XvMmRS4"
      },
      "source": [
        "with torch.no_grad():\n",
        "  #valid_acc_list = list()\n",
        "  encoding_new1 = tokenizer(test_replaced1, return_tensors='pt', padding=True, truncation=True)\n",
        "  encoding_old1 = tokenizer(test_orginal1, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "  encoding_new2 = tokenizer(test_replaced2, return_tensors='pt', padding=True, truncation=True)\n",
        "  encoding_old2 = tokenizer(test_orginal2, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "  input_ids_new1 = encoding_new1['input_ids'].to(device)\n",
        "  input_ids_old1 = encoding_old1['input_ids'].to(device)\n",
        "\n",
        "  input_ids_new2 = encoding_new2['input_ids'].to(device)\n",
        "  input_ids_old2 = encoding_old2['input_ids'].to(device)\n",
        "\n",
        "  attention_mask_new1 = encoding_new1['attention_mask'].to(device)\n",
        "  attention_mask_old1 = encoding_old1['attention_mask'].to(device)\n",
        "\n",
        "  attention_mask_new2 = encoding_new2['attention_mask'].to(device)\n",
        "  attention_mask_old2 = encoding_old2['attention_mask'].to(device)\n",
        "\n",
        "  new_title1 = test_replaced1\n",
        "  old_title1 = test_orginal1\n",
        "  \n",
        "  new_title2 = test_replaced2\n",
        "  old_title2 = test_orginal2\n",
        "\n",
        "  labels = torch.tensor(test_label).to(device)\n",
        "\n",
        "  encoding_index1 = tokenizer(new_title1, add_special_tokens=test_editword1)['input_ids']\n",
        "  encoding_index2 = tokenizer(new_title2, add_special_tokens=test_editword2)['input_ids']\n",
        "\n",
        "  replace_words1 = test_editword1\n",
        "  replace_words2 = test_editword2\n",
        "\n",
        "  replace_matrix1 = tokenizer(replace_words1, return_tensors='pt', padding=True, truncation=True, add_special_tokens=data_editword1)['input_ids']\n",
        "  replace_matrix2 = tokenizer(replace_words2, return_tensors='pt', padding=True, truncation=True, add_special_tokens=data_editword2)['input_ids']\n",
        "\n",
        "  edit_matrix1, edit_total_num1 = getEditMatrix(tokenizer,encoding_new1['input_ids'],encoding_index1,replace_matrix1,replace_words1)\n",
        "  edit_matrix2, edit_total_num2 = getEditMatrix(tokenizer,encoding_new2['input_ids'],encoding_index2,replace_matrix2,replace_words2)\n",
        "\n",
        "  outputs = model(input_ids_new1, attention_mask_new1, input_ids_old1, attention_mask_old1, edit_matrix1.to(device), edit_total_num1.to(device), \n",
        "            input_ids_new2, attention_mask_new2, input_ids_old2, attention_mask_old2, edit_matrix2.to(device), edit_total_num2.to(device))\n",
        "  labels = torch.tensor(test_label[batchStart:batchEnd]).to(device)\n",
        "  loss = loss_fn(outputs,labels)\n",
        "  valid_acc = accuracy(outputs, labels)\n",
        "  valid_loss = loss_fn(outputs,labels)\n",
        "  batchStart = batchStart + batchSize\n",
        "  print(f' Val. valid_acc: {valid_acc} |')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKyFuyhRfSk8"
      },
      "source": [
        "###SingleHeadlineModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI2GKLkwlg_D"
      },
      "source": [
        "with torch.no_grad():\n",
        "  encoding = [[test_replaced1[num],test_replaced2[num]]for num in range(len(test_editword1))]\n",
        "  encoding = tokenizer(encoding, return_tensors='pt', padding=True, truncation=True)\n",
        "  input_ids = encoding['input_ids'].to(device)\n",
        "  attention_mask = encoding['attention_mask'].to(device)\n",
        "  outputs = model(input_ids, attention_mask=attention_mask)\n",
        "  labels = torch.tensor(test_label).to(device)\n",
        "  valid_acc = accuracy(outputs, labels)\n",
        "  valid_loss = loss_fn(outputs,labels)\n",
        "  f_measure(outputs, labels)\n",
        "  print(f' Val. valid_acc: {valid_acc} |')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LWEWIXXfSk9"
      },
      "source": [
        "###EditedWordModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adffa3QYll6c"
      },
      "source": [
        "with torch.no_grad():\n",
        "  old_title = [[data_orginal1[num]+ data_orginal2[num]] for num in range(len(test_meanGrade))]\n",
        "  new_title = [[data_replaced1[num]+ data_replaced2[num]] for num in range(len(test_meanGrade))]\n",
        "  title = [[old_title[num], new_title[num]] for num in range(len(old_title))]\n",
        "  encoding = tokenizer(title return_tensors='pt', padding=True, truncation=True)\n",
        "  input_ids = encoding['input_ids'].to(device)\n",
        "  attention_mask = encoding['attention_mask'].to(device)\n",
        "  encoding_index = tokenizer(title, add_special_tokens=data_editword1+data_editword2)['input_ids']\n",
        "  edit_matrix1, edit_total_num1 = getEditMatrix(tokenizer,encoding['input_ids'],encoding_index,replace_matrix1,replace_words1)\n",
        "  edit_matrix2, edit_total_num2 = getEditMatrix(tokenizer,encoding['input_ids'],encoding_index,replace_matrix2,replace_words2)\n",
        "  edit_matrix = edit_matrix1 + edit_matrix2\n",
        "  edit_total_num = edit_total_num1 + edit_total_num2\n",
        "  outputs = model(input_ids, attention_mask=attention_mask,edit_matrix=edit_matrix.to(device), edit_total_num=edit_total_num.to(device))\n",
        "  labels = torch.tensor(test_meanGrade).to(device)\n",
        "  loss = loss_fn(outputs,labels)\n",
        "  valid_loss = loss.item()\n",
        "  print(f' Val. valid_acc: {valid_acc} |')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX_gK1SZfSk9"
      },
      "source": [
        "###TwoHeadlineModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oubdGpVOrlFe"
      },
      "source": [
        "with torch.no_grad():\n",
        "  old_title = [[test_orginal1[num]+'<s>'+test_orginal2[num]] for num in range(len(test_orginal))]\n",
        "  new_title = [[test_replaced1[num]+'<s>'+test_replaced2[num]] for num in range(len(test_orginal))]\n",
        "  encoding_old = tokenizer(old_title, return_tensors='pt', padding=True, truncation=True)\n",
        "  encoding_new = tokenizer(new_title, return_tensors='pt', padding=True, truncation=True)\n",
        "  input_ids_old = encoding_old['input_ids'].to(device)\n",
        "  input_ids_new = encoding_new['input_ids'].to(device)\n",
        "  attention_mask_old = encoding_old['attention_mask'].to(device)\n",
        "  attention_mask_new = encoding_new['attention_mask'].to(device)\n",
        "  #outputs = model(input_ids_new, input_ids_old, attention_mask_new, attention_mask_old)[0]\n",
        "  outputs = model(input_ids_new, attention_mask_new, input_ids_old, attention_mask_old)\n",
        "  labels = torch.tensor(test_label).to(device)\n",
        "  valid_acc = accuracy(outputs, labels)\n",
        "  valid_loss = loss_fn(outputs,labels)\n",
        "  f_measure(outputs, labels)\n",
        "  print(f' Val. valid_acc: {valid_acc} |')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB8X0K8EfSk9"
      },
      "source": [
        "###RoBERTaSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNHxlBbmlbEA"
      },
      "source": [
        "with torch.no_grad():\n",
        "  encoding = [[test_replaced1[num],test_replaced2[num]]for num in range(len(test_editword1))]\n",
        "  encoding = tokenizer(encoding, return_tensors='pt', padding=True, truncation=True)\n",
        "  input_ids = encoding['input_ids'].to(device)\n",
        "  attention_mask = encoding['attention_mask'].to(device)\n",
        "  outputs = model(input_ids, attention_mask=attention_mask)[0]\n",
        "  #outputs = model(input_ids, attention_mask=attention_mask)\n",
        "  labels = torch.tensor(test_label).to(device)\n",
        "  valid_acc = accuracy(outputs, labels)\n",
        "  valid_loss = loss_fn(outputs,labels)\n",
        "  f_measure(outputs, labels)\n",
        "  print(f' Val. valid_acc: {valid_acc} |')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1Ce2PYJRGlV"
      },
      "source": [
        "#ResultSave"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiOGgb0QeMcN"
      },
      "source": [
        "##Task1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JjrymhgeAS4"
      },
      "source": [
        "#RoBERTaSingleHeadlineModel\n",
        "#RoBERTaEditedWordModel\n",
        "#RoBERTaTwoHeadlineModel\n",
        "#RoBERTaSequenceClassification\n",
        "#BERTSingleHeadlineModel\n",
        "#BERTTwoHeadlineModel\n",
        "#BERTaSequenceClassificatio\n",
        "import time\n",
        "time_str = str(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
        "time_str = time_str.replace(' ','-').replace(':','-')\n",
        "save_name = './' + modelname + time_str + '.csv'\n",
        "folder = './'\n",
        "file = open(save_name,'w',encoding='utf-8')\n",
        "file.writelines('id,pred\\n')\n",
        "for num in range(len(outputs)):\n",
        "  str_content = test_id[num]  + ',' + str(outputs[num].item()) + '\\n'\n",
        "  file.writelines(str_content)\n",
        "file.close()\n",
        "\n",
        "f = zipfile.ZipFile('./'+model_name+'_' + dev_type +'.zip', 'w', zipfile.ZIP_DEFLATED)\n",
        "f.write(save_name)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJhV3iqx9r_v"
      },
      "source": [
        "outputs.size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlOW9uVZeMgD"
      },
      "source": [
        "##Task2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WABHll0mRB7V"
      },
      "source": [
        "\n",
        "with torch.no_grad():\n",
        "  file = open('./task-2-output.csv','w',encoding='utf-8')\n",
        "  file.writelines('id,pred\\n')\n",
        "  for num in range(len(test_id)):\n",
        "    str_content = str(test_id[num])  + ',' + str(torch.max(torch.softmax(outputs,-1), 1)[1][num].item()) + '\\n'\n",
        "    file.writelines(str_content)\n",
        "  print(batchEnd,len(test_id))\n",
        "  file.close()\n",
        "model_type = 'StackingModel'\n",
        "f = zipfile.ZipFile('./'+model_name+'_' + dev_type +'.zip', 'w', zipfile.ZIP_DEFLATED)\n",
        "f.write('./task-2-output.csv')\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ_dNbv5-HJ9"
      },
      "source": [
        "#RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-I-_iKd_uJn"
      },
      "source": [
        "totalEpoch = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__sSM1Zs-PRZ"
      },
      "source": [
        "##Single"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWU-x5GV-iea"
      },
      "source": [
        "from pathlib import Path\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from tokenizers.implementations import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "from tokenizers import AddedToken\n",
        "from transformers import RobertaTokenizer\n",
        "import math\n",
        "import csv\n",
        "import re\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import lr_scheduler\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "import transformers\n",
        "import tokenizers\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers import RobertaConfig, RobertaModel\n",
        "#from transformers.modeling_roberta import RobertaClassificationHead\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.autonotebook import tqdm\n",
        "import utils\n",
        "from transformers import RobertaConfig, RobertaModel, RobertaForSequenceClassification, RobertaTokenizer\n",
        "from transformers import BertConfig, BertModel, BertForSequenceClassification, BertTokenizer\n",
        "from transformers import AdamW\n",
        "import transformers\n",
        "import torch\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import zipfile\n",
        "import random\n",
        "def fix_seed(seed=1):\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  random.seed(seed)\n",
        "fix_seed()\n",
        "configuration = RobertaConfig()\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", do_lower_case=True)\n",
        "dev_type = 'No_dev'\n",
        "import csv\n",
        "import re\n",
        "def readTask1Data(data_address):\n",
        "  data_id = list()\n",
        "  data_mask = list()\n",
        "  data_orginal = list()\n",
        "  data_replaced = list()\n",
        "  data_grades = list()\n",
        "  data_meanGrade = list()\n",
        "  data_editword = list()\n",
        "  data_orginalword = list()\n",
        "  flag = 0\n",
        "  p = r\"(?<=<).+?(?=/>)\" \n",
        "  with open(data_address, 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    for row in reader:\n",
        "      if flag == 0:\n",
        "        flag = 1\n",
        "        continue\n",
        "      data_id.append(row[0])\n",
        "      line = row[1]\n",
        "      while '  ' in line:\n",
        "        line.replace('  ',' ')\n",
        "      line = line.replace('/ >','/>')\n",
        "      line = line.replace('< ','<')\n",
        "      line = line.replace(' />','/>')\n",
        "      data_orginalword.append(re.findall(p, line))\n",
        "      data_orginal_temp = line.replace('<','').replace('/>','').replace('  ',' ')\n",
        "      data_orginal.append(data_orginal_temp)\n",
        "      data_mask_temp = re.sub(p, 'edit', line)\n",
        "      if '<edit/>' not in data_mask_temp:\n",
        "        print(data_mask_temp)\n",
        "      data_mask.append(data_mask_temp)\n",
        "      data_replaced_temp = data_mask_temp.replace('<edit/>',row[2])\n",
        "      data_replaced.append(data_replaced_temp)\n",
        "      data_grades.append(row[3])\n",
        "      data_meanGrade.append([float(row[4])])\n",
        "      data_editword.append(row[2])\n",
        "  return  data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword\n",
        "data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword = readTask1Data('/content/drive/MyDrive/Colab Notebooks/semeval-2020-task-7-dataset/subtask-1/train.csv')\n",
        "test_id, test_mask, test_orginal, test_replaced, test_grades, test_meanGrade, test_editword, test_orginalword = readTask1Data('/content/drive/MyDrive/Colab Notebooks/semeval-2020-task-7-dataset/subtask-1/test.csv')\n",
        "def splitTrainValid(data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword, valid_num=0.3):\n",
        "  train_id = list()\n",
        "  train_mask = list()\n",
        "  train_orginal = list()\n",
        "  train_replaced = list()\n",
        "  train_grades = list()\n",
        "  train_meanGrade = list()\n",
        "  train_editword = list()\n",
        "  train_orginalword = list()\n",
        "  val_id = list()\n",
        "  val_mask = list()\n",
        "  val_orginal = list()\n",
        "  val_replaced = list()\n",
        "  val_grades = list()\n",
        "  val_meanGrade = list()\n",
        "  val_editword = list()\n",
        "  val_orginalword = list()\n",
        "  temp_list = list()\n",
        "  for num in range(len(data_id)):\n",
        "    temp_list.append([data_id[num], data_mask[num], data_orginal[num], data_replaced[num], data_grades[num], data_meanGrade[num], data_editword[num], data_orginalword[num]])\n",
        "  train_set,valid_set = train_test_split(temp_list,test_size = valid_num)\n",
        "  for (set_id_temp, set_mask_temp, set_orginal_temp, set_replaced_temp, set_grades_temp, set_meanGrade_temp, set_editword_temp, set_orginalword_temp) in train_set:\n",
        "    train_id.append(set_id_temp)\n",
        "    train_mask.append(set_mask_temp)\n",
        "    train_orginal.append(set_orginal_temp)\n",
        "    train_replaced.append(set_replaced_temp)\n",
        "    train_grades.append(set_grades_temp)\n",
        "    train_meanGrade.append(set_meanGrade_temp)\n",
        "    train_editword.append(set_editword_temp)\n",
        "    train_orginalword.append(set_orginalword_temp)\n",
        "  for (set_id_temp, set_mask_temp, set_orginal_temp, set_replaced_temp, set_grades_temp, set_meanGrade_temp, set_editword_temp, set_orginalword_temp) in valid_set:\n",
        "    val_id.append(set_id_temp)\n",
        "    val_mask.append(set_mask_temp)\n",
        "    val_orginal.append(set_orginal_temp)\n",
        "    val_replaced.append(set_replaced_temp)\n",
        "    val_grades.append(set_grades_temp)\n",
        "    val_meanGrade.append(set_meanGrade_temp)\n",
        "    val_editword.append(set_editword_temp)\n",
        "    val_orginalword.append(set_orginalword_temp)\n",
        "  return train_id, train_mask, train_orginal, train_replaced, train_grades, train_meanGrade, train_editword, train_orginalword, val_id,val_mask,val_orginal,val_replaced,val_grades,val_meanGrade,val_editword,val_orginalword\n",
        "data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword, val_id, val_mask, val_orginal, val_replaced, val_grades, val_meanGrade, val_editword, val_orginalword = splitTrainValid(data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword)\n",
        "dev_type = 'No_dev'\n",
        "import csv\n",
        "import re\n",
        "def readTask1Data(data_address):\n",
        "  data_id = list()\n",
        "  data_mask = list()\n",
        "  data_orginal = list()\n",
        "  data_replaced = list()\n",
        "  data_grades = list()\n",
        "  data_meanGrade = list()\n",
        "  data_editword = list()\n",
        "  data_orginalword = list()\n",
        "  flag = 0\n",
        "  p = r\"(?<=<).+?(?=/>)\" \n",
        "  with open(data_address, 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    for row in reader:\n",
        "      if flag == 0:\n",
        "        flag = 1\n",
        "        continue\n",
        "      data_id.append(row[0])\n",
        "      line = row[1]\n",
        "      while '  ' in line:\n",
        "        line.replace('  ',' ')\n",
        "      line = line.replace('/ >','/>')\n",
        "      line = line.replace('< ','<')\n",
        "      line = line.replace(' />','/>')\n",
        "      data_orginalword.append(re.findall(p, line))\n",
        "      data_orginal_temp = line.replace('<','').replace('/>','').replace('  ',' ')\n",
        "      data_orginal.append(data_orginal_temp)\n",
        "      data_mask_temp = re.sub(p, 'edit', line)\n",
        "      if '<edit/>' not in data_mask_temp:\n",
        "        print(data_mask_temp)\n",
        "      data_mask.append(data_mask_temp)\n",
        "      data_replaced_temp = data_mask_temp.replace('<edit/>',row[2])\n",
        "      data_replaced.append(data_replaced_temp)\n",
        "      data_grades.append(row[3])\n",
        "      data_meanGrade.append([float(row[4])])\n",
        "      data_editword.append(row[2])\n",
        "  return  data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword\n",
        "data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword = readTask1Data('/content/drive/MyDrive/Colab Notebooks/semeval-2020-task-7-dataset/subtask-1/train.csv')\n",
        "test_id, test_mask, test_orginal, test_replaced, test_grades, test_meanGrade, test_editword, test_orginalword = readTask1Data('/content/drive/MyDrive/Colab Notebooks/semeval-2020-task-7-dataset/subtask-1/test.csv')\n",
        "def splitTrainValid(data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword, valid_num=0.3):\n",
        "  train_id = list()\n",
        "  train_mask = list()\n",
        "  train_orginal = list()\n",
        "  train_replaced = list()\n",
        "  train_grades = list()\n",
        "  train_meanGrade = list()\n",
        "  train_editword = list()\n",
        "  train_orginalword = list()\n",
        "  val_id = list()\n",
        "  val_mask = list()\n",
        "  val_orginal = list()\n",
        "  val_replaced = list()\n",
        "  val_grades = list()\n",
        "  val_meanGrade = list()\n",
        "  val_editword = list()\n",
        "  val_orginalword = list()\n",
        "  temp_list = list()\n",
        "  for num in range(len(data_id)):\n",
        "    temp_list.append([data_id[num], data_mask[num], data_orginal[num], data_replaced[num], data_grades[num], data_meanGrade[num], data_editword[num], data_orginalword[num]])\n",
        "  train_set,valid_set = train_test_split(temp_list,test_size = valid_num)\n",
        "  for (set_id_temp, set_mask_temp, set_orginal_temp, set_replaced_temp, set_grades_temp, set_meanGrade_temp, set_editword_temp, set_orginalword_temp) in train_set:\n",
        "    train_id.append(set_id_temp)\n",
        "    train_mask.append(set_mask_temp)\n",
        "    train_orginal.append(set_orginal_temp)\n",
        "    train_replaced.append(set_replaced_temp)\n",
        "    train_grades.append(set_grades_temp)\n",
        "    train_meanGrade.append(set_meanGrade_temp)\n",
        "    train_editword.append(set_editword_temp)\n",
        "    train_orginalword.append(set_orginalword_temp)\n",
        "  for (set_id_temp, set_mask_temp, set_orginal_temp, set_replaced_temp, set_grades_temp, set_meanGrade_temp, set_editword_temp, set_orginalword_temp) in valid_set:\n",
        "    val_id.append(set_id_temp)\n",
        "    val_mask.append(set_mask_temp)\n",
        "    val_orginal.append(set_orginal_temp)\n",
        "    val_replaced.append(set_replaced_temp)\n",
        "    val_grades.append(set_grades_temp)\n",
        "    val_meanGrade.append(set_meanGrade_temp)\n",
        "    val_editword.append(set_editword_temp)\n",
        "    val_orginalword.append(set_orginalword_temp)\n",
        "  return train_id, train_mask, train_orginal, train_replaced, train_grades, train_meanGrade, train_editword, train_orginalword, val_id,val_mask,val_orginal,val_replaced,val_grades,val_meanGrade,val_editword,val_orginalword\n",
        "data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword, val_id, val_mask, val_orginal, val_replaced, val_grades, val_meanGrade, val_editword, val_orginalword = splitTrainValid(data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword)\n",
        "class editHeadlineModel(transformers.BertPreTrainedModel):\n",
        "  def __init__(self, conf):\n",
        "    super(editHeadlineModel, self).__init__(conf)\n",
        "    self.roberta = transformers.RobertaModel.from_pretrained('roberta-base')\n",
        "    self.drop_out = nn.Dropout(0.1)\n",
        "    #self.pooling = nn.AdaptiveAvgPool1d(1024)\n",
        "    self.dense = nn.Linear(768, 768)\n",
        "    self.linear = nn.Linear(768, 1)\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    out = self.roberta(input_ids,attention_mask=attention_mask)[0]\n",
        "    out = out[:,0,:]\n",
        "    out = self.drop_out(out)\n",
        "    out = torch.tanh(out)\n",
        "    out = self.drop_out(out)\n",
        "    out = self.linear(out)\n",
        "    return out\n",
        "model = editHeadlineModel(configuration)\n",
        "model_name = 'editHeadlineModel'\n",
        "lr = 3e-5\n",
        "max_grad_norm = 1.0\n",
        "eps = 1e-8\n",
        "batchSize = 16\n",
        "num_train_steps = totalEpoch\n",
        "WU = 0.2\n",
        "num_training_steps = ((len(data_id)/batchSize)+1) * totalEpoch\n",
        "num_warmup_steps = int(num_training_steps * WU)\n",
        "#scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_train_steps)\n",
        "optimizer = AdamW(model.parameters(), lr = lr, eps = eps)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_train_steps, num_training_steps=num_training_steps)\n",
        "loss_fn = nn.MSELoss()\n",
        "#loss_fn = nn.CrossEntropyLoss()##Task2\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)\n",
        "for epoch in range(totalEpoch):\n",
        "  batchStart = 0\n",
        "  train_loss = 0\n",
        "  optimizer.zero_grad()\n",
        "  while batchStart <= len(data_meanGrade):\n",
        "    batchEnd = batchStart + batchSize\n",
        "    if batchEnd > len(data_meanGrade):\n",
        "      batchEnd = len(data_meanGrade)\n",
        "    encoding = tokenizer(data_replaced[batchStart:batchEnd], return_tensors='pt', padding=True, truncation=True)\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    labels = torch.tensor(data_meanGrade[batchStart:batchEnd]).to(device)\n",
        "    model.train()\n",
        "    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    #outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    #loss = torch.sqrt(loss_fn(outputs,labels))\n",
        "    loss = torch.sqrt(((outputs - labels)**2).mean())\n",
        "    train_loss += loss.item()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    #torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "    batchStart = batchStart + batchSize\n",
        "  with torch.no_grad():\n",
        "    encoding = tokenizer(val_replaced, return_tensors='pt', padding=True, truncation=True)\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    #outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    labels = torch.tensor(val_meanGrade).to(device)\n",
        "    loss = torch.sqrt(loss_fn(outputs,labels))\n",
        "    valid_loss = loss.item()\n",
        "    print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f}')\n",
        "with torch.no_grad():\n",
        "  encoding = tokenizer(test_replaced, return_tensors='pt', padding=True, truncation=True)\n",
        "  input_ids = encoding['input_ids'].to(device)\n",
        "  attention_mask = encoding['attention_mask'].to(device)\n",
        "  outputs = model(input_ids, attention_mask=attention_mask)\n",
        "  #outputs = model(input_ids, attention_mask=attention_mask)\n",
        "  labels = torch.tensor(test_meanGrade).to(device)\n",
        "  loss = torch.sqrt(loss_fn(outputs,labels))\n",
        "  valid_loss = loss.item()\n",
        "  print(f'| Val. Loss: {valid_loss:.3f}')\n",
        "#RoBERTaSingleHeadlineModel\n",
        "#RoBERTaEditedWordModel\n",
        "#RoBERTaTwoHeadlineModel\n",
        "#RoBERTaSequenceClassification\n",
        "#BERTSingleHeadlineModel\n",
        "#BERTTwoHeadlineModel\n",
        "#BERTaSequenceClassificatio\n",
        "import time\n",
        "model2 = 'RoBERTaSingleHeadlineModel'\n",
        "time_str = str(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
        "time_str = time_str.replace(' ','-').replace(':','-')\n",
        "save_name = './' + model2 + str(totalEpoch) + '-' + time_str + '.csv'\n",
        "folder = './'\n",
        "file = open(save_name, 'w', encoding='utf-8')\n",
        "file.writelines('id,pred\\n')\n",
        "for num in range(len(outputs)):\n",
        "  str_content = test_id[num]  + ',' + str(outputs[num].item()) + '\\n'\n",
        "  file.writelines(str_content)\n",
        "file.close()\n",
        "\n",
        "f = zipfile.ZipFile('./'+model_name+'_' + dev_type +'.zip', 'w', zipfile.ZIP_DEFLATED)\n",
        "f.write(save_name)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VKGvGnbWRRm"
      },
      "source": [
        "totalEpoch = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK7sOfsE-PUy"
      },
      "source": [
        "##Edited"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAUJKUn3-izH"
      },
      "source": [
        "from pathlib import Path\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from tokenizers.implementations import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "from tokenizers import AddedToken\n",
        "from transformers import RobertaTokenizer\n",
        "import math\n",
        "import csv\n",
        "import re\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import lr_scheduler\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "import transformers\n",
        "import tokenizers\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers import RobertaConfig, RobertaModel\n",
        "#from transformers.modeling_roberta import RobertaClassificationHead\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.autonotebook import tqdm\n",
        "import utils\n",
        "from transformers import RobertaConfig, RobertaModel, RobertaForSequenceClassification, RobertaTokenizer\n",
        "from transformers import BertConfig, BertModel, BertForSequenceClassification, BertTokenizer\n",
        "from transformers import AdamW\n",
        "import transformers\n",
        "import torch\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import zipfile\n",
        "import random\n",
        "def fix_seed(seed=1):\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  random.seed(seed)\n",
        "fix_seed()\n",
        "configuration = RobertaConfig()\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", do_lower_case=True)\n",
        "dev_type = 'No_dev'\n",
        "import csv\n",
        "import re\n",
        "def readTask1Data(data_address):\n",
        "  data_id = list()\n",
        "  data_mask = list()\n",
        "  data_orginal = list()\n",
        "  data_replaced = list()\n",
        "  data_grades = list()\n",
        "  data_meanGrade = list()\n",
        "  data_editword = list()\n",
        "  data_orginalword = list()\n",
        "  flag = 0\n",
        "  p = r\"(?<=<).+?(?=/>)\" \n",
        "  with open(data_address, 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    for row in reader:\n",
        "      if flag == 0:\n",
        "        flag = 1\n",
        "        continue\n",
        "      data_id.append(row[0])\n",
        "      line = row[1]\n",
        "      while '  ' in line:\n",
        "        line.replace('  ',' ')\n",
        "      line = line.replace('/ >','/>')\n",
        "      line = line.replace('< ','<')\n",
        "      line = line.replace(' />','/>')\n",
        "      data_orginalword.append(re.findall(p, line))\n",
        "      data_orginal_temp = line.replace('<','').replace('/>','').replace('  ',' ')\n",
        "      data_orginal.append(data_orginal_temp)\n",
        "      data_mask_temp = re.sub(p, 'edit', line)\n",
        "      if '<edit/>' not in data_mask_temp:\n",
        "        print(data_mask_temp)\n",
        "      data_mask.append(data_mask_temp)\n",
        "      data_replaced_temp = data_mask_temp.replace('<edit/>',row[2])\n",
        "      data_replaced.append(data_replaced_temp)\n",
        "      data_grades.append(row[3])\n",
        "      data_meanGrade.append([float(row[4])])\n",
        "      data_editword.append(row[2])\n",
        "  return  data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword\n",
        "data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword = readTask1Data('/content/drive/MyDrive/Colab Notebooks/semeval-2020-task-7-dataset/subtask-1/train.csv')\n",
        "test_id, test_mask, test_orginal, test_replaced, test_grades, test_meanGrade, test_editword, test_orginalword = readTask1Data('/content/drive/MyDrive/Colab Notebooks/semeval-2020-task-7-dataset/subtask-1/test.csv')\n",
        "def splitTrainValid(data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword, valid_num=0.3):\n",
        "  train_id = list()\n",
        "  train_mask = list()\n",
        "  train_orginal = list()\n",
        "  train_replaced = list()\n",
        "  train_grades = list()\n",
        "  train_meanGrade = list()\n",
        "  train_editword = list()\n",
        "  train_orginalword = list()\n",
        "  val_id = list()\n",
        "  val_mask = list()\n",
        "  val_orginal = list()\n",
        "  val_replaced = list()\n",
        "  val_grades = list()\n",
        "  val_meanGrade = list()\n",
        "  val_editword = list()\n",
        "  val_orginalword = list()\n",
        "  temp_list = list()\n",
        "  for num in range(len(data_id)):\n",
        "    temp_list.append([data_id[num], data_mask[num], data_orginal[num], data_replaced[num], data_grades[num], data_meanGrade[num], data_editword[num], data_orginalword[num]])\n",
        "  train_set,valid_set = train_test_split(temp_list,test_size = valid_num)\n",
        "  for (set_id_temp, set_mask_temp, set_orginal_temp, set_replaced_temp, set_grades_temp, set_meanGrade_temp, set_editword_temp, set_orginalword_temp) in train_set:\n",
        "    train_id.append(set_id_temp)\n",
        "    train_mask.append(set_mask_temp)\n",
        "    train_orginal.append(set_orginal_temp)\n",
        "    train_replaced.append(set_replaced_temp)\n",
        "    train_grades.append(set_grades_temp)\n",
        "    train_meanGrade.append(set_meanGrade_temp)\n",
        "    train_editword.append(set_editword_temp)\n",
        "    train_orginalword.append(set_orginalword_temp)\n",
        "  for (set_id_temp, set_mask_temp, set_orginal_temp, set_replaced_temp, set_grades_temp, set_meanGrade_temp, set_editword_temp, set_orginalword_temp) in valid_set:\n",
        "    val_id.append(set_id_temp)\n",
        "    val_mask.append(set_mask_temp)\n",
        "    val_orginal.append(set_orginal_temp)\n",
        "    val_replaced.append(set_replaced_temp)\n",
        "    val_grades.append(set_grades_temp)\n",
        "    val_meanGrade.append(set_meanGrade_temp)\n",
        "    val_editword.append(set_editword_temp)\n",
        "    val_orginalword.append(set_orginalword_temp)\n",
        "  return train_id, train_mask, train_orginal, train_replaced, train_grades, train_meanGrade, train_editword, train_orginalword, val_id,val_mask,val_orginal,val_replaced,val_grades,val_meanGrade,val_editword,val_orginalword\n",
        "data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword, val_id, val_mask, val_orginal, val_replaced, val_grades, val_meanGrade, val_editword, val_orginalword = splitTrainValid(data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword)\n",
        "dev_type = 'No_dev'\n",
        "import csv\n",
        "import re\n",
        "def readTask1Data(data_address):\n",
        "  data_id = list()\n",
        "  data_mask = list()\n",
        "  data_orginal = list()\n",
        "  data_replaced = list()\n",
        "  data_grades = list()\n",
        "  data_meanGrade = list()\n",
        "  data_editword = list()\n",
        "  data_orginalword = list()\n",
        "  flag = 0\n",
        "  p = r\"(?<=<).+?(?=/>)\" \n",
        "  with open(data_address, 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    for row in reader:\n",
        "      if flag == 0:\n",
        "        flag = 1\n",
        "        continue\n",
        "      data_id.append(row[0])\n",
        "      line = row[1]\n",
        "      while '  ' in line:\n",
        "        line.replace('  ',' ')\n",
        "      line = line.replace('/ >','/>')\n",
        "      line = line.replace('< ','<')\n",
        "      line = line.replace(' />','/>')\n",
        "      data_orginalword.append(re.findall(p, line))\n",
        "      data_orginal_temp = line.replace('<','').replace('/>','').replace('  ',' ')\n",
        "      data_orginal.append(data_orginal_temp)\n",
        "      data_mask_temp = re.sub(p, 'edit', line)\n",
        "      if '<edit/>' not in data_mask_temp:\n",
        "        print(data_mask_temp)\n",
        "      data_mask.append(data_mask_temp)\n",
        "      data_replaced_temp = data_mask_temp.replace('<edit/>',row[2])\n",
        "      data_replaced.append(data_replaced_temp)\n",
        "      data_grades.append(row[3])\n",
        "      data_meanGrade.append([float(row[4])])\n",
        "      data_editword.append(row[2])\n",
        "  return  data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword\n",
        "data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword = readTask1Data('/content/drive/MyDrive/Colab Notebooks/semeval-2020-task-7-dataset/subtask-1/train.csv')\n",
        "test_id, test_mask, test_orginal, test_replaced, test_grades, test_meanGrade, test_editword, test_orginalword = readTask1Data('/content/drive/MyDrive/Colab Notebooks/semeval-2020-task-7-dataset/subtask-1/test.csv')\n",
        "def splitTrainValid(data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword, valid_num=0.3):\n",
        "  train_id = list()\n",
        "  train_mask = list()\n",
        "  train_orginal = list()\n",
        "  train_replaced = list()\n",
        "  train_grades = list()\n",
        "  train_meanGrade = list()\n",
        "  train_editword = list()\n",
        "  train_orginalword = list()\n",
        "  val_id = list()\n",
        "  val_mask = list()\n",
        "  val_orginal = list()\n",
        "  val_replaced = list()\n",
        "  val_grades = list()\n",
        "  val_meanGrade = list()\n",
        "  val_editword = list()\n",
        "  val_orginalword = list()\n",
        "  temp_list = list()\n",
        "  for num in range(len(data_id)):\n",
        "    temp_list.append([data_id[num], data_mask[num], data_orginal[num], data_replaced[num], data_grades[num], data_meanGrade[num], data_editword[num], data_orginalword[num]])\n",
        "  train_set,valid_set = train_test_split(temp_list,test_size = valid_num)\n",
        "  for (set_id_temp, set_mask_temp, set_orginal_temp, set_replaced_temp, set_grades_temp, set_meanGrade_temp, set_editword_temp, set_orginalword_temp) in train_set:\n",
        "    train_id.append(set_id_temp)\n",
        "    train_mask.append(set_mask_temp)\n",
        "    train_orginal.append(set_orginal_temp)\n",
        "    train_replaced.append(set_replaced_temp)\n",
        "    train_grades.append(set_grades_temp)\n",
        "    train_meanGrade.append(set_meanGrade_temp)\n",
        "    train_editword.append(set_editword_temp)\n",
        "    train_orginalword.append(set_orginalword_temp)\n",
        "  for (set_id_temp, set_mask_temp, set_orginal_temp, set_replaced_temp, set_grades_temp, set_meanGrade_temp, set_editword_temp, set_orginalword_temp) in valid_set:\n",
        "    val_id.append(set_id_temp)\n",
        "    val_mask.append(set_mask_temp)\n",
        "    val_orginal.append(set_orginal_temp)\n",
        "    val_replaced.append(set_replaced_temp)\n",
        "    val_grades.append(set_grades_temp)\n",
        "    val_meanGrade.append(set_meanGrade_temp)\n",
        "    val_editword.append(set_editword_temp)\n",
        "    val_orginalword.append(set_orginalword_temp)\n",
        "  return train_id, train_mask, train_orginal, train_replaced, train_grades, train_meanGrade, train_editword, train_orginalword, val_id,val_mask,val_orginal,val_replaced,val_grades,val_meanGrade,val_editword,val_orginalword\n",
        "data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword, val_id, val_mask, val_orginal, val_replaced, val_grades, val_meanGrade, val_editword, val_orginalword = splitTrainValid(data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword)\n",
        "class wordModel(transformers.BertPreTrainedModel):\n",
        "  def __init__(self, conf):\n",
        "    super(wordModel, self).__init__(conf)\n",
        "    self.roberta = transformers.RobertaModel.from_pretrained('roberta-base')\n",
        "    self.drop_out = nn.Dropout(0.1)\n",
        "    self.dense = nn.Linear(768, 768)\n",
        "    self.linear = nn.Linear(768, 1)\n",
        "  def forward(self, input_ids, attention_mask, edit_matrix, edit_total_num):\n",
        "    batch_size = input_ids.size()[0]\n",
        "    out = self.roberta(input_ids,attention_mask=attention_mask)[0]\n",
        "    seq_size = out.size()[1]\n",
        "    #print(edit_matrix.size(), out.size())\n",
        "    out = torch.bmm(edit_matrix, out)\n",
        "    #print(out.size(),out.squeeze(dim=1).size())\n",
        "    out = out/edit_total_num\n",
        "    out = out.squeeze(dim=1)\n",
        "    out = self.drop_out(out)\n",
        "    out = torch.tanh(out)\n",
        "    out = self.drop_out(out)\n",
        "    logits = self.linear(out)\n",
        "    return logits\n",
        "model = wordModel(configuration)\n",
        "model_name = 'wordModel'\n",
        "lr = 3e-5\n",
        "max_grad_norm = 1.0\n",
        "eps = 1e-8\n",
        "batchSize = 16\n",
        "num_train_steps = totalEpoch\n",
        "WU = 0.2\n",
        "num_training_steps = ((len(data_id)/batchSize)+1) * totalEpoch\n",
        "num_warmup_steps = int(num_training_steps * WU)\n",
        "#scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_train_steps)\n",
        "optimizer = AdamW(model.parameters(), lr = lr, eps = eps)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_train_steps, num_training_steps=num_training_steps)\n",
        "loss_fn = nn.MSELoss()\n",
        "#loss_fn = nn.CrossEntropyLoss()##Task2\n",
        "def getEditMatrix(tokenizer, encoding_sentence, encoding_index, replace_matrix, replace_words):\n",
        "  edit_matrix = torch.zeros(encoding_sentence.size()[0], 1, encoding_sentence.size()[1])\n",
        "  edit_total_num = torch.zeros(encoding_sentence.size()[0], 1, 1)\n",
        "  for batch_ in range(len(replace_words)):\n",
        "    token_list = tokenizer.convert_ids_to_tokens(encoding_index[batch_][1:-1])\n",
        "    token_str= \" \".join(token_list)\n",
        "    token_str = token_str.split('Ġ')\n",
        "    token_dict = dict()\n",
        "    for temp in token_str:\n",
        "      token_dict[temp.replace(' ','')] = ('Ġ' + temp).split(' ')\n",
        "    for key in token_dict:\n",
        "      if replace_words[batch_].lower() in key.lower() and float(len(replace_words[batch_])/len(key)) > 0.5:\n",
        "        for tok_num in range(len(token_dict[key])):\n",
        "          if len(token_dict[key]) == 2 and token_dict[key][0].replace('Ġ','') in token_list:\n",
        "            edit_matrix[batch_][0][encoding_index[batch_].index(tokenizer.convert_tokens_to_ids(token_dict[key][0].replace('Ġ','')))] = 1\n",
        "            continue\n",
        "          if token_dict[key][tok_num] == '':\n",
        "            continue\n",
        "          edit_token_ids = tokenizer.convert_tokens_to_ids(token_dict[key][tok_num])\n",
        "          edit_matrix[batch_][0][encoding_index[batch_].index(tokenizer.convert_tokens_to_ids(token_dict[key][tok_num]))] = 1\n",
        "  for batch_ in range(edit_matrix.size()[0]):\n",
        "    edit_total_num[batch_][0][0] = edit_matrix[batch_].sum()\n",
        "  return edit_matrix, edit_total_num\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)\n",
        "for epoch in range(totalEpoch):\n",
        "  batchStart = 0\n",
        "  train_loss = 0\n",
        "  optimizer.zero_grad()\n",
        "  while batchStart <= len(data_meanGrade):\n",
        "    batchEnd = batchStart + batchSize\n",
        "    if batchEnd > len(data_meanGrade):\n",
        "      batchEnd = len(data_meanGrade)\n",
        "    encoding = tokenizer(data_replaced[batchStart:batchEnd], return_tensors='pt', padding=True, truncation=True)\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    labels = torch.tensor(data_meanGrade[batchStart:batchEnd]).to(device)\n",
        "    encoding_index = tokenizer(data_replaced[batchStart:batchEnd], add_special_tokens=data_editword)['input_ids']\n",
        "    replace_words = data_editword[batchStart:batchEnd]\n",
        "    replace_matrix = tokenizer(replace_words, return_tensors='pt', padding=True, truncation=True, add_special_tokens=data_editword)['input_ids']\n",
        "    edit_matrix, edit_total_num = getEditMatrix(tokenizer,encoding['input_ids'],encoding_index,replace_matrix,replace_words)\n",
        "    model.train()\n",
        "    #outputs = model(input_ids, attention_mask=attention_mask)[0]\n",
        "    outputs = model(input_ids, attention_mask=attention_mask,edit_matrix=edit_matrix.to(device), edit_total_num=edit_total_num.to(device))\n",
        "    #loss = torch.sqrt(loss_fn(outputs,labels))\n",
        "    loss = torch.sqrt(((outputs - labels)**2).mean())\n",
        "    train_loss += loss.item()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    #torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "    batchStart = batchStart + batchSize\n",
        "    \n",
        "  with torch.no_grad():\n",
        "    encoding = tokenizer(val_replaced, return_tensors='pt', padding=True, truncation=True)\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    #outputs = model(input_ids, attention_mask=attention_mask)[0]\n",
        "    encoding_index = tokenizer(val_replaced, add_special_tokens=val_editword)['input_ids']\n",
        "    replace_words = val_editword\n",
        "    replace_matrix = tokenizer(replace_words, return_tensors='pt', padding=True, truncation=True, add_special_tokens=replace_words)['input_ids']\n",
        "    edit_matrix, edit_total_num = getEditMatrix(tokenizer,encoding['input_ids'],encoding_index,replace_matrix,replace_words)\n",
        "    outputs = model(input_ids, attention_mask=attention_mask,edit_matrix=edit_matrix.to(device), edit_total_num=edit_total_num.to(device))\n",
        "    labels = torch.tensor(val_meanGrade).to(device)\n",
        "    #loss = torch.sqrt(loss_fn(outputs,labels))\n",
        "    loss = torch.sqrt(((outputs - labels)**2).mean())\n",
        "    valid_loss = loss.item()\n",
        "    print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f}')\n",
        "with torch.no_grad():\n",
        "  encoding = tokenizer(test_replaced, return_tensors='pt', padding=True, truncation=True)\n",
        "  input_ids = encoding['input_ids'].to(device)\n",
        "  attention_mask = encoding['attention_mask'].to(device)\n",
        "  #outputs = model(input_ids, attention_mask=attention_mask)[0]\n",
        "  encoding_index = tokenizer(test_replaced, add_special_tokens=test_editword)['input_ids']\n",
        "  replace_words = test_editword\n",
        "  replace_matrix = tokenizer(replace_words, return_tensors='pt', padding=True, truncation=True, add_special_tokens=replace_words)['input_ids']\n",
        "  edit_matrix, edit_total_num = getEditMatrix(tokenizer,encoding['input_ids'],encoding_index,replace_matrix,replace_words)\n",
        "  outputs = model(input_ids, attention_mask=attention_mask,edit_matrix=edit_matrix.to(device), edit_total_num=edit_total_num.to(device))\n",
        "  labels = torch.tensor(test_meanGrade).to(device)\n",
        "  #loss = torch.sqrt(loss_fn(outputs,labels))\n",
        "  loss = torch.sqrt(((outputs - labels)**2).mean())\n",
        "  valid_loss = loss.item()\n",
        "  print(f'| Val. Loss: {valid_loss:.3f}')\n",
        "#RoBERTaSingleHeadlineModel\n",
        "#RoBERTaEditedWordModel\n",
        "#RoBERTaTwoHeadlineModel\n",
        "#RoBERTaSequenceClassification\n",
        "#BERTSingleHeadlineModel\n",
        "#BERTTwoHeadlineModel\n",
        "#BERTaSequenceClassificatio\n",
        "import time\n",
        "model2 = 'RoBERTaEditedWordModel'\n",
        "time_str = str(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
        "time_str = time_str.replace(' ','-').replace(':','-')\n",
        "save_name = './' + model2 + str(totalEpoch) + '-' + time_str + '.csv'\n",
        "folder = './'\n",
        "file = open(save_name, 'w', encoding='utf-8')\n",
        "file.writelines('id,pred\\n')\n",
        "for num in range(len(outputs)):\n",
        "  str_content = test_id[num]  + ',' + str(outputs[num].item()) + '\\n'\n",
        "  file.writelines(str_content)\n",
        "file.close()\n",
        "\n",
        "f = zipfile.ZipFile('./'+model_name+'_' + dev_type +'.zip', 'w', zipfile.ZIP_DEFLATED)\n",
        "f.write(save_name)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11hF1RaI-PXp"
      },
      "source": [
        "##TWO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZZSj5KI-drU"
      },
      "source": [
        "from pathlib import Path\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from tokenizers.implementations import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "from tokenizers import AddedToken\n",
        "from transformers import RobertaTokenizer\n",
        "import math\n",
        "import csv\n",
        "import re\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import lr_scheduler\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "import transformers\n",
        "import tokenizers\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers import RobertaConfig, RobertaModel\n",
        "#from transformers.modeling_roberta import RobertaClassificationHead\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.autonotebook import tqdm\n",
        "import utils\n",
        "from transformers import RobertaConfig, RobertaModel, RobertaForSequenceClassification, RobertaTokenizer\n",
        "from transformers import BertConfig, BertModel, BertForSequenceClassification, BertTokenizer\n",
        "from transformers import AdamW\n",
        "import transformers\n",
        "import torch\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import zipfile\n",
        "import random\n",
        "def fix_seed(seed=1):\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  random.seed(seed)\n",
        "fix_seed()\n",
        "configuration = RobertaConfig()\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", do_lower_case=True)\n",
        "dev_type = 'No_dev'\n",
        "import csv\n",
        "import re\n",
        "def readTask1Data(data_address):\n",
        "  data_id = list()\n",
        "  data_mask = list()\n",
        "  data_orginal = list()\n",
        "  data_replaced = list()\n",
        "  data_grades = list()\n",
        "  data_meanGrade = list()\n",
        "  data_editword = list()\n",
        "  data_orginalword = list()\n",
        "  flag = 0\n",
        "  p = r\"(?<=<).+?(?=/>)\" \n",
        "  with open(data_address, 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    for row in reader:\n",
        "      if flag == 0:\n",
        "        flag = 1\n",
        "        continue\n",
        "      data_id.append(row[0])\n",
        "      line = row[1]\n",
        "      while '  ' in line:\n",
        "        line.replace('  ',' ')\n",
        "      line = line.replace('/ >','/>')\n",
        "      line = line.replace('< ','<')\n",
        "      line = line.replace(' />','/>')\n",
        "      data_orginalword.append(re.findall(p, line))\n",
        "      data_orginal_temp = line.replace('<','').replace('/>','').replace('  ',' ')\n",
        "      data_orginal.append(data_orginal_temp)\n",
        "      data_mask_temp = re.sub(p, 'edit', line)\n",
        "      if '<edit/>' not in data_mask_temp:\n",
        "        print(data_mask_temp)\n",
        "      data_mask.append(data_mask_temp)\n",
        "      data_replaced_temp = data_mask_temp.replace('<edit/>',row[2])\n",
        "      data_replaced.append(data_replaced_temp)\n",
        "      data_grades.append(row[3])\n",
        "      data_meanGrade.append([float(row[4])])\n",
        "      data_editword.append(row[2])\n",
        "  return  data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword\n",
        "data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword = readTask1Data('/content/drive/MyDrive/Colab Notebooks/semeval-2020-task-7-dataset/subtask-1/train.csv')\n",
        "test_id, test_mask, test_orginal, test_replaced, test_grades, test_meanGrade, test_editword, test_orginalword = readTask1Data('/content/drive/MyDrive/Colab Notebooks/semeval-2020-task-7-dataset/subtask-1/test.csv')\n",
        "def splitTrainValid(data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword, valid_num=0.3):\n",
        "  train_id = list()\n",
        "  train_mask = list()\n",
        "  train_orginal = list()\n",
        "  train_replaced = list()\n",
        "  train_grades = list()\n",
        "  train_meanGrade = list()\n",
        "  train_editword = list()\n",
        "  train_orginalword = list()\n",
        "  val_id = list()\n",
        "  val_mask = list()\n",
        "  val_orginal = list()\n",
        "  val_replaced = list()\n",
        "  val_grades = list()\n",
        "  val_meanGrade = list()\n",
        "  val_editword = list()\n",
        "  val_orginalword = list()\n",
        "  temp_list = list()\n",
        "  for num in range(len(data_id)):\n",
        "    temp_list.append([data_id[num], data_mask[num], data_orginal[num], data_replaced[num], data_grades[num], data_meanGrade[num], data_editword[num], data_orginalword[num]])\n",
        "  train_set,valid_set = train_test_split(temp_list,test_size = valid_num)\n",
        "  for (set_id_temp, set_mask_temp, set_orginal_temp, set_replaced_temp, set_grades_temp, set_meanGrade_temp, set_editword_temp, set_orginalword_temp) in train_set:\n",
        "    train_id.append(set_id_temp)\n",
        "    train_mask.append(set_mask_temp)\n",
        "    train_orginal.append(set_orginal_temp)\n",
        "    train_replaced.append(set_replaced_temp)\n",
        "    train_grades.append(set_grades_temp)\n",
        "    train_meanGrade.append(set_meanGrade_temp)\n",
        "    train_editword.append(set_editword_temp)\n",
        "    train_orginalword.append(set_orginalword_temp)\n",
        "  for (set_id_temp, set_mask_temp, set_orginal_temp, set_replaced_temp, set_grades_temp, set_meanGrade_temp, set_editword_temp, set_orginalword_temp) in valid_set:\n",
        "    val_id.append(set_id_temp)\n",
        "    val_mask.append(set_mask_temp)\n",
        "    val_orginal.append(set_orginal_temp)\n",
        "    val_replaced.append(set_replaced_temp)\n",
        "    val_grades.append(set_grades_temp)\n",
        "    val_meanGrade.append(set_meanGrade_temp)\n",
        "    val_editword.append(set_editword_temp)\n",
        "    val_orginalword.append(set_orginalword_temp)\n",
        "  return train_id, train_mask, train_orginal, train_replaced, train_grades, train_meanGrade, train_editword, train_orginalword, val_id,val_mask,val_orginal,val_replaced,val_grades,val_meanGrade,val_editword,val_orginalword\n",
        "data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword, val_id, val_mask, val_orginal, val_replaced, val_grades, val_meanGrade, val_editword, val_orginalword = splitTrainValid(data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword)\n",
        "dev_type = 'No_dev'\n",
        "import csv\n",
        "import re\n",
        "def readTask1Data(data_address):\n",
        "  data_id = list()\n",
        "  data_mask = list()\n",
        "  data_orginal = list()\n",
        "  data_replaced = list()\n",
        "  data_grades = list()\n",
        "  data_meanGrade = list()\n",
        "  data_editword = list()\n",
        "  data_orginalword = list()\n",
        "  flag = 0\n",
        "  p = r\"(?<=<).+?(?=/>)\" \n",
        "  with open(data_address, 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    for row in reader:\n",
        "      if flag == 0:\n",
        "        flag = 1\n",
        "        continue\n",
        "      data_id.append(row[0])\n",
        "      line = row[1]\n",
        "      while '  ' in line:\n",
        "        line.replace('  ',' ')\n",
        "      line = line.replace('/ >','/>')\n",
        "      line = line.replace('< ','<')\n",
        "      line = line.replace(' />','/>')\n",
        "      data_orginalword.append(re.findall(p, line))\n",
        "      data_orginal_temp = line.replace('<','').replace('/>','').replace('  ',' ')\n",
        "      data_orginal.append(data_orginal_temp)\n",
        "      data_mask_temp = re.sub(p, 'edit', line)\n",
        "      if '<edit/>' not in data_mask_temp:\n",
        "        print(data_mask_temp)\n",
        "      data_mask.append(data_mask_temp)\n",
        "      data_replaced_temp = data_mask_temp.replace('<edit/>',row[2])\n",
        "      data_replaced.append(data_replaced_temp)\n",
        "      data_grades.append(row[3])\n",
        "      data_meanGrade.append([float(row[4])])\n",
        "      data_editword.append(row[2])\n",
        "  return  data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword\n",
        "data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword = readTask1Data('/content/drive/MyDrive/Colab Notebooks/semeval-2020-task-7-dataset/subtask-1/train.csv')\n",
        "test_id, test_mask, test_orginal, test_replaced, test_grades, test_meanGrade, test_editword, test_orginalword = readTask1Data('/content/drive/MyDrive/Colab Notebooks/semeval-2020-task-7-dataset/subtask-1/test.csv')\n",
        "def splitTrainValid(data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword, valid_num=0.3):\n",
        "  train_id = list()\n",
        "  train_mask = list()\n",
        "  train_orginal = list()\n",
        "  train_replaced = list()\n",
        "  train_grades = list()\n",
        "  train_meanGrade = list()\n",
        "  train_editword = list()\n",
        "  train_orginalword = list()\n",
        "  val_id = list()\n",
        "  val_mask = list()\n",
        "  val_orginal = list()\n",
        "  val_replaced = list()\n",
        "  val_grades = list()\n",
        "  val_meanGrade = list()\n",
        "  val_editword = list()\n",
        "  val_orginalword = list()\n",
        "  temp_list = list()\n",
        "  for num in range(len(data_id)):\n",
        "    temp_list.append([data_id[num], data_mask[num], data_orginal[num], data_replaced[num], data_grades[num], data_meanGrade[num], data_editword[num], data_orginalword[num]])\n",
        "  train_set,valid_set = train_test_split(temp_list,test_size = valid_num)\n",
        "  for (set_id_temp, set_mask_temp, set_orginal_temp, set_replaced_temp, set_grades_temp, set_meanGrade_temp, set_editword_temp, set_orginalword_temp) in train_set:\n",
        "    train_id.append(set_id_temp)\n",
        "    train_mask.append(set_mask_temp)\n",
        "    train_orginal.append(set_orginal_temp)\n",
        "    train_replaced.append(set_replaced_temp)\n",
        "    train_grades.append(set_grades_temp)\n",
        "    train_meanGrade.append(set_meanGrade_temp)\n",
        "    train_editword.append(set_editword_temp)\n",
        "    train_orginalword.append(set_orginalword_temp)\n",
        "  for (set_id_temp, set_mask_temp, set_orginal_temp, set_replaced_temp, set_grades_temp, set_meanGrade_temp, set_editword_temp, set_orginalword_temp) in valid_set:\n",
        "    val_id.append(set_id_temp)\n",
        "    val_mask.append(set_mask_temp)\n",
        "    val_orginal.append(set_orginal_temp)\n",
        "    val_replaced.append(set_replaced_temp)\n",
        "    val_grades.append(set_grades_temp)\n",
        "    val_meanGrade.append(set_meanGrade_temp)\n",
        "    val_editword.append(set_editword_temp)\n",
        "    val_orginalword.append(set_orginalword_temp)\n",
        "  return train_id, train_mask, train_orginal, train_replaced, train_grades, train_meanGrade, train_editword, train_orginalword, val_id,val_mask,val_orginal,val_replaced,val_grades,val_meanGrade,val_editword,val_orginalword\n",
        "data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword, val_id, val_mask, val_orginal, val_replaced, val_grades, val_meanGrade, val_editword, val_orginalword = splitTrainValid(data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword)\n",
        "class twoHeadlineModel_v1(transformers.BertPreTrainedModel):\n",
        "  def __init__(self, conf):\n",
        "    super(twoHeadlineModel_v1, self).__init__(conf)\n",
        "    self.roberta = transformers.RobertaModel.from_pretrained('roberta-base')\n",
        "    #self.roberta = transformers.RobertaModel('roberta-base')\n",
        "    self.dense = nn.Linear(768*4, 768*4)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.linear = nn.Linear(768*4,1)\n",
        "  def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2):\n",
        "    out1 = self.roberta(input_ids1,attention_mask=attention_mask1)[0]\n",
        "    out2 = self.roberta(input_ids2,attention_mask=attention_mask2)[0]\n",
        "    out1 = out1[:,0,:]\n",
        "    out2 = out2[:,0,:]\n",
        "    merge = [out1, out2, (out1 - out2).abs(), out1 * out2]\n",
        "    cat = torch.cat(merge, dim=-1)\n",
        "    out = self.dense(cat)\n",
        "    logits = self.linear(out)\n",
        "    return logits\n",
        "model = twoHeadlineModel_v1(configuration)\n",
        "model_name = 'twoHeadlineModel_v1'\n",
        "lr = 3e-5\n",
        "max_grad_norm = 1.0\n",
        "eps = 1e-8\n",
        "batchSize = 16\n",
        "num_train_steps = totalEpoch\n",
        "WU = 0.2\n",
        "num_training_steps = ((len(data_id)/batchSize)+1) * totalEpoch\n",
        "num_warmup_steps = int(num_training_steps * WU)\n",
        "#scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_train_steps)\n",
        "optimizer = AdamW(model.parameters(), lr = lr, eps = eps)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_train_steps, num_training_steps=num_training_steps)\n",
        "loss_fn = nn.MSELoss()\n",
        "#loss_fn = nn.CrossEntropyLoss()##Task2\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)\n",
        "for epoch in range(totalEpoch):\n",
        "  batchStart = 0\n",
        "  train_loss = 0\n",
        "  optimizer.zero_grad()\n",
        "  while batchStart <= len(data_meanGrade):\n",
        "    batchEnd = batchStart + batchSize\n",
        "    if batchEnd > len(data_meanGrade):\n",
        "      batchEnd = len(data_meanGrade)\n",
        "    old_title = data_orginal[batchStart:batchEnd]\n",
        "    new_title = data_replaced[batchStart:batchEnd]\n",
        "    encoding_old = tokenizer(old_title, return_tensors='pt', padding=True, truncation=True)\n",
        "    encoding_new = tokenizer(new_title, return_tensors='pt', padding=True, truncation=True)\n",
        "    input_ids_old = encoding_old['input_ids'].to(device)\n",
        "    input_ids_new = encoding_new['input_ids'].to(device)\n",
        "    attention_mask_old = encoding_old['attention_mask'].to(device)\n",
        "    attention_mask_new = encoding_new['attention_mask'].to(device)\n",
        "    labels = torch.tensor(data_meanGrade[batchStart:batchEnd]).to(device)\n",
        "    model.train()\n",
        "    #outputs = model(input_ids_new, input_ids_old, attention_mask_new, attention_mask_old)[0]\n",
        "    outputs = model(input_ids_new, attention_mask_new, input_ids_old, attention_mask_old)\n",
        "    #loss = torch.sqrt(loss_fn(outputs,labels))\n",
        "    loss = torch.sqrt(((outputs - labels)**2).mean())\n",
        "    train_loss += loss.item()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    #torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "    batchStart = batchStart + batchSize\n",
        "    \n",
        "  with torch.no_grad():\n",
        "    new_title = val_replaced\n",
        "    old_title = val_orginal\n",
        "    encoding_old = tokenizer(old_title, return_tensors='pt', padding=True, truncation=True)\n",
        "    encoding_new = tokenizer(new_title, return_tensors='pt', padding=True, truncation=True)\n",
        "    input_ids_old = encoding_old['input_ids'].to(device)\n",
        "    input_ids_new = encoding_new['input_ids'].to(device)\n",
        "    attention_mask_old = encoding_old['attention_mask'].to(device)\n",
        "    attention_mask_new = encoding_new['attention_mask'].to(device)\n",
        "    #outputs = model(input_ids_new, input_ids_old, attention_mask_new, attention_mask_old)[0]\n",
        "    outputs = model(input_ids_new, attention_mask_new, input_ids_old, attention_mask_old)\n",
        "    labels = torch.tensor(val_meanGrade).to(device)\n",
        "    loss = torch.sqrt(loss_fn(outputs,labels))\n",
        "    valid_loss = loss.item()\n",
        "    print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f}')\n",
        "with torch.no_grad():\n",
        "  new_title = test_replaced\n",
        "  old_title = test_orginal\n",
        "  encoding_old = tokenizer(old_title, return_tensors='pt', padding=True, truncation=True)\n",
        "  encoding_new = tokenizer(new_title, return_tensors='pt', padding=True, truncation=True)\n",
        "  input_ids_old = encoding_old['input_ids'].to(device)\n",
        "  input_ids_new = encoding_new['input_ids'].to(device)\n",
        "  attention_mask_old = encoding_old['attention_mask'].to(device)\n",
        "  attention_mask_new = encoding_new['attention_mask'].to(device)\n",
        "  #outputs = model(input_ids_new, input_ids_old, attention_mask_new, attention_mask_old)[0]\n",
        "  outputs = model(input_ids_new, attention_mask_new, input_ids_old, attention_mask_old)\n",
        "  labels = torch.tensor(test_meanGrade).to(device)\n",
        "  loss = torch.sqrt(loss_fn(outputs,labels))\n",
        "  valid_loss = loss.item()\n",
        "  print(f'| Val. Loss: {valid_loss:.3f}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acQaYJj7-Pam"
      },
      "source": [
        "##CLASS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Scfk5v1e-Iw_"
      },
      "source": [
        "from pathlib import Path\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from tokenizers.implementations import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "from tokenizers import AddedToken\n",
        "from transformers import RobertaTokenizer\n",
        "import math\n",
        "import csv\n",
        "import re\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import lr_scheduler\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "import transformers\n",
        "import tokenizers\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers import RobertaConfig, RobertaModel\n",
        "#from transformers.modeling_roberta import RobertaClassificationHead\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.autonotebook import tqdm\n",
        "import utils\n",
        "from transformers import RobertaConfig, RobertaModel, RobertaForSequenceClassification, RobertaTokenizer\n",
        "from transformers import BertConfig, BertModel, BertForSequenceClassification, BertTokenizer\n",
        "from transformers import AdamW\n",
        "import transformers\n",
        "import torch\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import zipfile\n",
        "import random\n",
        "def fix_seed(seed=1):\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  random.seed(seed)\n",
        "fix_seed()\n",
        "configuration = RobertaConfig()\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", do_lower_case=True)\n",
        "dev_type = 'No_dev'\n",
        "import csv\n",
        "import re\n",
        "def readTask1Data(data_address):\n",
        "  data_id = list()\n",
        "  data_mask = list()\n",
        "  data_orginal = list()\n",
        "  data_replaced = list()\n",
        "  data_grades = list()\n",
        "  data_meanGrade = list()\n",
        "  data_editword = list()\n",
        "  data_orginalword = list()\n",
        "  flag = 0\n",
        "  p = r\"(?<=<).+?(?=/>)\" \n",
        "  with open(data_address, 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    for row in reader:\n",
        "      if flag == 0:\n",
        "        flag = 1\n",
        "        continue\n",
        "      data_id.append(row[0])\n",
        "      line = row[1]\n",
        "      while '  ' in line:\n",
        "        line.replace('  ',' ')\n",
        "      line = line.replace('/ >','/>')\n",
        "      line = line.replace('< ','<')\n",
        "      line = line.replace(' />','/>')\n",
        "      data_orginalword.append(re.findall(p, line))\n",
        "      data_orginal_temp = line.replace('<','').replace('/>','').replace('  ',' ')\n",
        "      data_orginal.append(data_orginal_temp)\n",
        "      data_mask_temp = re.sub(p, 'edit', line)\n",
        "      if '<edit/>' not in data_mask_temp:\n",
        "        print(data_mask_temp)\n",
        "      data_mask.append(data_mask_temp)\n",
        "      data_replaced_temp = data_mask_temp.replace('<edit/>',row[2])\n",
        "      data_replaced.append(data_replaced_temp)\n",
        "      data_grades.append(row[3])\n",
        "      data_meanGrade.append([float(row[4])])\n",
        "      data_editword.append(row[2])\n",
        "  return  data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword\n",
        "data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword = readTask1Data('/content/drive/MyDrive/Colab Notebooks/semeval-2020-task-7-dataset/subtask-1/train.csv')\n",
        "test_id, test_mask, test_orginal, test_replaced, test_grades, test_meanGrade, test_editword, test_orginalword = readTask1Data('/content/drive/MyDrive/Colab Notebooks/semeval-2020-task-7-dataset/subtask-1/test.csv')\n",
        "def splitTrainValid(data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword, valid_num=0.3):\n",
        "  train_id = list()\n",
        "  train_mask = list()\n",
        "  train_orginal = list()\n",
        "  train_replaced = list()\n",
        "  train_grades = list()\n",
        "  train_meanGrade = list()\n",
        "  train_editword = list()\n",
        "  train_orginalword = list()\n",
        "  val_id = list()\n",
        "  val_mask = list()\n",
        "  val_orginal = list()\n",
        "  val_replaced = list()\n",
        "  val_grades = list()\n",
        "  val_meanGrade = list()\n",
        "  val_editword = list()\n",
        "  val_orginalword = list()\n",
        "  temp_list = list()\n",
        "  for num in range(len(data_id)):\n",
        "    temp_list.append([data_id[num], data_mask[num], data_orginal[num], data_replaced[num], data_grades[num], data_meanGrade[num], data_editword[num], data_orginalword[num]])\n",
        "  train_set,valid_set = train_test_split(temp_list,test_size = valid_num)\n",
        "  for (set_id_temp, set_mask_temp, set_orginal_temp, set_replaced_temp, set_grades_temp, set_meanGrade_temp, set_editword_temp, set_orginalword_temp) in train_set:\n",
        "    train_id.append(set_id_temp)\n",
        "    train_mask.append(set_mask_temp)\n",
        "    train_orginal.append(set_orginal_temp)\n",
        "    train_replaced.append(set_replaced_temp)\n",
        "    train_grades.append(set_grades_temp)\n",
        "    train_meanGrade.append(set_meanGrade_temp)\n",
        "    train_editword.append(set_editword_temp)\n",
        "    train_orginalword.append(set_orginalword_temp)\n",
        "  for (set_id_temp, set_mask_temp, set_orginal_temp, set_replaced_temp, set_grades_temp, set_meanGrade_temp, set_editword_temp, set_orginalword_temp) in valid_set:\n",
        "    val_id.append(set_id_temp)\n",
        "    val_mask.append(set_mask_temp)\n",
        "    val_orginal.append(set_orginal_temp)\n",
        "    val_replaced.append(set_replaced_temp)\n",
        "    val_grades.append(set_grades_temp)\n",
        "    val_meanGrade.append(set_meanGrade_temp)\n",
        "    val_editword.append(set_editword_temp)\n",
        "    val_orginalword.append(set_orginalword_temp)\n",
        "  return train_id, train_mask, train_orginal, train_replaced, train_grades, train_meanGrade, train_editword, train_orginalword, val_id,val_mask,val_orginal,val_replaced,val_grades,val_meanGrade,val_editword,val_orginalword\n",
        "data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword, val_id, val_mask, val_orginal, val_replaced, val_grades, val_meanGrade, val_editword, val_orginalword = splitTrainValid(data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword)\n",
        "dev_type = 'No_dev'\n",
        "import csv\n",
        "import re\n",
        "def readTask1Data(data_address):\n",
        "  data_id = list()\n",
        "  data_mask = list()\n",
        "  data_orginal = list()\n",
        "  data_replaced = list()\n",
        "  data_grades = list()\n",
        "  data_meanGrade = list()\n",
        "  data_editword = list()\n",
        "  data_orginalword = list()\n",
        "  flag = 0\n",
        "  p = r\"(?<=<).+?(?=/>)\" \n",
        "  with open(data_address, 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    for row in reader:\n",
        "      if flag == 0:\n",
        "        flag = 1\n",
        "        continue\n",
        "      data_id.append(row[0])\n",
        "      line = row[1]\n",
        "      while '  ' in line:\n",
        "        line.replace('  ',' ')\n",
        "      line = line.replace('/ >','/>')\n",
        "      line = line.replace('< ','<')\n",
        "      line = line.replace(' />','/>')\n",
        "      data_orginalword.append(re.findall(p, line))\n",
        "      data_orginal_temp = line.replace('<','').replace('/>','').replace('  ',' ')\n",
        "      data_orginal.append(data_orginal_temp)\n",
        "      data_mask_temp = re.sub(p, 'edit', line)\n",
        "      if '<edit/>' not in data_mask_temp:\n",
        "        print(data_mask_temp)\n",
        "      data_mask.append(data_mask_temp)\n",
        "      data_replaced_temp = data_mask_temp.replace('<edit/>',row[2])\n",
        "      data_replaced.append(data_replaced_temp)\n",
        "      data_grades.append(row[3])\n",
        "      data_meanGrade.append([float(row[4])])\n",
        "      data_editword.append(row[2])\n",
        "  return  data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword\n",
        "data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword = readTask1Data('/content/drive/MyDrive/Colab Notebooks/semeval-2020-task-7-dataset/subtask-1/train.csv')\n",
        "test_id, test_mask, test_orginal, test_replaced, test_grades, test_meanGrade, test_editword, test_orginalword = readTask1Data('/content/drive/MyDrive/Colab Notebooks/semeval-2020-task-7-dataset/subtask-1/test.csv')\n",
        "def splitTrainValid(data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword, valid_num=0.3):\n",
        "  train_id = list()\n",
        "  train_mask = list()\n",
        "  train_orginal = list()\n",
        "  train_replaced = list()\n",
        "  train_grades = list()\n",
        "  train_meanGrade = list()\n",
        "  train_editword = list()\n",
        "  train_orginalword = list()\n",
        "  val_id = list()\n",
        "  val_mask = list()\n",
        "  val_orginal = list()\n",
        "  val_replaced = list()\n",
        "  val_grades = list()\n",
        "  val_meanGrade = list()\n",
        "  val_editword = list()\n",
        "  val_orginalword = list()\n",
        "  temp_list = list()\n",
        "  for num in range(len(data_id)):\n",
        "    temp_list.append([data_id[num], data_mask[num], data_orginal[num], data_replaced[num], data_grades[num], data_meanGrade[num], data_editword[num], data_orginalword[num]])\n",
        "  train_set,valid_set = train_test_split(temp_list,test_size = valid_num)\n",
        "  for (set_id_temp, set_mask_temp, set_orginal_temp, set_replaced_temp, set_grades_temp, set_meanGrade_temp, set_editword_temp, set_orginalword_temp) in train_set:\n",
        "    train_id.append(set_id_temp)\n",
        "    train_mask.append(set_mask_temp)\n",
        "    train_orginal.append(set_orginal_temp)\n",
        "    train_replaced.append(set_replaced_temp)\n",
        "    train_grades.append(set_grades_temp)\n",
        "    train_meanGrade.append(set_meanGrade_temp)\n",
        "    train_editword.append(set_editword_temp)\n",
        "    train_orginalword.append(set_orginalword_temp)\n",
        "  for (set_id_temp, set_mask_temp, set_orginal_temp, set_replaced_temp, set_grades_temp, set_meanGrade_temp, set_editword_temp, set_orginalword_temp) in valid_set:\n",
        "    val_id.append(set_id_temp)\n",
        "    val_mask.append(set_mask_temp)\n",
        "    val_orginal.append(set_orginal_temp)\n",
        "    val_replaced.append(set_replaced_temp)\n",
        "    val_grades.append(set_grades_temp)\n",
        "    val_meanGrade.append(set_meanGrade_temp)\n",
        "    val_editword.append(set_editword_temp)\n",
        "    val_orginalword.append(set_orginalword_temp)\n",
        "  return train_id, train_mask, train_orginal, train_replaced, train_grades, train_meanGrade, train_editword, train_orginalword, val_id,val_mask,val_orginal,val_replaced,val_grades,val_meanGrade,val_editword,val_orginalword\n",
        "data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword, val_id, val_mask, val_orginal, val_replaced, val_grades, val_meanGrade, val_editword, val_orginalword = splitTrainValid(data_id, data_mask, data_orginal, data_replaced, data_grades, data_meanGrade, data_editword, data_orginalword)\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base', return_dict=True, num_labels=1)\n",
        "model_name = 'OrginalRoBERTaSequenceClassification'\n",
        "lr = 3e-5\n",
        "max_grad_norm = 1.0\n",
        "eps = 1e-8\n",
        "batchSize = 16\n",
        "num_train_steps = totalEpoch\n",
        "WU = 0.2\n",
        "num_training_steps = ((len(data_id)/batchSize)+1) * totalEpoch\n",
        "num_warmup_steps = int(num_training_steps * WU)\n",
        "#scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_train_steps)\n",
        "optimizer = AdamW(model.parameters(), lr = lr, eps = eps)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_train_steps, num_training_steps=num_training_steps)\n",
        "loss_fn = nn.MSELoss()\n",
        "#loss_fn = nn.CrossEntropyLoss()##Task2\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)\n",
        "for epoch in range(totalEpoch):\n",
        "  batchStart = 0\n",
        "  train_loss = 0\n",
        "  optimizer.zero_grad()\n",
        "  while batchStart <= len(data_meanGrade):\n",
        "    batchEnd = batchStart + batchSize\n",
        "    if batchEnd > len(data_meanGrade):\n",
        "      batchEnd = len(data_meanGrade)\n",
        "    input_title = data_replaced[batchStart:batchEnd]\n",
        "    input_word = data_editword[batchStart:batchEnd]\n",
        "    input_data = [[input_title[num_],input_word[num_]] for num_ in range(len(input_title))]\n",
        "    encoding = tokenizer(input_data, return_tensors='pt', padding=True, truncation=True)\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    labels = torch.tensor(data_meanGrade[batchStart:batchEnd]).to(device)\n",
        "    model.train()\n",
        "    outputs = model(input_ids, attention_mask=attention_mask)[0]\n",
        "    #outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    loss = torch.sqrt(loss_fn(outputs,labels))\n",
        "    #loss = 2*torch.sqrt(((outputs - labels)**2).mean())\n",
        "    train_loss += loss.item()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    #torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "    batchStart = batchStart + batchSize\n",
        "    \n",
        "  with torch.no_grad():\n",
        "    input_title = val_replaced\n",
        "    input_word = val_editword\n",
        "    input_data = [[input_title[num_],input_word[num_]] for num_ in range(len(input_title))]\n",
        "    encoding = tokenizer(input_data, return_tensors='pt', padding=True, truncation=True)\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    outputs = model(input_ids, attention_mask=attention_mask)[0]\n",
        "    #outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    labels = torch.tensor(val_meanGrade).to(device)\n",
        "    loss = torch.sqrt(loss_fn(outputs,labels))\n",
        "    valid_loss = loss.item()\n",
        "    print(f'| Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f}')\n",
        "with torch.no_grad():\n",
        "  input_title = test_replaced\n",
        "  input_word = test_editword\n",
        "  input_data = [[input_title[num_],input_word[num_]] for num_ in range(len(input_title))]\n",
        "  encoding = tokenizer(input_data, return_tensors='pt', padding=True, truncation=True)\n",
        "  input_ids = encoding['input_ids'].to(device)\n",
        "  attention_mask = encoding['attention_mask'].to(device)\n",
        "  outputs = model(input_ids, attention_mask=attention_mask)[0]\n",
        "  #outputs = model(input_ids, attention_mask=attention_mask)\n",
        "  labels = torch.tensor(test_meanGrade).to(device)\n",
        "  loss = torch.sqrt(loss_fn(outputs,labels))\n",
        "  valid_loss = loss.item()\n",
        "  print(f'| Val. Loss: {valid_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KRW0mLIG-Lt"
      },
      "source": [
        "#Verify"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On7e5LV0G9lT"
      },
      "source": [
        "import csv\n",
        "import os\n",
        "import math\n",
        "true_address = '/content/drive/MyDrive/Colab Notebooks/semeval-2020-task-7-dataset/subtask-1/test.csv'\n",
        "flag = 0\n",
        "true_dict = dict()\n",
        "predict_dict = dict()\n",
        "with open(true_address, 'r') as f:\n",
        "  reader = csv.reader(f)\n",
        "  for row in reader:\n",
        "    if flag == 0:\n",
        "      flag = 1\n",
        "      continue\n",
        "    true_dict[row[0]] = float(row[4])\n",
        "folder = '/content/drive/MyDrive/Colab Notebooks/FinalProject/result'\n",
        "csv_file = dict()\n",
        "import os\n",
        "for file in os.listdir(folder):\n",
        "    if file.endswith(\".csv\"):\n",
        "      print(file)\n",
        "      csv_file[file] = os.path.join(folder, file)\n",
        "ff = open('result.csv','w',encoding='utf-8')\n",
        "str_ = 'Method,MAE,MSE,RMSE,MEAN,R_quared\\n'\n",
        "ff.write(str_)\n",
        "for key in csv_file:\n",
        "  MAE = 0.0\n",
        "  MSE = 0.0\n",
        "  RMSE = 0.0\n",
        "  flag = 0\n",
        "  MEAN = 0.0\n",
        "  R_quared = 0.0\n",
        "\n",
        "  with open(csv_file[key], 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    for row in reader:\n",
        "      if flag == 0:\n",
        "        flag = 1\n",
        "        continue\n",
        "      predict_dict[row[0]] = float(row[1])\n",
        "  for IDs in predict_dict:\n",
        "    MAE = MAE + abs(predict_dict[IDs] - true_dict[IDs])\n",
        "    RMSE = RMSE + (predict_dict[IDs] - true_dict[IDs]) ** 2\n",
        "    MSE = MSE + (predict_dict[IDs] - true_dict[IDs]) ** 2\n",
        "    MEAN = MEAN + predict_dict[IDs] \n",
        "  MAE = MAE/len(predict_dict)\n",
        "  RMSE = math.sqrt(RMSE/len(predict_dict))\n",
        "  MSE = MSE/len(predict_dict)\n",
        "  MEAN = MEAN/len(predict_dict)\n",
        "  R_quared_top = 0.0\n",
        "  R_quared_bottom = 0.0\n",
        "  for IDs in predict_dict:\n",
        "    R_quared_top = R_quared_top + (predict_dict[IDs] - true_dict[IDs]) ** 2\n",
        "    R_quared_bottom = R_quared_bottom + (MEAN - true_dict[IDs]) ** 2\n",
        "  R_quared = 1-(R_quared_top/R_quared_bottom)\n",
        "  str_ = key.replace('.csv','') + ',' + str(MAE) + ',' + str(MSE) + ',' + str(RMSE) + ',' + str(MEAN) + ',' + str(R_quared) +'\\n'\n",
        "  ff.write(str_)\n",
        "ff.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cw74ZkxrEqsw"
      },
      "source": [
        "R_quared"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKJETvhXEqu-"
      },
      "source": [
        "RMSE"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
